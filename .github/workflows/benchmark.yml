name: Continuous Benchmarking

permissions:
  contents: read

on:
  push:
    branches:
      - master
    paths:
      - 'src/**'
      - 'polars_bio/**'
      - 'benchmarks/**'
      - 'pyproject.toml'
      - 'Cargo.toml'
  pull_request:
    branches:
      - master
    paths:
      - 'src/**'
      - 'polars_bio/**'
      - 'benchmarks/**'
      - 'pyproject.toml'
      - 'Cargo.toml'
  workflow_dispatch:
    inputs:
      benchmark_tools:
        description: 'List of tools to benchmark (all, performance, memory, scalability, optimizations)'
        required: false
        default: 'default'
        type: string
      skip_data_download:
        description: 'Skip test data download (use cache)'
        required: false
        default: false
        type: boolean

env:
  POETRY_VERSION: 1.8.4
  BENCHMARK_DATA_URL: 'https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_49/gencode.v49.annotation.gff3.gz'
  DEFAULT_BENCHMARK_TOOLS: 'performance,memory,optimizations'

jobs:
  benchmark:
    permissions:
      contents: write
      pull-requests: write
    runs-on: ubuntu-latest
    timeout-minutes: 120
    
    steps:
      - name: Checkout polars-bio
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Required for performance comparison between commits
          
      - name: Checkout polars-bio-bench
        uses: actions/checkout@v4
        with:
          repository: 'biodatageeks/polars-bio-bench'
          path: 'polars-bio-bench'
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install Poetry
        uses: abatilo/actions-poetry@v2
        with:
          poetry-version: ${{ env.POETRY_VERSION }}

      - name: Set up Rust
        run: rustup show
        
      - name: Cache Rust build
        uses: mozilla-actions/sccache-action@v0.0.9

      - name: Cache benchmark data
        if: ${{ !inputs.skip_data_download }}
        uses: actions/cache@v3
        with:
          path: /tmp/gencode.v49.annotation.gff3.bgz
          key: gencode-v49-${{ hashFiles('**/benchmark_data_version.txt') }}
          restore-keys: gencode-v49-

      - name: Install dependencies
        run: |
          make venv
          make install

      - name: Download benchmark data
        if: ${{ !inputs.skip_data_download }}
        run: |
          mkdir -p /tmp
          if [ ! -f /tmp/gencode.v49.annotation.gff3.bgz ]; then
            echo "Downloading benchmark data..."
            wget -O /tmp/gencode.v49.annotation.gff3.bgz ${{ env.BENCHMARK_DATA_URL }}
          else
            echo "Using cached benchmark data"
          fi
          ls -la /tmp/gencode.v49.annotation.gff3.bgz

      - name: Run specific benchmarks
        id: run_benchmarks
        run: |
          cd benchmarks
          source ../.venv/bin/activate
          set -o pipefail

          # Determine which benchmarks to run based on input
          if [ "${{ inputs.benchmark_tools }}" = "default" ] || [ -z "${{ inputs.benchmark_tools }}" ]; then
            TOOLS="$DEFAULT_BENCHMARK_TOOLS"
          else
            TOOLS="${{ inputs.benchmark_tools }}"
          fi
          LOG_FILE=benchmark_output.txt
          : > "$LOG_FILE"

          if [ "$TOOLS" = "all" ]; then
            echo "Running all benchmarks..."
            python run_all_benchmarks.py 2>&1 | tee "$LOG_FILE"
          else
            echo "Running selective benchmarks: $TOOLS"

            TOOLS_NORMALIZED=$(echo "$TOOLS" | tr '[:upper:]' '[:lower:]')
            TOOLS_NORMALIZED=${TOOLS_NORMALIZED// /,}
            IFS=',' read -ra SELECTED <<< "$TOOLS_NORMALIZED"

            RUN_PERFORMANCE=false
            RUN_MEMORY=false
            RUN_SCALABILITY=false
            RUN_OPTIMIZATIONS=false

            for TOOL in "${SELECTED[@]}"; do
              TOOL=$(echo "$TOOL" | tr -d '[:space:]')
              if [ -z "$TOOL" ]; then
                continue
              fi
              case "$TOOL" in
                performance)
                  RUN_PERFORMANCE=true
                  ;;
                memory)
                  RUN_MEMORY=true
                  ;;
                scalability)
                  RUN_SCALABILITY=true
                  ;;
                optimizations)
                  RUN_OPTIMIZATIONS=true
                  ;;
                all)
                  RUN_PERFORMANCE=true
                  RUN_MEMORY=true
                  RUN_SCALABILITY=true
                  RUN_OPTIMIZATIONS=true
                  ;;
                *)
                  echo "Skipping unknown benchmark selector: $TOOL"
                  ;;
              esac
            done
            unset IFS

            if ! $RUN_PERFORMANCE && ! $RUN_MEMORY && ! $RUN_SCALABILITY && ! $RUN_OPTIMIZATIONS; then
              echo "No valid benchmark selectors were provided"
              exit 1
            fi

            if $RUN_PERFORMANCE; then
              echo "â†’ Running general performance benchmark"
              python 01_general_performance.py 2>&1 | tee -a "$LOG_FILE"
            fi
            if $RUN_MEMORY; then
              echo "â†’ Running memory profiling benchmark"
              python 02_memory_profiling.py 2>&1 | tee -a "$LOG_FILE"
            fi
            if $RUN_SCALABILITY; then
              echo "â†’ Running thread scalability benchmark"
              python 03_thread_scalability.py 2>&1 | tee -a "$LOG_FILE"
            fi
            if $RUN_OPTIMIZATIONS; then
              echo "â†’ Running projection pruning benchmark"
              python 04_projection_pruning.py 2>&1 | tee -a "$LOG_FILE"
              echo "â†’ Running predicate pushdown benchmark"
              python 05_predicate_pushdown.py 2>&1 | tee -a "$LOG_FILE"
              echo "â†’ Running combined optimizations benchmark"
              python 06_combined_optimizations.py 2>&1 | tee -a "$LOG_FILE"
            fi
          fi

          # Convert results to JSON format
          python ../scripts/convert_benchmarks_to_json.py

          echo "benchmark_completed=true" >> $GITHUB_OUTPUT

      - name: Store benchmark results (for performance tracking)
        if: steps.run_benchmarks.outputs.benchmark_completed == 'true'
        uses: benchmark-action/github-action-benchmark@v1
        with:
          name: 'Polars-Bio Performance Benchmarks'
          tool: 'customBiggerIsBetter'  # Higher performance (ops/sec) = better
          output-file-path: 'benchmarks/benchmark_results.json'
          benchmark-data-dir-path: 'benchmark-data'
          github-token: ${{ secrets.GITHUB_TOKEN }}
          auto-push: true
          comment-on-alert: true
          alert-threshold: '120%'  # Alert when performance drops by 20%
          fail-on-alert: false
          alert-comment-cc-users: '@biodatageeks/polars-bio-maintainers'
          max-items-in-chart: 100

      - name: Upload benchmark artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ github.sha }}
          path: |
            benchmarks/results/
            benchmarks/benchmark_output.txt
            benchmarks/benchmark_results.json
          retention-days: 30

      - name: Integration with polars-bio-bench
        if: steps.run_benchmarks.outputs.benchmark_completed == 'true'
        run: |
          cd polars-bio-bench
          
          # Create results directory marked with date and SHA
          RESULT_DIR="results/polars-bio-$(date +%Y%m%d)-${{ github.sha }}"
          mkdir -p "$RESULT_DIR"
          
          # Copy results
          cp ../benchmarks/results/*.csv "$RESULT_DIR/" || true
          cp ../benchmarks/benchmark_results.json "$RESULT_DIR/" || true
          
          # Save benchmark metadata
          cat > "$RESULT_DIR/benchmark_metadata.json" << EOF
          {
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "commit_sha": "${{ github.sha }}",
            "commit_ref": "${{ github.ref }}",
            "event_name": "${{ github.event_name }}",
            "actor": "${{ github.actor }}",
            "polars_bio_version": "$(cd .. && python -c 'import toml; print(toml.load(\"pyproject.toml\")[\"tool\"][\"poetry\"][\"version\"])')",
            "benchmark_tools": "${{ inputs.benchmark_tools || 'all' }}"
          }
          EOF
          
          # Run comparative analysis if available
          if [ -f "analyze_results.py" ]; then
            echo "Running comparative analysis..."
            python analyze_results.py --input "$RESULT_DIR/"
          fi
          
          echo "âœ… Results archived in polars-bio-bench: $RESULT_DIR"

  benchmark-comparison:
    if: github.event_name == 'pull_request'
    runs-on: ubuntu-latest
    needs: benchmark
    
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        
      - name: Download benchmark artifacts
        uses: actions/download-artifact@v4
        with:
          name: benchmark-results-${{ github.sha }}
          path: ./current-benchmarks

      - name: Performance regression check
        run: |
          echo "## ðŸ“Š Benchmark Results Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f "./current-benchmarks/benchmark_results.json" ]; then
            # Count number of benchmarks
            BENCHMARK_COUNT=$(jq 'length' ./current-benchmarks/benchmark_results.json)
            echo "### âœ… Benchmarks completed successfully" >> $GITHUB_STEP_SUMMARY
            echo "- **Benchmarks executed:** $BENCHMARK_COUNT" >> $GITHUB_STEP_SUMMARY
            echo "- **Commit SHA:** \`${{ github.sha }}\`" >> $GITHUB_STEP_SUMMARY
            echo "- **Check details:** [benchmark dashboard](https://biodatageeks.github.io/polars-bio/dev/bench/)" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            
            # Find best polars-bio results
            echo "### ðŸš€ Top Performance Results (polars-bio)" >> $GITHUB_STEP_SUMMARY
            jq -r '.[] | select(.extra.library == "polars-bio") | "- **\(.name)**: \(.value | tostring) ops/sec"' ./current-benchmarks/benchmark_results.json | head -5 >> $GITHUB_STEP_SUMMARY
            
          else
            echo "### âŒ Benchmarks failed" >> $GITHUB_STEP_SUMMARY
            echo "Check the workflow logs for more details." >> $GITHUB_STEP_SUMMARY
          fi

  notify-on-regression:
    if: github.event_name == 'push' && github.ref == 'refs/heads/master'
    runs-on: ubuntu-latest
    needs: benchmark
    
    steps:
      - name: Check for performance regressions
        run: |
          echo "Performance monitoring job completed for master branch"
          echo "Continuous benchmark tracking is active"