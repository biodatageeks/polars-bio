{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"polars-bio","text":""},{"location":"#next-gen-python-dataframe-operations-for-genomics","title":"Next-gen Python DataFrame operations for genomics!","text":"<p>polars-bio is a blazing fast Python DataFrame library for genomics\ud83e\uddec  built on top of Apache DataFusion, Apache Arrow and  polars. It is designed to be easy to use, fast and memory efficient with a focus on genomics data.</p> <p></p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>optimized for performance and memory efficiency for large-scale genomics datasets analyses both when reading input data and performing operations</li> <li>popular genomics operations with a DataFrame API (both Pandas and polars)</li> <li>SQL-powered bioinformatic data querying or manipulation/pre-processing</li> <li>native parallel engine powered by Apache DataFusion and datafusion-bio-functions</li> <li>out-of-core/streaming processing (for data too large to fit into a computer's main memory)  with Apache DataFusion and polars</li> <li>support for federated and streamed reading data from cloud storages (e.g. S3, GCS) with Apache OpenDAL  enabling processing large-scale genomics data without materializing in memory</li> <li>zero-copy data exchange with Apache Arrow</li> <li>bioinformatics file formats with noodles</li> <li>fast overlap operations with COITrees: Cache Oblivious Interval Trees</li> <li>pre-built wheel packages for Linux, Windows and MacOS (arm64 and x86_64) available on PyPI</li> </ul>"},{"location":"#performance-benchmarks","title":"Performance benchmarks","text":"<p>See quick start for the installation options.</p>"},{"location":"#citing","title":"Citing","text":"<p>If you use polars-bio in your work, please cite:</p> <pre><code>@article{10.1093/bioinformatics/btaf640,\n    author = {Wiewi\u00f3rka, Marek and Khamutou, Pavel and Zbysi\u0144ski, Marek and Gambin, Tomasz},\n    title = {polars-bio\u2014fast, scalable and out-of-core operations on large genomic interval datasets},\n    journal = {Bioinformatics},\n    pages = {btaf640},\n    year = {2025},\n    month = {12},\n    abstract = {Genomic studies very often rely on computationally intensive analyses of relationships between features, which are typically represented as intervals along a one-dimensional coordinate system (such as positions on a chromosome). In this context, the Python programming language is extensively used for manipulating and analyzing data stored in a tabular form of rows and columns, called a DataFrame. Pandas is the most widely used Python DataFrame package and has been criticized for inefficiencies and scalability issues, which its modern alternative\u2014Polars\u2014aims to address with a native backend written in the Rust programming language.polars-bio is a Python library that enables fast, parallel and out-of-core operations on large genomic interval datasets. Its main components are implemented in Rust, using the Apache DataFusion query engine and Apache Arrow for efficient data representation. It is compatible with Polars and Pandas DataFrame formats. In a real-world comparison (107 vs. 1.2\u00d7106 intervals), our library runs overlap queries 6.5x, nearest queries 15.5x, count\\_overlaps queries 38x, and coverage queries 15x faster than Bioframe. On equally-sized synthetic sets (107 vs. 107), the corresponding speedups are 1.6x, 5.5x, 6x, and 6x. In streaming mode, on real and synthetic interval pairs, our implementation uses 90x and 15x less memory for overlap, 4.5x and 6.5x less for nearest, 60x and 12x less for count\\_overlaps, and 34x and 7x less for coverage than Bioframe. Multi-threaded benchmarks show good scalability characteristics. To the best of our knowledge, polars-bio is the most efficient single-node library for genomic interval DataFrames in Python.polars-bio is an open-source Python package distributed under the Apache License available for major platforms, including Linux, macOS, and Windows in the PyPI registry. The online documentation is https://biodatageeks.org/polars-bio/ and the source code is available on GitHub: https://github.com/biodatageeks/polars-bio and Zenodo: https://doi.org/10.5281/zenodo.16374290. Supplementary Materials are available at Bioinformatics online.},\n    issn = {1367-4811},\n    doi = {10.1093/bioinformatics/btaf640},\n    url = {https://doi.org/10.1093/bioinformatics/btaf640},\n    eprint = {https://academic.oup.com/bioinformatics/advance-article-pdf/doi/10.1093/bioinformatics/btaf640/65667510/btaf640.pdf},\n}\n</code></pre>"},{"location":"#performance-benchmarks_1","title":"Performance benchmarks","text":""},{"location":"#single-thread","title":"Single-thread \ud83c\udfc3\u200d","text":""},{"location":"#parallel","title":"Parallel \ud83c\udfc3\u200d\ud83c\udfc3\u200d","text":""},{"location":"api/","title":"\u2699\ufe0f API reference","text":"<p>polars-bio API is grouped into the following categories:</p> <ul> <li>File I/O: Reading files in various biological formats from local and cloud storage.</li> <li>Data Processing: Exposing end user to the rich SQL programming interface powered by Apache Datafusion for operations, such as sorting, filtering and other transformations on input bioinformatic datasets registered as tables. You can easily query and process file formats such as VCF, GFF, BAM, FASTQ, Pairs using SQL syntax.</li> <li>Interval Operations: Functions for performing common interval operations, such as overlap, nearest, coverage, merge, cluster, complement, and subtract.</li> <li>Pileup Operations: Per-base read depth computation from BAM/SAM/CRAM files using CIGAR operations, similar to mosdepth/samtools depth.</li> </ul> <p>There are 2 ways of using polars-bio API:</p> <ul> <li>using <code>polars_bio</code> module</li> </ul> <p>Example</p> <pre><code>import polars_bio as pb\npb.read_fastq(\"gs://genomics-public-data/platinum-genomes/fastq/ERR194146.fastq.gz\").limit(1).collect()\n</code></pre> <ul> <li>directly on a Polars LazyFrame under a registered <code>pb</code> namespace</li> </ul> <p>Example</p> <p></p><pre><code> &gt;&gt;&gt; type(df)\n &lt;class 'polars.lazyframe.frame.LazyFrame'&gt;\n</code></pre> <pre><code>   import polars_bio as pb\n   df.pb.sort().limit(5).collect()\n</code></pre><p></p> <p>Tip</p> <ol> <li>Not all are available in both ways.</li> <li>You can of course use both ways in the same script.</li> </ol>"},{"location":"api/#polars_bio.CoordinateSystemMismatchError","title":"<code>CoordinateSystemMismatchError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when two DataFrames have different coordinate systems.</p> <p>This error occurs when attempting range operations (overlap, nearest, etc.) on DataFrames where one uses 0-based coordinates and the other uses 1-based coordinates.</p> Example <pre><code>df1 = pb.scan_vcf(\"file1.vcf\", one_based=False)  # 0-based\ndf2 = pb.scan_vcf(\"file2.vcf\", one_based=True)   # 1-based\npb.overlap(df1, df2)  # Raises CoordinateSystemMismatchError\n</code></pre> Source code in <code>polars_bio/exceptions.py</code> <pre><code>class CoordinateSystemMismatchError(Exception):\n    \"\"\"Raised when two DataFrames have different coordinate systems.\n\n    This error occurs when attempting range operations (overlap, nearest, etc.)\n    on DataFrames where one uses 0-based coordinates and the other uses 1-based\n    coordinates.\n\n    Example:\n        ```python\n        df1 = pb.scan_vcf(\"file1.vcf\", one_based=False)  # 0-based\n        df2 = pb.scan_vcf(\"file2.vcf\", one_based=True)   # 1-based\n        pb.overlap(df1, df2)  # Raises CoordinateSystemMismatchError\n        ```\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/#polars_bio.MissingCoordinateSystemError","title":"<code>MissingCoordinateSystemError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when a DataFrame lacks coordinate system metadata.</p> <p>Range operations require coordinate system metadata to determine the correct interval semantics. This error is raised when:</p> <ul> <li>A Polars LazyFrame/DataFrame lacks polars-config-meta metadata</li> <li>A Pandas DataFrame lacks df.attrs[\"coordinate_system_zero_based\"]</li> <li>A file path registers a table without Arrow schema metadata</li> </ul> <p>For Polars DataFrames, use polars-bio I/O functions (scan_, read_) which automatically set the metadata.</p> <p>For Pandas DataFrames, set the attribute before passing to range operations:     </p><pre><code>df.attrs[\"coordinate_system_zero_based\"] = True  # 0-based coords\n</code></pre><p></p> Example <pre><code>import pandas as pd\nimport polars_bio as pb\n\npdf = pd.read_csv(\"intervals.bed\", sep=\"        \", names=[\"chrom\", \"start\", \"end\"])\npb.overlap(pdf, pdf)  # Raises MissingCoordinateSystemError\n\n# Fix: set the coordinate system metadata\npdf.attrs[\"coordinate_system_zero_based\"] = True\npb.overlap(pdf, pdf)  # Works correctly\n</code></pre> Source code in <code>polars_bio/exceptions.py</code> <pre><code>class MissingCoordinateSystemError(Exception):\n    \"\"\"Raised when a DataFrame lacks coordinate system metadata.\n\n    Range operations require coordinate system metadata to determine the\n    correct interval semantics. This error is raised when:\n\n    - A Polars LazyFrame/DataFrame lacks polars-config-meta metadata\n    - A Pandas DataFrame lacks df.attrs[\"coordinate_system_zero_based\"]\n    - A file path registers a table without Arrow schema metadata\n\n    For Polars DataFrames, use polars-bio I/O functions (scan_*, read_*) which\n    automatically set the metadata.\n\n    For Pandas DataFrames, set the attribute before passing to range operations:\n        ```python\n        df.attrs[\"coordinate_system_zero_based\"] = True  # 0-based coords\n        ```\n\n    Example:\n        ```python\n        import pandas as pd\n        import polars_bio as pb\n\n        pdf = pd.read_csv(\"intervals.bed\", sep=\"\\t\", names=[\"chrom\", \"start\", \"end\"])\n        pb.overlap(pdf, pdf)  # Raises MissingCoordinateSystemError\n\n        # Fix: set the coordinate system metadata\n        pdf.attrs[\"coordinate_system_zero_based\"] = True\n        pb.overlap(pdf, pdf)  # Works correctly\n        ```\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/#polars_bio.data_input","title":"<code>data_input</code>","text":"Source code in <code>polars_bio/io.py</code> <pre><code>class IOOperations:\n    @staticmethod\n    def read_fasta(\n        path: str,\n        chunk_size: int = 8,\n        concurrent_fetches: int = 1,\n        allow_anonymous: bool = True,\n        enable_request_payer: bool = False,\n        max_retries: int = 5,\n        timeout: int = 300,\n        compression_type: str = \"auto\",\n        projection_pushdown: bool = True,\n    ) -&gt; pl.DataFrame:\n        \"\"\"\n\n        Read a FASTA file into a DataFrame.\n\n        Parameters:\n            path: The path to the FASTA file.\n            chunk_size: The size in MB of a chunk when reading from an object store. The default is 8 MB. For large scale operations, it is recommended to increase this value to 64.\n            concurrent_fetches: [GCS] The number of concurrent fetches when reading from an object store. The default is 1. For large scale operations, it is recommended to increase this value to 8 or even more.\n            allow_anonymous: [GCS, AWS S3] Whether to allow anonymous access to object storage.\n            enable_request_payer: [AWS S3] Whether to enable request payer for object storage. This is useful for reading files from AWS S3 buckets that require request payer.\n            max_retries:  The maximum number of retries for reading the file from object storage.\n            timeout: The timeout in seconds for reading the file from object storage.\n            compression_type: The compression type of the FASTA file. If not specified, it will be detected automatically based on the file extension. BGZF and GZIP compressions are supported ('bgz', 'gz').\n            projection_pushdown: Enable column projection pushdown optimization. When True, only requested columns are processed at the DataFusion execution level, improving performance and reducing memory usage.\n\n        !!! Example\n            ```shell\n            wget https://www.ebi.ac.uk/ena/browser/api/fasta/BK006935.2?download=true -O /tmp/test.fasta\n            ```\n\n            ```python\n            import polars_bio as pb\n            pb.read_fasta(\"/tmp/test.fasta\").limit(1)\n            ```\n            ```shell\n             shape: (1, 3)\n            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n            \u2502 name                    \u2506 description                     \u2506 sequence                        \u2502\n            \u2502 ---                     \u2506 ---                             \u2506 ---                             \u2502\n            \u2502 str                     \u2506 str                             \u2506 str                             \u2502\n            \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n            \u2502 ENA|BK006935|BK006935.2 \u2506 TPA_inf: Saccharomyces cerevis\u2026 \u2506 CCACACCACACCCACACACCCACACACCAC\u2026 \u2502\n            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n            ```\n        \"\"\"\n        return IOOperations.scan_fasta(\n            path,\n            chunk_size,\n            concurrent_fetches,\n            allow_anonymous,\n            enable_request_payer,\n            max_retries,\n            timeout,\n            compression_type,\n            projection_pushdown,\n        ).collect()\n\n    @staticmethod\n    def scan_fasta(\n        path: str,\n        chunk_size: int = 8,\n        concurrent_fetches: int = 1,\n        allow_anonymous: bool = True,\n        enable_request_payer: bool = False,\n        max_retries: int = 5,\n        timeout: int = 300,\n        compression_type: str = \"auto\",\n        projection_pushdown: bool = True,\n    ) -&gt; pl.LazyFrame:\n        \"\"\"\n\n        Lazily read a FASTA file into a LazyFrame.\n\n        Parameters:\n            path: The path to the FASTA file.\n            chunk_size: The size in MB of a chunk when reading from an object store. The default is 8 MB. For large scale operations, it is recommended to increase this value to 64.\n            concurrent_fetches: [GCS] The number of concurrent fetches when reading from an object store. The default is 1. For large scale operations, it is recommended to increase this value to 8 or even more.\n            allow_anonymous: [GCS, AWS S3] Whether to allow anonymous access to object storage.\n            enable_request_payer: [AWS S3] Whether to enable request payer for object storage. This is useful for reading files from AWS S3 buckets that require request payer.\n            max_retries:  The maximum number of retries for reading the file from object storage.\n            timeout: The timeout in seconds for reading the file from object storage.\n            compression_type: The compression type of the FASTA file. If not specified, it will be detected automatically based on the file extension. BGZF and GZIP compressions are supported ('bgz', 'gz').\n            projection_pushdown: Enable column projection pushdown to optimize query performance by only reading the necessary columns at the DataFusion level.\n\n        !!! Example\n            ```shell\n            wget https://www.ebi.ac.uk/ena/browser/api/fasta/BK006935.2?download=true -O /tmp/test.fasta\n            ```\n\n            ```python\n            import polars_bio as pb\n            pb.scan_fasta(\"/tmp/test.fasta\").limit(1).collect()\n            ```\n            ```shell\n             shape: (1, 3)\n            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n            \u2502 name                    \u2506 description                     \u2506 sequence                        \u2502\n            \u2502 ---                     \u2506 ---                             \u2506 ---                             \u2502\n            \u2502 str                     \u2506 str                             \u2506 str                             \u2502\n            \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n            \u2502 ENA|BK006935|BK006935.2 \u2506 TPA_inf: Saccharomyces cerevis\u2026 \u2506 CCACACCACACCCACACACCCACACACCAC\u2026 \u2502\n            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n            ```\n        \"\"\"\n        object_storage_options = PyObjectStorageOptions(\n            allow_anonymous=allow_anonymous,\n            enable_request_payer=enable_request_payer,\n            chunk_size=chunk_size,\n            concurrent_fetches=concurrent_fetches,\n            max_retries=max_retries,\n            timeout=timeout,\n            compression_type=compression_type,\n        )\n        fasta_read_options = FastaReadOptions(\n            object_storage_options=object_storage_options\n        )\n        read_options = ReadOptions(fasta_read_options=fasta_read_options)\n        return _read_file(path, InputFormat.Fasta, read_options, projection_pushdown)\n\n    @staticmethod\n    def read_vcf(\n        path: str,\n        info_fields: Union[list[str], None] = None,\n        format_fields: Union[list[str], None] = None,\n        chunk_size: int = 8,\n        concurrent_fetches: int = 1,\n        allow_anonymous: bool = True,\n        enable_request_payer: bool = False,\n        max_retries: int = 5,\n        timeout: int = 300,\n        compression_type: str = \"auto\",\n        projection_pushdown: bool = True,\n        predicate_pushdown: bool = True,\n        use_zero_based: Optional[bool] = None,\n    ) -&gt; pl.DataFrame:\n        \"\"\"\n        Read a VCF file into a DataFrame.\n\n        !!! hint \"Parallelism &amp; Indexed Reads\"\n            Indexed parallel reads and predicate pushdown are automatic when a TBI/CSI index\n            is present. See [File formats support](/polars-bio/features/#file-formats-support),\n            [Indexed reads](/polars-bio/features/#indexed-reads-predicate-pushdown),\n            and [Automatic parallel partitioning](/polars-bio/features/#automatic-parallel-partitioning) for details.\n\n        Parameters:\n            path: The path to the VCF file.\n            info_fields: List of INFO field names to include. If *None*, all INFO fields from the VCF header are included by default. Use this to limit fields for better performance.\n            format_fields: List of FORMAT field names to include (per-sample genotype data). If *None*, all FORMAT fields are included by default. For **single-sample** VCFs, FORMAT fields are top-level columns (e.g., `GT`, `DP`). For **multi-sample** VCFs, FORMAT data is exposed as a nested `genotypes` column (`list&lt;struct&lt;sample_id, values&gt;&gt;`).\n            chunk_size: The size in MB of a chunk when reading from an object store. The default is 8 MB. For large scale operations, it is recommended to increase this value to 64.\n            concurrent_fetches: [GCS] The number of concurrent fetches when reading from an object store. The default is 1. For large scale operations, it is recommended to increase this value to 8 or even more.\n            allow_anonymous: [GCS, AWS S3] Whether to allow anonymous access to object storage.\n            enable_request_payer: [AWS S3] Whether to enable request payer for object storage. This is useful for reading files from AWS S3 buckets that require request payer.\n            max_retries:  The maximum number of retries for reading the file from object storage.\n            timeout: The timeout in seconds for reading the file from object storage.\n            compression_type: The compression type of the VCF file. If not specified, it will be detected automatically..\n            projection_pushdown: Enable column projection pushdown to optimize query performance by only reading the necessary columns at the DataFusion level.\n            predicate_pushdown: Enable predicate pushdown using index files (TBI/CSI) for efficient region-based filtering. Index files are auto-discovered (e.g., `file.vcf.gz.tbi`). Only simple predicates are pushed down (equality, comparisons, IN); complex predicates like `.str.contains()` or OR logic are filtered client-side. Correctness is always guaranteed.\n            use_zero_based: If True, output 0-based half-open coordinates. If False, output 1-based closed coordinates. If None (default), uses the global configuration `datafusion.bio.coordinate_system_zero_based`.\n\n        !!! note\n            By default, coordinates are output in **1-based closed** format. Use `use_zero_based=True` or set `pb.set_option(pb.POLARS_BIO_COORDINATE_SYSTEM_ZERO_BASED, True)` for 0-based half-open coordinates.\n\n        !!! Example \"Reading VCF with INFO and FORMAT fields\"\n            ```python\n            import polars_bio as pb\n\n            # Read VCF with both INFO and FORMAT fields\n            df = pb.read_vcf(\n                \"sample.vcf.gz\",\n                info_fields=[\"END\"],              # INFO field\n                format_fields=[\"GT\", \"DP\", \"GQ\"]  # FORMAT fields\n            )\n\n            # Single-sample VCF: FORMAT fields are top-level columns (GT, DP, GQ)\n            print(df.select([\"chrom\", \"start\", \"ref\", \"alt\", \"END\", \"GT\", \"DP\", \"GQ\"]))\n            # Output:\n            # shape: (10, 8)\n            # \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n            # \u2502 chrom \u2506 start \u2506 ref \u2506 alt \u2506 END  \u2506 GT  \u2506 DP  \u2506 GQ  \u2502\n            # \u2502 str   \u2506 u32   \u2506 str \u2506 str \u2506 i32  \u2506 str \u2506 i32 \u2506 i32 \u2502\n            # \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n            # \u2502 1     \u2506 10009 \u2506 A   \u2506 .   \u2506 null \u2506 0/0 \u2506 10  \u2506 27  \u2502\n            # \u2502 1     \u2506 10015 \u2506 A   \u2506 .   \u2506 null \u2506 0/0 \u2506 17  \u2506 35  \u2502\n            # \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n\n            # Multi-sample VCF: FORMAT data is nested in \"genotypes\"\n            df = pb.read_vcf(\"multisample.vcf\", format_fields=[\"GT\", \"DP\"])\n            print(df.select([\"chrom\", \"start\", \"genotypes\"]))\n            ```\n        \"\"\"\n        lf = IOOperations.scan_vcf(\n            path,\n            info_fields,\n            format_fields,\n            chunk_size,\n            concurrent_fetches,\n            allow_anonymous,\n            enable_request_payer,\n            max_retries,\n            timeout,\n            compression_type,\n            projection_pushdown,\n            predicate_pushdown,\n            use_zero_based,\n        )\n        # Get metadata before collecting (polars-config-meta doesn't preserve through collect)\n        zero_based = lf.config_meta.get_metadata().get(\"coordinate_system_zero_based\")\n        df = lf.collect()\n        # Set metadata on the collected DataFrame\n        if zero_based is not None:\n            set_coordinate_system(df, zero_based)\n        return df\n\n    @staticmethod\n    def scan_vcf(\n        path: str,\n        info_fields: Union[list[str], None] = None,\n        format_fields: Union[list[str], None] = None,\n        chunk_size: int = 8,\n        concurrent_fetches: int = 1,\n        allow_anonymous: bool = True,\n        enable_request_payer: bool = False,\n        max_retries: int = 5,\n        timeout: int = 300,\n        compression_type: str = \"auto\",\n        projection_pushdown: bool = True,\n        predicate_pushdown: bool = True,\n        use_zero_based: Optional[bool] = None,\n    ) -&gt; pl.LazyFrame:\n        \"\"\"\n        Lazily read a VCF file into a LazyFrame.\n\n        !!! hint \"Parallelism &amp; Indexed Reads\"\n            Indexed parallel reads and predicate pushdown are automatic when a TBI/CSI index\n            is present. See [File formats support](/polars-bio/features/#file-formats-support),\n            [Indexed reads](/polars-bio/features/#indexed-reads-predicate-pushdown),\n            and [Automatic parallel partitioning](/polars-bio/features/#automatic-parallel-partitioning) for details.\n\n        Parameters:\n            path: The path to the VCF file.\n            info_fields: List of INFO field names to include. If *None*, all INFO fields from the VCF header are included by default. Use this to limit fields for better performance.\n            format_fields: List of FORMAT field names to include (per-sample genotype data). If *None*, all FORMAT fields are included by default. For **single-sample** VCFs, FORMAT fields are top-level columns (e.g., `GT`, `DP`). For **multi-sample** VCFs, FORMAT data is exposed as a nested `genotypes` column (`list&lt;struct&lt;sample_id, values&gt;&gt;`).\n            chunk_size: The size in MB of a chunk when reading from an object store. The default is 8 MB. For large scale operations, it is recommended to increase this value to 64.\n            concurrent_fetches: [GCS] The number of concurrent fetches when reading from an object store. The default is 1. For large scale operations, it is recommended to increase this value to 8 or even more.\n            allow_anonymous: [GCS, AWS S3] Whether to allow anonymous access to object storage.\n            enable_request_payer: [AWS S3] Whether to enable request payer for object storage. This is useful for reading files from AWS S3 buckets that require request payer.\n            max_retries:  The maximum number of retries for reading the file from object storage.\n            timeout: The timeout in seconds for reading the file from object storage.\n            compression_type: The compression type of the VCF file. If not specified, it will be detected automatically..\n            projection_pushdown: Enable column projection pushdown to optimize query performance by only reading the necessary columns at the DataFusion level.\n            predicate_pushdown: Enable predicate pushdown using index files (TBI/CSI) for efficient region-based filtering. Index files are auto-discovered (e.g., `file.vcf.gz.tbi`). Only simple predicates are pushed down (equality, comparisons, IN); complex predicates like `.str.contains()` or OR logic are filtered client-side. Correctness is always guaranteed.\n            use_zero_based: If True, output 0-based half-open coordinates. If False, output 1-based closed coordinates. If None (default), uses the global configuration `datafusion.bio.coordinate_system_zero_based`.\n\n        !!! note\n            By default, coordinates are output in **1-based closed** format. Use `use_zero_based=True` or set `pb.set_option(pb.POLARS_BIO_COORDINATE_SYSTEM_ZERO_BASED, True)` for 0-based half-open coordinates.\n\n        !!! Example \"Lazy scanning VCF with INFO and FORMAT fields\"\n            ```python\n            import polars_bio as pb\n\n            # Lazily scan VCF with both INFO and FORMAT fields\n            lf = pb.scan_vcf(\n                \"sample.vcf.gz\",\n                info_fields=[\"END\"],              # INFO field\n                format_fields=[\"GT\", \"DP\", \"GQ\"]  # FORMAT fields\n            )\n\n            # Apply filters and collect only what's needed\n            df = lf.filter(pl.col(\"DP\") &gt; 20).select(\n                [\"chrom\", \"start\", \"ref\", \"alt\", \"GT\", \"DP\", \"GQ\"]\n            ).collect()\n\n            # Single-sample VCF: FORMAT fields are top-level columns (GT, DP, GQ)\n            # Multi-sample VCF: FORMAT data is nested in \"genotypes\"\n            ```\n        \"\"\"\n        object_storage_options = PyObjectStorageOptions(\n            allow_anonymous=allow_anonymous,\n            enable_request_payer=enable_request_payer,\n            chunk_size=chunk_size,\n            concurrent_fetches=concurrent_fetches,\n            max_retries=max_retries,\n            timeout=timeout,\n            compression_type=compression_type,\n        )\n\n        # Upstream VCF reader projects all INFO fields by default when info_fields is None.\n        initial_info_fields = info_fields\n\n        zero_based = _resolve_zero_based(use_zero_based)\n        vcf_read_options = VcfReadOptions(\n            info_fields=initial_info_fields,\n            format_fields=format_fields,\n            object_storage_options=object_storage_options,\n            zero_based=zero_based,\n        )\n        read_options = ReadOptions(vcf_read_options=vcf_read_options)\n        return _read_file(\n            path,\n            InputFormat.Vcf,\n            read_options,\n            projection_pushdown,\n            predicate_pushdown,\n            zero_based=zero_based,\n        )\n\n    @staticmethod\n    def read_gff(\n        path: str,\n        attr_fields: Union[list[str], None] = None,\n        chunk_size: int = 8,\n        concurrent_fetches: int = 1,\n        allow_anonymous: bool = True,\n        enable_request_payer: bool = False,\n        max_retries: int = 5,\n        timeout: int = 300,\n        compression_type: str = \"auto\",\n        projection_pushdown: bool = True,\n        predicate_pushdown: bool = True,\n        use_zero_based: Optional[bool] = None,\n    ) -&gt; pl.DataFrame:\n        \"\"\"\n        Read a GFF file into a DataFrame.\n\n        Parameters:\n            path: The path to the GFF file.\n            attr_fields: List of attribute field names to extract as separate columns. If *None*, attributes will be kept as a nested structure. Use this to extract specific attributes like 'ID', 'gene_name', 'gene_type', etc. as direct columns for easier access.\n            chunk_size: The size in MB of a chunk when reading from an object store. The default is 8 MB. For large scale operations, it is recommended to increase this value to 64.\n            concurrent_fetches: [GCS] The number of concurrent fetches when reading from an object store. The default is 1. For large scale operations, it is recommended to increase this value to 8 or even more.\n            allow_anonymous: [GCS, AWS S3] Whether to allow anonymous access to object storage.\n            enable_request_payer: [AWS S3] Whether to enable request payer for object storage. This is useful for reading files from AWS S3 buckets that require request payer.\n            max_retries:  The maximum number of retries for reading the file from object storage.\n            timeout: The timeout in seconds for reading the file from object storage.\n            compression_type: The compression type of the GFF file. If not specified, it will be detected automatically..\n            projection_pushdown: Enable column projection pushdown to optimize query performance by only reading the necessary columns at the DataFusion level.\n            predicate_pushdown: Enable predicate pushdown using index files (TBI/CSI) for efficient region-based filtering. Index files are auto-discovered (e.g., `file.gff.gz.tbi`). Only simple predicates are pushed down (equality, comparisons, IN); complex predicates like `.str.contains()` or OR logic are filtered client-side. Correctness is always guaranteed.\n            use_zero_based: If True, output 0-based half-open coordinates. If False, output 1-based closed coordinates. If None (default), uses the global configuration `datafusion.bio.coordinate_system_zero_based`.\n\n        !!! note\n            By default, coordinates are output in **1-based closed** format. Use `use_zero_based=True` or set `pb.set_option(pb.POLARS_BIO_COORDINATE_SYSTEM_ZERO_BASED, True)` for 0-based half-open coordinates.\n        \"\"\"\n        lf = IOOperations.scan_gff(\n            path,\n            attr_fields,\n            chunk_size,\n            concurrent_fetches,\n            allow_anonymous,\n            enable_request_payer,\n            max_retries,\n            timeout,\n            compression_type,\n            projection_pushdown,\n            predicate_pushdown,\n            use_zero_based,\n        )\n        # Get metadata before collecting (polars-config-meta doesn't preserve through collect)\n        zero_based = lf.config_meta.get_metadata().get(\"coordinate_system_zero_based\")\n        df = lf.collect()\n        # Set metadata on the collected DataFrame\n        if zero_based is not None:\n            set_coordinate_system(df, zero_based)\n        return df\n\n    @staticmethod\n    def scan_gff(\n        path: str,\n        attr_fields: Union[list[str], None] = None,\n        chunk_size: int = 8,\n        concurrent_fetches: int = 1,\n        allow_anonymous: bool = True,\n        enable_request_payer: bool = False,\n        max_retries: int = 5,\n        timeout: int = 300,\n        compression_type: str = \"auto\",\n        projection_pushdown: bool = True,\n        predicate_pushdown: bool = True,\n        use_zero_based: Optional[bool] = None,\n    ) -&gt; pl.LazyFrame:\n        \"\"\"\n        Lazily read a GFF file into a LazyFrame.\n\n        Parameters:\n            path: The path to the GFF file.\n            attr_fields: List of attribute field names to extract as separate columns. If *None*, attributes will be kept as a nested structure. Use this to extract specific attributes like 'ID', 'gene_name', 'gene_type', etc. as direct columns for easier access.\n            chunk_size: The size in MB of a chunk when reading from an object store. The default is 8 MB. For large scale operations, it is recommended to increase this value to 64.\n            concurrent_fetches: [GCS] The number of concurrent fetches when reading from an object store. The default is 1. For large-scale operations, it is recommended to increase this value to 8 or even more.\n            allow_anonymous: [GCS, AWS S3] Whether to allow anonymous access to object storage.\n            enable_request_payer: [AWS S3] Whether to enable request payer for object storage. This is useful for reading files from AWS S3 buckets that require request payer.\n            max_retries:  The maximum number of retries for reading the file from object storage.\n            timeout: The timeout in seconds for reading the file from object storage.\n            compression_type: The compression type of the GFF file. If not specified, it will be detected automatically.\n            projection_pushdown: Enable column projection pushdown to optimize query performance by only reading the necessary columns at the DataFusion level.\n            predicate_pushdown: Enable predicate pushdown using index files (TBI/CSI) for efficient region-based filtering. Index files are auto-discovered (e.g., `file.gff.gz.tbi`). Only simple predicates are pushed down (equality, comparisons, IN); complex predicates like `.str.contains()` or OR logic are filtered client-side. Correctness is always guaranteed.\n            use_zero_based: If True, output 0-based half-open coordinates. If False, output 1-based closed coordinates. If None (default), uses the global configuration `datafusion.bio.coordinate_system_zero_based`.\n\n        !!! note\n            By default, coordinates are output in **1-based closed** format. Use `use_zero_based=True` or set `pb.set_option(pb.POLARS_BIO_COORDINATE_SYSTEM_ZERO_BASED, True)` for 0-based half-open coordinates.\n        \"\"\"\n        object_storage_options = PyObjectStorageOptions(\n            allow_anonymous=allow_anonymous,\n            enable_request_payer=enable_request_payer,\n            chunk_size=chunk_size,\n            concurrent_fetches=concurrent_fetches,\n            max_retries=max_retries,\n            timeout=timeout,\n            compression_type=compression_type,\n        )\n\n        zero_based = _resolve_zero_based(use_zero_based)\n        gff_read_options = GffReadOptions(\n            attr_fields=attr_fields,\n            object_storage_options=object_storage_options,\n            zero_based=zero_based,\n        )\n        read_options = ReadOptions(gff_read_options=gff_read_options)\n        return _read_file(\n            path,\n            InputFormat.Gff,\n            read_options,\n            projection_pushdown,\n            predicate_pushdown,\n            zero_based=zero_based,\n        )\n\n    @staticmethod\n    def read_bam(\n        path: str,\n        tag_fields: Union[list[str], None] = None,\n        chunk_size: int = 8,\n        concurrent_fetches: int = 1,\n        allow_anonymous: bool = True,\n        enable_request_payer: bool = False,\n        max_retries: int = 5,\n        timeout: int = 300,\n        projection_pushdown: bool = True,\n        predicate_pushdown: bool = True,\n        use_zero_based: Optional[bool] = None,\n    ) -&gt; pl.DataFrame:\n        \"\"\"\n        Read a BAM file into a DataFrame.\n\n        !!! hint \"Parallelism &amp; Indexed Reads\"\n            Indexed parallel reads and predicate pushdown are automatic when a BAI/CSI index\n            is present. See [File formats support](/polars-bio/features/#file-formats-support),\n            [Indexed reads](/polars-bio/features/#indexed-reads-predicate-pushdown),\n            and [Automatic parallel partitioning](/polars-bio/features/#automatic-parallel-partitioning) for details.\n\n        Parameters:\n            path: The path to the BAM file.\n            tag_fields: List of BAM tag names to include as columns (e.g., [\"NM\", \"MD\", \"AS\"]). If None, no optional tags are parsed (default). Common tags include: NM (edit distance), MD (mismatch string), AS (alignment score), XS (secondary alignment score), RG (read group), CB (cell barcode), UB (UMI barcode).\n            chunk_size: The size in MB of a chunk when reading from an object store. The default is 8 MB. For large-scale operations, it is recommended to increase this value to 64.\n            concurrent_fetches: [GCS] The number of concurrent fetches when reading from an object store. The default is 1. For large-scale operations, it is recommended to increase this value to 8 or even more.\n            allow_anonymous: [GCS, AWS S3] Whether to allow anonymous access to object storage.\n            enable_request_payer: [AWS S3] Whether to enable request payer for object storage. This is useful for reading files from AWS S3 buckets that require request payer.\n            max_retries:  The maximum number of retries for reading the file from object storage.\n            timeout: The timeout in seconds for reading the file from object storage.\n            projection_pushdown: Enable column projection pushdown to optimize query performance by only reading the necessary columns at the DataFusion level.\n            predicate_pushdown: Enable predicate pushdown using index files (BAI/CSI) for efficient region-based filtering. Index files are auto-discovered (e.g., `file.bam.bai`). Only simple predicates are pushed down (equality, comparisons, IN); complex predicates like `.str.contains()` or OR logic are filtered client-side. Correctness is always guaranteed.\n            use_zero_based: If True, output 0-based half-open coordinates. If False, output 1-based closed coordinates. If None (default), uses the global configuration `datafusion.bio.coordinate_system_zero_based`.\n\n        !!! note\n            By default, coordinates are output in **1-based closed** format. Use `use_zero_based=True` or set `pb.set_option(pb.POLARS_BIO_COORDINATE_SYSTEM_ZERO_BASED, True)` for 0-based half-open coordinates.\n        \"\"\"\n        lf = IOOperations.scan_bam(\n            path,\n            tag_fields,\n            chunk_size,\n            concurrent_fetches,\n            allow_anonymous,\n            enable_request_payer,\n            max_retries,\n            timeout,\n            projection_pushdown,\n            predicate_pushdown,\n            use_zero_based,\n        )\n        # Get metadata before collecting (polars-config-meta doesn't preserve through collect)\n        zero_based = lf.config_meta.get_metadata().get(\"coordinate_system_zero_based\")\n        df = lf.collect()\n        # Set metadata on the collected DataFrame\n        if zero_based is not None:\n            set_coordinate_system(df, zero_based)\n        return df\n\n    @staticmethod\n    def scan_bam(\n        path: str,\n        tag_fields: Union[list[str], None] = None,\n        chunk_size: int = 8,\n        concurrent_fetches: int = 1,\n        allow_anonymous: bool = True,\n        enable_request_payer: bool = False,\n        max_retries: int = 5,\n        timeout: int = 300,\n        projection_pushdown: bool = True,\n        predicate_pushdown: bool = True,\n        use_zero_based: Optional[bool] = None,\n    ) -&gt; pl.LazyFrame:\n        \"\"\"\n        Lazily read a BAM file into a LazyFrame.\n\n        !!! hint \"Parallelism &amp; Indexed Reads\"\n            Indexed parallel reads and predicate pushdown are automatic when a BAI/CSI index\n            is present. See [File formats support](/polars-bio/features/#file-formats-support),\n            [Indexed reads](/polars-bio/features/#indexed-reads-predicate-pushdown),\n            and [Automatic parallel partitioning](/polars-bio/features/#automatic-parallel-partitioning) for details.\n\n        Parameters:\n            path: The path to the BAM file.\n            tag_fields: List of BAM tag names to include as columns (e.g., [\"NM\", \"MD\", \"AS\"]). If None, no optional tags are parsed (default). Common tags include: NM (edit distance), MD (mismatch string), AS (alignment score), XS (secondary alignment score), RG (read group), CB (cell barcode), UB (UMI barcode).\n            chunk_size: The size in MB of a chunk when reading from an object store. The default is 8 MB. For large scale operations, it is recommended to increase this value to 64.\n            concurrent_fetches: [GCS] The number of concurrent fetches when reading from an object store. The default is 1. For large scale operations, it is recommended to increase this value to 8 or even more.\n            allow_anonymous: [GCS, AWS S3] Whether to allow anonymous access to object storage.\n            enable_request_payer: [AWS S3] Whether to enable request payer for object storage. This is useful for reading files from AWS S3 buckets that require request payer.\n            max_retries:  The maximum number of retries for reading the file from object storage.\n            timeout: The timeout in seconds for reading the file from object storage.\n            projection_pushdown: Enable column projection pushdown to optimize query performance by only reading the necessary columns at the DataFusion level.\n            predicate_pushdown: Enable predicate pushdown using index files (BAI/CSI) for efficient region-based filtering. Index files are auto-discovered (e.g., `file.bam.bai`). Only simple predicates are pushed down (equality, comparisons, IN); complex predicates like `.str.contains()` or OR logic are filtered client-side. Correctness is always guaranteed.\n            use_zero_based: If True, output 0-based half-open coordinates. If False, output 1-based closed coordinates. If None (default), uses the global configuration `datafusion.bio.coordinate_system_zero_based`.\n\n        !!! note\n            By default, coordinates are output in **1-based closed** format. Use `use_zero_based=True` or set `pb.set_option(pb.POLARS_BIO_COORDINATE_SYSTEM_ZERO_BASED, True)` for 0-based half-open coordinates.\n        \"\"\"\n        object_storage_options = PyObjectStorageOptions(\n            allow_anonymous=allow_anonymous,\n            enable_request_payer=enable_request_payer,\n            chunk_size=chunk_size,\n            concurrent_fetches=concurrent_fetches,\n            max_retries=max_retries,\n            timeout=timeout,\n            compression_type=\"auto\",\n        )\n\n        zero_based = _resolve_zero_based(use_zero_based)\n        bam_read_options = BamReadOptions(\n            object_storage_options=object_storage_options,\n            zero_based=zero_based,\n            tag_fields=tag_fields,\n        )\n        read_options = ReadOptions(bam_read_options=bam_read_options)\n        return _read_file(\n            path,\n            InputFormat.Bam,\n            read_options,\n            projection_pushdown,\n            predicate_pushdown,\n            zero_based=zero_based,\n        )\n\n    @staticmethod\n    def read_cram(\n        path: str,\n        reference_path: str = None,\n        tag_fields: Union[list[str], None] = None,\n        chunk_size: int = 8,\n        concurrent_fetches: int = 1,\n        allow_anonymous: bool = True,\n        enable_request_payer: bool = False,\n        max_retries: int = 5,\n        timeout: int = 300,\n        projection_pushdown: bool = True,\n        predicate_pushdown: bool = True,\n        use_zero_based: Optional[bool] = None,\n    ) -&gt; pl.DataFrame:\n        \"\"\"\n        Read a CRAM file into a DataFrame.\n\n        !!! hint \"Parallelism &amp; Indexed Reads\"\n            Indexed parallel reads and predicate pushdown are automatic when a CRAI index\n            is present. See [File formats support](/polars-bio/features/#file-formats-support),\n            [Indexed reads](/polars-bio/features/#indexed-reads-predicate-pushdown),\n            and [Automatic parallel partitioning](/polars-bio/features/#automatic-parallel-partitioning) for details.\n\n        Parameters:\n            path: The path to the CRAM file (local or cloud storage: S3, GCS, Azure Blob).\n            reference_path: Optional path to external FASTA reference file (**local path only**, cloud storage not supported). If not provided, the CRAM file must contain embedded reference sequences. The FASTA file must have an accompanying index file (.fai) in the same directory. Create the index using: `samtools faidx reference.fasta`\n            tag_fields: List of CRAM tag names to include as columns (e.g., [\"NM\", \"MD\", \"AS\"]). If None, no optional tags are parsed (default). Common tags include: NM (edit distance), MD (mismatch string), AS (alignment score), XS (secondary alignment score), RG (read group), CB (cell barcode), UB (UMI barcode).\n            chunk_size: The size in MB of a chunk when reading from an object store. The default is 8 MB. For large scale operations, it is recommended to increase this value to 64.\n            concurrent_fetches: [GCS] The number of concurrent fetches when reading from an object store. The default is 1. For large scale operations, it is recommended to increase this value to 8 or even more.\n            allow_anonymous: [GCS, AWS S3] Whether to allow anonymous access to object storage.\n            enable_request_payer: [AWS S3] Whether to enable request payer for object storage. This is useful for reading files from AWS S3 buckets that require request payer.\n            max_retries: The maximum number of retries for reading the file from object storage.\n            timeout: The timeout in seconds for reading the file from object storage.\n            projection_pushdown: Enable column projection pushdown optimization. When True, only requested columns are processed at the DataFusion execution level, improving performance and reducing memory usage.\n            predicate_pushdown: Enable predicate pushdown using index files (CRAI) for efficient region-based filtering. Index files are auto-discovered (e.g., `file.cram.crai`). Only simple predicates are pushed down (equality, comparisons, IN); complex predicates like `.str.contains()` or OR logic are filtered client-side. Correctness is always guaranteed.\n            use_zero_based: If True, output 0-based half-open coordinates. If False, output 1-based closed coordinates. If None (default), uses the global configuration `datafusion.bio.coordinate_system_zero_based`.\n\n        !!! note\n            By default, coordinates are output in **1-based closed** format. Use `use_zero_based=True` or set `pb.set_option(pb.POLARS_BIO_COORDINATE_SYSTEM_ZERO_BASED, True)` for 0-based half-open coordinates.\n\n        !!! warning \"Known Limitation: MD and NM Tags\"\n            Due to a limitation in the underlying noodles-cram library, **MD (mismatch descriptor) and NM (edit distance) tags are not accessible** from CRAM files, even when stored in the file. These tags can be seen with samtools but are not exposed through the noodles-cram record.data() interface.\n\n            Other optional tags (RG, MQ, AM, OQ, etc.) work correctly. This issue is tracked at: https://github.com/biodatageeks/datafusion-bio-formats/issues/54\n\n            **Workaround**: Use BAM format if MD/NM tags are required for your analysis.\n\n        !!! example \"Using External Reference\"\n            ```python\n            import polars_bio as pb\n\n            # Read CRAM with external reference\n            df = pb.read_cram(\n                \"/path/to/file.cram\",\n                reference_path=\"/path/to/reference.fasta\"\n            )\n            ```\n\n        !!! example \"Public CRAM File Example\"\n            Download and read a public CRAM file from 42basepairs:\n            ```bash\n            # Download the CRAM file and reference\n            wget https://42basepairs.com/download/s3/gatk-test-data/wgs_cram/NA12878_20k_hg38/NA12878.cram\n            wget https://storage.googleapis.com/genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.fasta\n\n            # Create FASTA index (required)\n            samtools faidx Homo_sapiens_assembly38.fasta\n            ```\n\n            ```python\n            import polars_bio as pb\n\n            # Read first 5 reads from the CRAM file\n            df = pb.scan_cram(\n                \"NA12878.cram\",\n                reference_path=\"Homo_sapiens_assembly38.fasta\"\n            ).limit(5).collect()\n\n            print(df.select([\"name\", \"chrom\", \"start\", \"end\", \"cigar\"]))\n            ```\n\n        !!! example \"Creating CRAM with Embedded Reference\"\n            To create a CRAM file with embedded reference using samtools:\n            ```bash\n            samtools view -C -o output.cram --output-fmt-option embed_ref=1 input.bam\n            ```\n\n        Returns:\n            A Polars DataFrame with the following schema:\n                - name: Read name (String)\n                - chrom: Chromosome/contig name (String)\n                - start: Alignment start position, 1-based (UInt32)\n                - end: Alignment end position, 1-based (UInt32)\n                - flags: SAM flags (UInt32)\n                - cigar: CIGAR string (String)\n                - mapping_quality: Mapping quality (UInt32)\n                - mate_chrom: Mate chromosome/contig name (String)\n                - mate_start: Mate alignment start position, 1-based (UInt32)\n                - sequence: Read sequence (String)\n                - quality_scores: Base quality scores (String)\n        \"\"\"\n        lf = IOOperations.scan_cram(\n            path,\n            reference_path,\n            tag_fields,\n            chunk_size,\n            concurrent_fetches,\n            allow_anonymous,\n            enable_request_payer,\n            max_retries,\n            timeout,\n            projection_pushdown,\n            predicate_pushdown,\n            use_zero_based,\n        )\n        # Get metadata before collecting (polars-config-meta doesn't preserve through collect)\n        zero_based = lf.config_meta.get_metadata().get(\"coordinate_system_zero_based\")\n        df = lf.collect()\n        # Set metadata on the collected DataFrame\n        if zero_based is not None:\n            set_coordinate_system(df, zero_based)\n        return df\n\n    @staticmethod\n    def scan_cram(\n        path: str,\n        reference_path: str = None,\n        tag_fields: Union[list[str], None] = None,\n        chunk_size: int = 8,\n        concurrent_fetches: int = 1,\n        allow_anonymous: bool = True,\n        enable_request_payer: bool = False,\n        max_retries: int = 5,\n        timeout: int = 300,\n        projection_pushdown: bool = True,\n        predicate_pushdown: bool = True,\n        use_zero_based: Optional[bool] = None,\n    ) -&gt; pl.LazyFrame:\n        \"\"\"\n        Lazily read a CRAM file into a LazyFrame.\n\n        !!! hint \"Parallelism &amp; Indexed Reads\"\n            Indexed parallel reads and predicate pushdown are automatic when a CRAI index\n            is present. See [File formats support](/polars-bio/features/#file-formats-support),\n            [Indexed reads](/polars-bio/features/#indexed-reads-predicate-pushdown),\n            and [Automatic parallel partitioning](/polars-bio/features/#automatic-parallel-partitioning) for details.\n\n        Parameters:\n            path: The path to the CRAM file (local or cloud storage: S3, GCS, Azure Blob).\n            reference_path: Optional path to external FASTA reference file (**local path only**, cloud storage not supported). If not provided, the CRAM file must contain embedded reference sequences. The FASTA file must have an accompanying index file (.fai) in the same directory. Create the index using: `samtools faidx reference.fasta`\n            tag_fields: List of CRAM tag names to include as columns (e.g., [\"NM\", \"MD\", \"AS\"]). If None, no optional tags are parsed (default). Common tags include: NM (edit distance), MD (mismatch string), AS (alignment score), XS (secondary alignment score), RG (read group), CB (cell barcode), UB (UMI barcode).\n            chunk_size: The size in MB of a chunk when reading from an object store. The default is 8 MB. For large scale operations, it is recommended to increase this value to 64.\n            concurrent_fetches: [GCS] The number of concurrent fetches when reading from an object store. The default is 1. For large scale operations, it is recommended to increase this value to 8 or even more.\n            allow_anonymous: [GCS, AWS S3] Whether to allow anonymous access to object storage.\n            enable_request_payer: [AWS S3] Whether to enable request payer for object storage. This is useful for reading files from AWS S3 buckets that require request payer.\n            max_retries: The maximum number of retries for reading the file from object storage.\n            timeout: The timeout in seconds for reading the file from object storage.\n            projection_pushdown: Enable column projection pushdown optimization. When True, only requested columns are processed at the DataFusion execution level, improving performance and reducing memory usage.\n            predicate_pushdown: Enable predicate pushdown using index files (CRAI) for efficient region-based filtering. Index files are auto-discovered (e.g., `file.cram.crai`). Only simple predicates are pushed down (equality, comparisons, IN); complex predicates like `.str.contains()` or OR logic are filtered client-side. Correctness is always guaranteed.\n            use_zero_based: If True, output 0-based half-open coordinates. If False, output 1-based closed coordinates. If None (default), uses the global configuration `datafusion.bio.coordinate_system_zero_based`.\n\n        !!! note\n            By default, coordinates are output in **1-based closed** format. Use `use_zero_based=True` or set `pb.set_option(pb.POLARS_BIO_COORDINATE_SYSTEM_ZERO_BASED, True)` for 0-based half-open coordinates.\n\n        !!! warning \"Known Limitation: MD and NM Tags\"\n            Due to a limitation in the underlying noodles-cram library, **MD (mismatch descriptor) and NM (edit distance) tags are not accessible** from CRAM files, even when stored in the file. These tags can be seen with samtools but are not exposed through the noodles-cram record.data() interface.\n\n            Other optional tags (RG, MQ, AM, OQ, etc.) work correctly. This issue is tracked at: https://github.com/biodatageeks/datafusion-bio-formats/issues/54\n\n            **Workaround**: Use BAM format if MD/NM tags are required for your analysis.\n\n        !!! example \"Using External Reference\"\n            ```python\n            import polars_bio as pb\n\n            # Lazy scan CRAM with external reference\n            lf = pb.scan_cram(\n                \"/path/to/file.cram\",\n                reference_path=\"/path/to/reference.fasta\"\n            )\n\n            # Apply transformations and collect\n            df = lf.filter(pl.col(\"chrom\") == \"chr1\").collect()\n            ```\n\n        !!! example \"Public CRAM File Example\"\n            Download and read a public CRAM file from 42basepairs:\n            ```bash\n            # Download the CRAM file and reference\n            wget https://42basepairs.com/download/s3/gatk-test-data/wgs_cram/NA12878_20k_hg38/NA12878.cram\n            wget https://storage.googleapis.com/genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.fasta\n\n            # Create FASTA index (required)\n            samtools faidx Homo_sapiens_assembly38.fasta\n            ```\n\n            ```python\n            import polars_bio as pb\n            import polars as pl\n\n            # Lazy scan and filter for chromosome 20 reads\n            df = pb.scan_cram(\n                \"NA12878.cram\",\n                reference_path=\"Homo_sapiens_assembly38.fasta\"\n            ).filter(\n                pl.col(\"chrom\") == \"chr20\"\n            ).select(\n                [\"name\", \"chrom\", \"start\", \"end\", \"mapping_quality\"]\n            ).limit(10).collect()\n\n            print(df)\n            ```\n\n        !!! example \"Creating CRAM with Embedded Reference\"\n            To create a CRAM file with embedded reference using samtools:\n            ```bash\n            samtools view -C -o output.cram --output-fmt-option embed_ref=1 input.bam\n            ```\n\n        Returns:\n            A Polars LazyFrame with the following schema:\n                - name: Read name (String)\n                - chrom: Chromosome/contig name (String)\n                - start: Alignment start position, 1-based (UInt32)\n                - end: Alignment end position, 1-based (UInt32)\n                - flags: SAM flags (UInt32)\n                - cigar: CIGAR string (String)\n                - mapping_quality: Mapping quality (UInt32)\n                - mate_chrom: Mate chromosome/contig name (String)\n                - mate_start: Mate alignment start position, 1-based (UInt32)\n                - sequence: Read sequence (String)\n                - quality_scores: Base quality scores (String)\n        \"\"\"\n        object_storage_options = PyObjectStorageOptions(\n            allow_anonymous=allow_anonymous,\n            enable_request_payer=enable_request_payer,\n            chunk_size=chunk_size,\n            concurrent_fetches=concurrent_fetches,\n            max_retries=max_retries,\n            timeout=timeout,\n            compression_type=\"auto\",\n        )\n\n        zero_based = _resolve_zero_based(use_zero_based)\n        cram_read_options = CramReadOptions(\n            reference_path=reference_path,\n            object_storage_options=object_storage_options,\n            zero_based=zero_based,\n            tag_fields=tag_fields,\n        )\n        read_options = ReadOptions(cram_read_options=cram_read_options)\n        return _read_file(\n            path,\n            InputFormat.Cram,\n            read_options,\n            projection_pushdown,\n            predicate_pushdown,\n            zero_based=zero_based,\n        )\n\n    @staticmethod\n    def describe_bam(\n        path: str,\n        sample_size: int = 100,\n        chunk_size: int = 8,\n        concurrent_fetches: int = 1,\n        allow_anonymous: bool = True,\n        enable_request_payer: bool = False,\n        max_retries: int = 5,\n        timeout: int = 300,\n        compression_type: str = \"auto\",\n        use_zero_based: Optional[bool] = None,\n    ) -&gt; pl.DataFrame:\n        \"\"\"\n        Get schema information for a BAM file with automatic tag discovery.\n\n        Samples the first N records to discover all available tags and their types.\n        Returns detailed schema information including column names, data types,\n        nullability, category (standard/tag), SAM type, and descriptions.\n\n        Parameters:\n            path: The path to the BAM file.\n            sample_size: Number of records to sample for tag discovery (default: 100).\n                Use higher values for more comprehensive tag discovery.\n            chunk_size: The size in MB of a chunk when reading from object storage.\n            concurrent_fetches: The number of concurrent fetches when reading from object storage.\n            allow_anonymous: Whether to allow anonymous access to object storage.\n            enable_request_payer: Whether to enable request payer for object storage.\n            max_retries: The maximum number of retries for reading the file.\n            timeout: The timeout in seconds for reading the file.\n            compression_type: The compression type of the file. If \"auto\" (default), compression is detected automatically.\n            use_zero_based: If True, output 0-based coordinates. If False, 1-based coordinates.\n\n        Returns:\n            DataFrame with columns:\n            - column_name: Name of the column/field\n            - data_type: Arrow data type (e.g., \"Utf8\", \"Int32\")\n            - nullable: Whether the field can be null\n            - category: \"core\" for fixed columns, \"tag\" for optional SAM tags\n            - sam_type: SAM type code (e.g., \"Z\", \"i\") for tags, null for core columns\n            - description: Human-readable description of the field\n\n        Example:\n            ```python\n            import polars_bio as pb\n\n            # Auto-discover all tags present in the file\n            schema = pb.describe_bam(\"file.bam\", sample_size=100)\n            print(schema)\n            # Output:\n            # shape: (15, 6)\n            # \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n            # \u2502 column_name \u2506 data_type \u2506 nullable \u2506 category \u2506 sam_type \u2506 description          \u2502\n            # \u2502 ---         \u2506 ---       \u2506 ---      \u2506 ---      \u2506 ---      \u2506 ---                  \u2502\n            # \u2502 str         \u2506 str       \u2506 bool     \u2506 str      \u2506 str      \u2506 str                  \u2502\n            # \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n            # \u2502 name        \u2506 Utf8      \u2506 true     \u2506 core     \u2506 null     \u2506 Query name           \u2502\n            # \u2502 chrom       \u2506 Utf8      \u2506 true     \u2506 core     \u2506 null     \u2506 Reference name       \u2502\n            # \u2502 ...         \u2506 ...       \u2506 ...      \u2506 ...      \u2506 ...      \u2506 ...                  \u2502\n            # \u2502 NM          \u2506 Int32     \u2506 true     \u2506 tag      \u2506 i        \u2506 Edit distance        \u2502\n            # \u2502 AS          \u2506 Int32     \u2506 true     \u2506 tag      \u2506 i        \u2506 Alignment score      \u2502\n            # \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n            ```\n        \"\"\"\n        # Build object storage options\n        object_storage_options = PyObjectStorageOptions(\n            chunk_size=chunk_size,\n            concurrent_fetches=concurrent_fetches,\n            allow_anonymous=allow_anonymous,\n            enable_request_payer=enable_request_payer,\n            max_retries=max_retries,\n            timeout=timeout,\n            compression_type=compression_type,\n        )\n\n        # Resolve zero_based setting\n        zero_based = _resolve_zero_based(use_zero_based)\n\n        # Call Rust function with tag auto-discovery (tag_fields=None)\n        df = py_describe_bam(\n            ctx,  # PyBioSessionContext\n            path,\n            object_storage_options,\n            zero_based,\n            None,  # tag_fields=None enables auto-discovery\n            sample_size,\n        )\n\n        # Convert DataFusion DataFrame to Polars DataFrame\n        return pl.from_arrow(df.to_arrow_table())\n\n    @staticmethod\n    def describe_cram(\n        path: str,\n        reference_path: str = None,\n        sample_size: int = 100,\n        chunk_size: int = 8,\n        concurrent_fetches: int = 1,\n        allow_anonymous: bool = True,\n        enable_request_payer: bool = False,\n        max_retries: int = 5,\n        timeout: int = 300,\n        compression_type: str = \"auto\",\n        use_zero_based: Optional[bool] = None,\n    ) -&gt; pl.DataFrame:\n        \"\"\"\n        Get schema information for a CRAM file with automatic tag discovery.\n\n        Samples the first N records to discover all available tags and their types.\n        Returns detailed schema information including column names, data types,\n        nullability, category (core/tag), SAM type, and descriptions.\n\n        Parameters:\n            path: The path to the CRAM file.\n            reference_path: Optional path to external FASTA reference file.\n            sample_size: Number of records to sample for tag discovery (default: 100).\n            chunk_size: The size in MB of a chunk when reading from object storage.\n            concurrent_fetches: The number of concurrent fetches when reading from object storage.\n            allow_anonymous: Whether to allow anonymous access to object storage.\n            enable_request_payer: Whether to enable request payer for object storage.\n            max_retries: The maximum number of retries for reading the file.\n            timeout: The timeout in seconds for reading the file.\n            compression_type: The compression type of the file. If \"auto\" (default), compression is detected automatically.\n            use_zero_based: If True, output 0-based coordinates. If False, 1-based coordinates.\n\n        Returns:\n            DataFrame with columns:\n            - column_name: Name of the column/field\n            - data_type: Arrow data type (e.g., \"Utf8\", \"Int32\")\n            - nullable: Whether the field can be null\n            - category: \"core\" for fixed columns, \"tag\" for optional SAM tags\n            - sam_type: SAM type code (e.g., \"Z\", \"i\") for tags, null for core columns\n            - description: Human-readable description of the field\n\n        !!! warning \"Known Limitation: MD and NM Tags\"\n            Due to a limitation in the underlying noodles-cram library, **MD (mismatch descriptor) and NM (edit distance) tags are not discoverable** from CRAM files, even when stored. Automatic tag discovery will not include MD/NM tags. Other optional tags (RG, MQ, AM, OQ, etc.) are discovered correctly. See: https://github.com/biodatageeks/datafusion-bio-formats/issues/54\n\n        Example:\n            ```python\n            import polars_bio as pb\n\n            # Auto-discover all tags present in the file\n            schema = pb.describe_cram(\"file.cram\", sample_size=100)\n            print(schema)\n\n            # Filter to see only tag columns\n            tags = schema.filter(schema[\"category\"] == \"tag\")\n            print(tags[\"column_name\"])\n            ```\n        \"\"\"\n        # Build object storage options\n        object_storage_options = PyObjectStorageOptions(\n            chunk_size=chunk_size,\n            concurrent_fetches=concurrent_fetches,\n            allow_anonymous=allow_anonymous,\n            enable_request_payer=enable_request_payer,\n            max_retries=max_retries,\n            timeout=timeout,\n            compression_type=compression_type,\n        )\n\n        # Resolve zero_based setting\n        zero_based = _resolve_zero_based(use_zero_based)\n\n        # Call Rust function with tag auto-discovery (tag_fields=None)\n        df = py_describe_cram(\n            ctx,\n            path,\n            reference_path,\n            object_storage_options,\n            zero_based,\n            None,  # tag_fields=None enables auto-discovery\n            sample_size,\n        )\n\n        # Convert DataFusion DataFrame to Polars DataFrame\n        return pl.from_arrow(df.to_arrow_table())\n\n    @staticmethod\n    def read_fastq(\n        path: str,\n        chunk_size: int = 8,\n        concurrent_fetches: int = 1,\n        allow_anonymous: bool = True,\n        enable_request_payer: bool = False,\n        max_retries: int = 5,\n        timeout: int = 300,\n        compression_type: str = \"auto\",\n        projection_pushdown: bool = True,\n    ) -&gt; pl.DataFrame:\n        \"\"\"\n        Read a FASTQ file into a DataFrame.\n\n        !!! hint \"Parallelism &amp; Compression\"\n            See [File formats support](/polars-bio/features/#file-formats-support),\n            [Compression](/polars-bio/features/#compression),\n            and [Automatic parallel partitioning](/polars-bio/features/#automatic-parallel-partitioning) for details on parallel reads and supported compression types.\n\n        Parameters:\n            path: The path to the FASTQ file.\n            chunk_size: The size in MB of a chunk when reading from an object store. The default is 8 MB. For large scale operations, it is recommended to increase this value to 64.\n            concurrent_fetches: [GCS] The number of concurrent fetches when reading from an object store. The default is 1. For large scale operations, it is recommended to increase this value to 8 or even more.\n            allow_anonymous: [GCS, AWS S3] Whether to allow anonymous access to object storage.\n            enable_request_payer: [AWS S3] Whether to enable request payer for object storage. This is useful for reading files from AWS S3 buckets that require request payer.\n            max_retries:  The maximum number of retries for reading the file from object storage.\n            timeout: The timeout in seconds for reading the file from object storage.\n            compression_type: The compression type of the FASTQ file. If not specified, it will be detected automatically based on the file extension. BGZF and GZIP compressions are supported ('bgz', 'gz').\n            projection_pushdown: Enable column projection pushdown to optimize query performance by only reading the necessary columns at the DataFusion level.\n        \"\"\"\n        return IOOperations.scan_fastq(\n            path,\n            chunk_size,\n            concurrent_fetches,\n            allow_anonymous,\n            enable_request_payer,\n            max_retries,\n            timeout,\n            compression_type,\n            projection_pushdown,\n        ).collect()\n\n    @staticmethod\n    def scan_fastq(\n        path: str,\n        chunk_size: int = 8,\n        concurrent_fetches: int = 1,\n        allow_anonymous: bool = True,\n        enable_request_payer: bool = False,\n        max_retries: int = 5,\n        timeout: int = 300,\n        compression_type: str = \"auto\",\n        projection_pushdown: bool = True,\n    ) -&gt; pl.LazyFrame:\n        \"\"\"\n        Lazily read a FASTQ file into a LazyFrame.\n\n        !!! hint \"Parallelism &amp; Compression\"\n            See [File formats support](/polars-bio/features/#file-formats-support),\n            [Compression](/polars-bio/features/#compression),\n            and [Automatic parallel partitioning](/polars-bio/features/#automatic-parallel-partitioning) for details on parallel reads and supported compression types.\n\n        Parameters:\n            path: The path to the FASTQ file.\n            chunk_size: The size in MB of a chunk when reading from an object store. The default is 8 MB. For large scale operations, it is recommended to increase this value to 64.\n            concurrent_fetches: [GCS] The number of concurrent fetches when reading from an object store. The default is 1. For large scale operations, it is recommended to increase this value to 8 or even more.\n            allow_anonymous: [GCS, AWS S3] Whether to allow anonymous access to object storage.\n            enable_request_payer: [AWS S3] Whether to enable request payer for object storage. This is useful for reading files from AWS S3 buckets that require request payer.\n            max_retries:  The maximum number of retries for reading the file from object storage.\n            timeout: The timeout in seconds for reading the file from object storage.\n            compression_type: The compression type of the FASTQ file. If not specified, it will be detected automatically based on the file extension. BGZF and GZIP compressions are supported ('bgz', 'gz').\n            projection_pushdown: Enable column projection pushdown to optimize query performance by only reading the necessary columns at the DataFusion level.\n        \"\"\"\n        object_storage_options = PyObjectStorageOptions(\n            allow_anonymous=allow_anonymous,\n            enable_request_payer=enable_request_payer,\n            chunk_size=chunk_size,\n            concurrent_fetches=concurrent_fetches,\n            max_retries=max_retries,\n            timeout=timeout,\n            compression_type=compression_type,\n        )\n\n        fastq_read_options = FastqReadOptions(\n            object_storage_options=object_storage_options,\n        )\n        read_options = ReadOptions(fastq_read_options=fastq_read_options)\n        return _read_file(path, InputFormat.Fastq, read_options, projection_pushdown)\n\n    @staticmethod\n    def read_pairs(\n        path: str,\n        chunk_size: int = 8,\n        concurrent_fetches: int = 1,\n        allow_anonymous: bool = True,\n        enable_request_payer: bool = False,\n        max_retries: int = 5,\n        timeout: int = 300,\n        compression_type: str = \"auto\",\n        projection_pushdown: bool = True,\n        predicate_pushdown: bool = True,\n        use_zero_based: Optional[bool] = None,\n    ) -&gt; pl.DataFrame:\n        \"\"\"\n        Read a Pairs (Hi-C) file into a DataFrame.\n\n        The Pairs format (4DN project) stores chromatin contact data with columns:\n        readID, chr1, pos1, chr2, pos2, strand1, strand2.\n\n        !!! hint \"Parallelism &amp; Indexed Reads\"\n            Indexed parallel reads and predicate pushdown are automatic when a TBI index\n            is present. See [File formats support](/polars-bio/features/#file-formats-support)\n            and [Indexed reads](/polars-bio/features/#indexed-reads-predicate-pushdown) for details.\n\n        Parameters:\n            path: The path to the Pairs file (.pairs, .pairs.gz, .pairs.bgz).\n            chunk_size: The size in MB of a chunk when reading from an object store.\n            concurrent_fetches: The number of concurrent fetches when reading from an object store.\n            allow_anonymous: Whether to allow anonymous access to object storage.\n            enable_request_payer: Whether to enable request payer for object storage.\n            max_retries: The maximum number of retries for reading the file from object storage.\n            timeout: The timeout in seconds for reading the file from object storage.\n            compression_type: The compression type. If not specified, it will be detected automatically.\n            projection_pushdown: Enable column projection pushdown to optimize query performance.\n            predicate_pushdown: Enable predicate pushdown using index files (TBI) for efficient region-based filtering. Index files are auto-discovered (e.g., `file.pairs.gz.tbi`). Only simple predicates are pushed down (equality, comparisons, IN); complex predicates are filtered client-side. Correctness is always guaranteed.\n            use_zero_based: If True, output 0-based half-open coordinates. If False, output 1-based closed coordinates. If None (default), uses the global configuration `datafusion.bio.coordinate_system_zero_based`.\n\n        !!! note\n            By default, coordinates are output in **1-based closed** format. Use `use_zero_based=True` or set `pb.set_option(pb.POLARS_BIO_COORDINATE_SYSTEM_ZERO_BASED, True)` for 0-based half-open coordinates.\n        \"\"\"\n        lf = IOOperations.scan_pairs(\n            path,\n            chunk_size,\n            concurrent_fetches,\n            allow_anonymous,\n            enable_request_payer,\n            max_retries,\n            timeout,\n            compression_type,\n            projection_pushdown,\n            predicate_pushdown,\n            use_zero_based,\n        )\n        zero_based = lf.config_meta.get_metadata().get(\"coordinate_system_zero_based\")\n        df = lf.collect()\n        if zero_based is not None:\n            set_coordinate_system(df, zero_based)\n        return df\n\n    @staticmethod\n    def scan_pairs(\n        path: str,\n        chunk_size: int = 8,\n        concurrent_fetches: int = 1,\n        allow_anonymous: bool = True,\n        enable_request_payer: bool = False,\n        max_retries: int = 5,\n        timeout: int = 300,\n        compression_type: str = \"auto\",\n        projection_pushdown: bool = True,\n        predicate_pushdown: bool = True,\n        use_zero_based: Optional[bool] = None,\n    ) -&gt; pl.LazyFrame:\n        \"\"\"\n        Lazily read a Pairs (Hi-C) file into a LazyFrame.\n\n        The Pairs format (4DN project) stores chromatin contact data with columns:\n        readID, chr1, pos1, chr2, pos2, strand1, strand2.\n\n        !!! hint \"Parallelism &amp; Indexed Reads\"\n            Indexed parallel reads and predicate pushdown are automatic when a TBI index\n            is present. See [File formats support](/polars-bio/features/#file-formats-support)\n            and [Indexed reads](/polars-bio/features/#indexed-reads-predicate-pushdown) for details.\n\n        Parameters:\n            path: The path to the Pairs file (.pairs, .pairs.gz, .pairs.bgz).\n            chunk_size: The size in MB of a chunk when reading from an object store.\n            concurrent_fetches: The number of concurrent fetches when reading from an object store.\n            allow_anonymous: Whether to allow anonymous access to object storage.\n            enable_request_payer: Whether to enable request payer for object storage.\n            max_retries: The maximum number of retries for reading the file from object storage.\n            timeout: The timeout in seconds for reading the file from object storage.\n            compression_type: The compression type. If not specified, it will be detected automatically.\n            projection_pushdown: Enable column projection pushdown to optimize query performance.\n            predicate_pushdown: Enable predicate pushdown using index files (TBI) for efficient region-based filtering. Index files are auto-discovered (e.g., `file.pairs.gz.tbi`). Only simple predicates are pushed down (equality, comparisons, IN); complex predicates are filtered client-side. Correctness is always guaranteed.\n            use_zero_based: If True, output 0-based half-open coordinates. If False, output 1-based closed coordinates. If None (default), uses the global configuration `datafusion.bio.coordinate_system_zero_based`.\n\n        !!! note\n            By default, coordinates are output in **1-based closed** format. Use `use_zero_based=True` or set `pb.set_option(pb.POLARS_BIO_COORDINATE_SYSTEM_ZERO_BASED, True)` for 0-based half-open coordinates.\n        \"\"\"\n        object_storage_options = PyObjectStorageOptions(\n            allow_anonymous=allow_anonymous,\n            enable_request_payer=enable_request_payer,\n            chunk_size=chunk_size,\n            concurrent_fetches=concurrent_fetches,\n            max_retries=max_retries,\n            timeout=timeout,\n            compression_type=compression_type,\n        )\n\n        zero_based = _resolve_zero_based(use_zero_based)\n        pairs_read_options = PairsReadOptions(\n            object_storage_options=object_storage_options,\n            zero_based=zero_based,\n        )\n        read_options = ReadOptions(pairs_read_options=pairs_read_options)\n        return _read_file(\n            path,\n            InputFormat.Pairs,\n            read_options,\n            projection_pushdown,\n            predicate_pushdown,\n            zero_based=zero_based,\n        )\n\n    @staticmethod\n    def read_bed(\n        path: str,\n        chunk_size: int = 8,\n        concurrent_fetches: int = 1,\n        allow_anonymous: bool = True,\n        enable_request_payer: bool = False,\n        max_retries: int = 5,\n        timeout: int = 300,\n        compression_type: str = \"auto\",\n        projection_pushdown: bool = True,\n        use_zero_based: Optional[bool] = None,\n    ) -&gt; pl.DataFrame:\n        \"\"\"\n        Read a BED file into a DataFrame.\n\n        Parameters:\n            path: The path to the BED file.\n            chunk_size: The size in MB of a chunk when reading from an object store. The default is 8 MB. For large scale operations, it is recommended to increase this value to 64.\n            concurrent_fetches: [GCS] The number of concurrent fetches when reading from an object store. The default is 1. For large scale operations, it is recommended to increase this value to 8 or even more.\n            allow_anonymous: [GCS, AWS S3] Whether to allow anonymous access to object storage.\n            enable_request_payer: [AWS S3] Whether to enable request payer for object storage. This is useful for reading files from AWS S3 buckets that require request payer.\n            max_retries:  The maximum number of retries for reading the file from object storage.\n            timeout: The timeout in seconds for reading the file from object storage.\n            compression_type: The compression type of the BED file. If not specified, it will be detected automatically based on the file extension. BGZF compressions is supported ('bgz').\n            projection_pushdown: Enable column projection pushdown to optimize query performance by only reading the necessary columns at the DataFusion level.\n            use_zero_based: If True, output 0-based half-open coordinates. If False, output 1-based closed coordinates. If None (default), uses the global configuration `datafusion.bio.coordinate_system_zero_based`.\n\n        !!! Note\n            Only **BED4** format is supported. It extends the basic BED format (BED3) by adding a name field, resulting in four columns: chromosome, start position, end position, and name.\n            Also unlike other text formats, **GZIP** compression is not supported.\n\n        !!! note\n            By default, coordinates are output in **1-based closed** format. Use `use_zero_based=True` or set `pb.set_option(pb.POLARS_BIO_COORDINATE_SYSTEM_ZERO_BASED, True)` for 0-based half-open coordinates.\n        \"\"\"\n        lf = IOOperations.scan_bed(\n            path,\n            chunk_size,\n            concurrent_fetches,\n            allow_anonymous,\n            enable_request_payer,\n            max_retries,\n            timeout,\n            compression_type,\n            projection_pushdown,\n            use_zero_based,\n        )\n        # Get metadata before collecting (polars-config-meta doesn't preserve through collect)\n        zero_based = lf.config_meta.get_metadata().get(\"coordinate_system_zero_based\")\n        df = lf.collect()\n        # Set metadata on the collected DataFrame\n        if zero_based is not None:\n            set_coordinate_system(df, zero_based)\n        return df\n\n    @staticmethod\n    def scan_bed(\n        path: str,\n        chunk_size: int = 8,\n        concurrent_fetches: int = 1,\n        allow_anonymous: bool = True,\n        enable_request_payer: bool = False,\n        max_retries: int = 5,\n        timeout: int = 300,\n        compression_type: str = \"auto\",\n        projection_pushdown: bool = True,\n        use_zero_based: Optional[bool] = None,\n    ) -&gt; pl.LazyFrame:\n        \"\"\"\n        Lazily read a BED file into a LazyFrame.\n\n        Parameters:\n            path: The path to the BED file.\n            chunk_size: The size in MB of a chunk when reading from an object store. The default is 8 MB. For large scale operations, it is recommended to increase this value to 64.\n            concurrent_fetches: [GCS] The number of concurrent fetches when reading from an object store. The default is 1. For large scale operations, it is recommended to increase this value to 8 or even more.\n            allow_anonymous: [GCS, AWS S3] Whether to allow anonymous access to object storage.\n            enable_request_payer: [AWS S3] Whether to enable request payer for object storage. This is useful for reading files from AWS S3 buckets that require request payer.\n            max_retries:  The maximum number of retries for reading the file from object storage.\n            timeout: The timeout in seconds for reading the file from object storage.\n            compression_type: The compression type of the BED file. If not specified, it will be detected automatically based on the file extension. BGZF compressions is supported ('bgz').\n            projection_pushdown: Enable column projection pushdown to optimize query performance by only reading the necessary columns at the DataFusion level.\n            use_zero_based: If True, output 0-based half-open coordinates. If False, output 1-based closed coordinates. If None (default), uses the global configuration `datafusion.bio.coordinate_system_zero_based`.\n\n        !!! Note\n            Only **BED4** format is supported. It extends the basic BED format (BED3) by adding a name field, resulting in four columns: chromosome, start position, end position, and name.\n            Also unlike other text formats, **GZIP** compression is not supported.\n\n        !!! note\n            By default, coordinates are output in **1-based closed** format. Use `use_zero_based=True` or set `pb.set_option(pb.POLARS_BIO_COORDINATE_SYSTEM_ZERO_BASED, True)` for 0-based half-open coordinates.\n        \"\"\"\n        object_storage_options = PyObjectStorageOptions(\n            allow_anonymous=allow_anonymous,\n            enable_request_payer=enable_request_payer,\n            chunk_size=chunk_size,\n            concurrent_fetches=concurrent_fetches,\n            max_retries=max_retries,\n            timeout=timeout,\n            compression_type=compression_type,\n        )\n\n        zero_based = _resolve_zero_based(use_zero_based)\n        bed_read_options = BedReadOptions(\n            object_storage_options=object_storage_options,\n            zero_based=zero_based,\n        )\n        read_options = ReadOptions(bed_read_options=bed_read_options)\n        return _read_file(\n            path,\n            InputFormat.Bed,\n            read_options,\n            projection_pushdown,\n            zero_based=zero_based,\n        )\n\n    @staticmethod\n    def read_table(path: str, schema: Dict = None, **kwargs) -&gt; pl.DataFrame:\n        \"\"\"\n         Read a tab-delimited (i.e. BED) file into a Polars DataFrame.\n         Tries to be compatible with Bioframe's [read_table](https://bioframe.readthedocs.io/en/latest/guide-io.html)\n         but faster. Schema should follow the Bioframe's schema [format](https://github.com/open2c/bioframe/blob/2b685eebef393c2c9e6220dcf550b3630d87518e/bioframe/io/schemas.py#L174).\n\n        Parameters:\n            path: The path to the file.\n            schema: Schema should follow the Bioframe's schema [format](https://github.com/open2c/bioframe/blob/2b685eebef393c2c9e6220dcf550b3630d87518e/bioframe/io/schemas.py#L174).\n        \"\"\"\n        return IOOperations.scan_table(path, schema, **kwargs).collect()\n\n    @staticmethod\n    def scan_table(path: str, schema: Dict = None, **kwargs) -&gt; pl.LazyFrame:\n        \"\"\"\n         Lazily read a tab-delimited (i.e. BED) file into a Polars LazyFrame.\n         Tries to be compatible with Bioframe's [read_table](https://bioframe.readthedocs.io/en/latest/guide-io.html)\n         but faster and lazy. Schema should follow the Bioframe's schema [format](https://github.com/open2c/bioframe/blob/2b685eebef393c2c9e6220dcf550b3630d87518e/bioframe/io/schemas.py#L174).\n\n        Parameters:\n            path: The path to the file.\n            schema: Schema should follow the Bioframe's schema [format](https://github.com/open2c/bioframe/blob/2b685eebef393c2c9e6220dcf550b3630d87518e/bioframe/io/schemas.py#L174).\n        \"\"\"\n        df = pl.scan_csv(path, separator=\"\\t\", has_header=False, **kwargs)\n        if schema is not None:\n            columns = SCHEMAS[schema]\n            if len(columns) != len(df.collect_schema()):\n                raise ValueError(\n                    f\"Schema incompatible with the input. Expected {len(columns)} columns in a schema, got {len(df.collect_schema())} in the input data file. Please provide a valid schema.\"\n                )\n            for i, c in enumerate(columns):\n                df = df.rename({f\"column_{i+1}\": c})\n        return df\n\n    @staticmethod\n    def describe_vcf(\n        path: str,\n        allow_anonymous: bool = True,\n        enable_request_payer: bool = False,\n        compression_type: str = \"auto\",\n    ) -&gt; pl.DataFrame:\n        \"\"\"\n        Describe VCF INFO schema.\n\n        Parameters:\n            path: The path to the VCF file.\n            allow_anonymous: Whether to allow anonymous access to object storage (GCS and S3 supported).\n            enable_request_payer: Whether to enable request payer for object storage. This is useful for reading files from AWS S3 buckets that require request payer.\n            compression_type: The compression type of the VCF file. If not specified, it will be detected automatically..\n        \"\"\"\n        object_storage_options = PyObjectStorageOptions(\n            allow_anonymous=allow_anonymous,\n            enable_request_payer=enable_request_payer,\n            chunk_size=8,\n            concurrent_fetches=1,\n            max_retries=1,\n            timeout=10,\n            compression_type=compression_type,\n        )\n        return py_describe_vcf(ctx, path, object_storage_options).to_polars()\n\n    @staticmethod\n    def from_polars(name: str, df: Union[pl.DataFrame, pl.LazyFrame]) -&gt; None:\n        \"\"\"\n        Register a Polars DataFrame as a DataFusion table.\n\n        Parameters:\n            name: The name of the table.\n            df: The Polars DataFrame.\n        \"\"\"\n        reader = (\n            df.to_arrow()\n            if isinstance(df, pl.DataFrame)\n            else df.collect().to_arrow().to_reader()\n        )\n        py_from_polars(ctx, name, reader)\n\n    @staticmethod\n    def write_vcf(\n        df: Union[pl.DataFrame, pl.LazyFrame],\n        path: str,\n    ) -&gt; int:\n        \"\"\"\n        Write a DataFrame to VCF format.\n\n        Coordinate system is automatically read from DataFrame metadata (set during\n        read_vcf). Compression is auto-detected from the file extension.\n\n        Parameters:\n            df: The DataFrame or LazyFrame to write.\n            path: The output file path. Compression is auto-detected from extension\n                  (.vcf.bgz for BGZF, .vcf.gz for GZIP, .vcf for uncompressed).\n\n        Returns:\n            The number of rows written.\n\n        !!! Example \"Writing VCF files\"\n            ```python\n            import polars_bio as pb\n\n            # Read a VCF file\n            df = pb.read_vcf(\"input.vcf\")\n\n            # Write to uncompressed VCF\n            pb.write_vcf(df, \"output.vcf\")\n\n            # Write to BGZF-compressed VCF\n            pb.write_vcf(df, \"output.vcf.bgz\")\n\n            # Write to GZIP-compressed VCF\n            pb.write_vcf(df, \"output.vcf.gz\")\n            ```\n        \"\"\"\n        return _write_file(df, path, OutputFormat.Vcf)\n\n    @staticmethod\n    def sink_vcf(\n        lf: pl.LazyFrame,\n        path: str,\n    ) -&gt; None:\n        \"\"\"\n        Streaming write a LazyFrame to VCF format.\n\n        This method executes the LazyFrame immediately and writes the results\n        to the specified path. Unlike `write_vcf`, it doesn't return the row count.\n\n        Coordinate system is automatically read from LazyFrame metadata (set during\n        scan_vcf). Compression is auto-detected from the file extension.\n\n        Parameters:\n            lf: The LazyFrame to write.\n            path: The output file path. Compression is auto-detected from extension\n                  (.vcf.bgz for BGZF, .vcf.gz for GZIP, .vcf for uncompressed).\n\n        !!! Example \"Streaming write VCF\"\n            ```python\n            import polars_bio as pb\n\n            # Lazy read and filter, then sink to VCF\n            lf = pb.scan_vcf(\"large_input.vcf\").filter(pl.col(\"qual\") &gt; 30)\n            pb.sink_vcf(lf, \"filtered_output.vcf.bgz\")\n            ```\n        \"\"\"\n        _write_file(lf, path, OutputFormat.Vcf)\n\n    @staticmethod\n    def write_fastq(\n        df: Union[pl.DataFrame, pl.LazyFrame],\n        path: str,\n    ) -&gt; int:\n        \"\"\"\n        Write a DataFrame to FASTQ format.\n\n        Compression is auto-detected from the file extension.\n\n        Parameters:\n            df: The DataFrame or LazyFrame to write. Must have columns:\n                - name: Read name/identifier\n                - sequence: DNA sequence\n                - quality_scores: Quality scores string\n                Optional: description (added after name on header line)\n            path: The output file path. Compression is auto-detected from extension\n                  (.fastq.bgz for BGZF, .fastq.gz for GZIP, .fastq for uncompressed).\n\n        Returns:\n            The number of rows written.\n\n        !!! Example \"Writing FASTQ files\"\n            ```python\n            import polars_bio as pb\n\n            # Read a FASTQ file\n            df = pb.read_fastq(\"input.fastq\")\n\n            # Write to uncompressed FASTQ\n            pb.write_fastq(df, \"output.fastq\")\n\n            # Write to GZIP-compressed FASTQ\n            pb.write_fastq(df, \"output.fastq.gz\")\n            ```\n        \"\"\"\n        return _write_file(df, path, OutputFormat.Fastq)\n\n    @staticmethod\n    def sink_fastq(\n        lf: pl.LazyFrame,\n        path: str,\n    ) -&gt; None:\n        \"\"\"\n        Streaming write a LazyFrame to FASTQ format.\n\n        Compression is auto-detected from the file extension.\n\n        Parameters:\n            lf: The LazyFrame to write.\n            path: The output file path. Compression is auto-detected from extension\n                  (.fastq.bgz for BGZF, .fastq.gz for GZIP, .fastq for uncompressed).\n\n        !!! Example \"Streaming write FASTQ\"\n            ```python\n            import polars_bio as pb\n\n            # Lazy read, filter by quality, then sink\n            lf = pb.scan_fastq(\"large_input.fastq.gz\")\n            pb.sink_fastq(lf.limit(1000), \"sample_output.fastq\")\n            ```\n        \"\"\"\n        _write_file(lf, path, OutputFormat.Fastq)\n\n    @staticmethod\n    def write_bam(\n        df: Union[pl.DataFrame, pl.LazyFrame],\n        path: str,\n        sort_on_write: bool = False,\n    ) -&gt; int:\n        \"\"\"\n        Write a DataFrame to BAM/SAM format.\n\n        Compression is auto-detected from file extension:\n        - .sam \u2192 Uncompressed SAM (plain text)\n        - .bam \u2192 BGZF-compressed BAM\n\n        For CRAM format, use `write_cram()` instead.\n\n        Parameters:\n            df: DataFrame or LazyFrame with 11 core BAM columns + optional tag columns\n            path: Output file path (.bam or .sam)\n            sort_on_write: If True, sort records by (chrom, start) and set header SO:coordinate.\n                If False (default), set header SO:unsorted.\n\n        Returns:\n            Number of rows written\n\n        !!! Example \"Write BAM files\"\n            ```python\n            import polars_bio as pb\n            df = pb.read_bam(\"input.bam\", tag_fields=[\"NM\", \"AS\"])\n            pb.write_bam(df, \"output.bam\")\n            pb.write_bam(df, \"output.sam\")\n            ```\n        \"\"\"\n        return _write_bam_file(\n            df, path, OutputFormat.Bam, None, sort_on_write=sort_on_write\n        )\n\n    @staticmethod\n    def sink_bam(\n        lf: pl.LazyFrame,\n        path: str,\n        sort_on_write: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Streaming write a LazyFrame to BAM/SAM format.\n\n        For CRAM format, use `sink_cram()` instead.\n\n        Parameters:\n            lf: LazyFrame to write\n            path: Output file path (.bam or .sam)\n            sort_on_write: If True, sort records by (chrom, start) and set header SO:coordinate.\n                If False (default), set header SO:unsorted.\n\n        !!! Example \"Streaming write BAM\"\n            ```python\n            import polars_bio as pb\n            lf = pb.scan_bam(\"input.bam\").filter(pl.col(\"mapping_quality\") &gt; 20)\n            pb.sink_bam(lf, \"filtered.bam\")\n            ```\n        \"\"\"\n        _write_bam_file(lf, path, OutputFormat.Bam, None, sort_on_write=sort_on_write)\n\n    @staticmethod\n    def read_sam(\n        path: str,\n        tag_fields: Union[list[str], None] = None,\n        projection_pushdown: bool = True,\n        use_zero_based: Optional[bool] = None,\n    ) -&gt; pl.DataFrame:\n        \"\"\"\n        Read a SAM file into a DataFrame.\n\n        SAM (Sequence Alignment/Map) is the plain-text counterpart of BAM.\n        This function reuses the BAM reader, which auto-detects the format\n        from the file extension.\n\n        Parameters:\n            path: The path to the SAM file.\n            tag_fields: List of SAM tag names to include as columns (e.g., [\"NM\", \"MD\", \"AS\"]).\n                If None, no optional tags are parsed (default).\n            projection_pushdown: Enable column projection pushdown to optimize query performance.\n            use_zero_based: If True, output 0-based half-open coordinates.\n                If False, output 1-based closed coordinates.\n                If None (default), uses the global configuration.\n\n        !!! note\n            By default, coordinates are output in **1-based closed** format.\n        \"\"\"\n        lf = IOOperations.scan_sam(\n            path,\n            tag_fields,\n            projection_pushdown,\n            use_zero_based,\n        )\n        zero_based = lf.config_meta.get_metadata().get(\"coordinate_system_zero_based\")\n        df = lf.collect()\n        if zero_based is not None:\n            set_coordinate_system(df, zero_based)\n        return df\n\n    @staticmethod\n    def scan_sam(\n        path: str,\n        tag_fields: Union[list[str], None] = None,\n        projection_pushdown: bool = True,\n        use_zero_based: Optional[bool] = None,\n    ) -&gt; pl.LazyFrame:\n        \"\"\"\n        Lazily read a SAM file into a LazyFrame.\n\n        SAM (Sequence Alignment/Map) is the plain-text counterpart of BAM.\n        This function reuses the BAM reader, which auto-detects the format\n        from the file extension.\n\n        Parameters:\n            path: The path to the SAM file.\n            tag_fields: List of SAM tag names to include as columns (e.g., [\"NM\", \"MD\", \"AS\"]).\n                If None, no optional tags are parsed (default).\n            projection_pushdown: Enable column projection pushdown to optimize query performance.\n            use_zero_based: If True, output 0-based half-open coordinates.\n                If False, output 1-based closed coordinates.\n                If None (default), uses the global configuration.\n\n        !!! note\n            By default, coordinates are output in **1-based closed** format.\n        \"\"\"\n        zero_based = _resolve_zero_based(use_zero_based)\n        bam_read_options = BamReadOptions(\n            zero_based=zero_based,\n            tag_fields=tag_fields,\n        )\n        read_options = ReadOptions(bam_read_options=bam_read_options)\n        return _read_file(\n            path,\n            InputFormat.Sam,\n            read_options,\n            projection_pushdown,\n            zero_based=zero_based,\n        )\n\n    @staticmethod\n    def describe_sam(\n        path: str,\n        sample_size: int = 100,\n        use_zero_based: Optional[bool] = None,\n    ) -&gt; pl.DataFrame:\n        \"\"\"\n        Get schema information for a SAM file with automatic tag discovery.\n\n        Samples the first N records to discover all available tags and their types.\n        Reuses the BAM describe logic, which auto-detects SAM from the file extension.\n\n        Parameters:\n            path: The path to the SAM file.\n            sample_size: Number of records to sample for tag discovery (default: 100).\n            use_zero_based: If True, output 0-based coordinates. If False, 1-based coordinates.\n\n        Returns:\n            DataFrame with columns: column_name, data_type, nullable, category, sam_type, description\n        \"\"\"\n        zero_based = _resolve_zero_based(use_zero_based)\n\n        df = py_describe_bam(\n            ctx,\n            path,\n            None,\n            zero_based,\n            None,\n            sample_size,\n        )\n\n        return pl.from_arrow(df.to_arrow_table())\n\n    @staticmethod\n    def write_sam(\n        df: Union[pl.DataFrame, pl.LazyFrame],\n        path: str,\n        sort_on_write: bool = False,\n    ) -&gt; int:\n        \"\"\"\n        Write a DataFrame to SAM format (plain text).\n\n        Parameters:\n            df: DataFrame or LazyFrame with 11 core BAM/SAM columns + optional tag columns\n            path: Output file path (.sam)\n            sort_on_write: If True, sort records by (chrom, start) and set header SO:coordinate.\n                If False (default), set header SO:unsorted.\n\n        Returns:\n            Number of rows written\n\n        !!! Example \"Write SAM files\"\n            ```python\n            import polars_bio as pb\n            df = pb.read_bam(\"input.bam\", tag_fields=[\"NM\", \"AS\"])\n            pb.write_sam(df, \"output.sam\")\n            ```\n        \"\"\"\n        return _write_bam_file(\n            df, path, OutputFormat.Sam, None, sort_on_write=sort_on_write\n        )\n\n    @staticmethod\n    def sink_sam(\n        lf: pl.LazyFrame,\n        path: str,\n        sort_on_write: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Streaming write a LazyFrame to SAM format (plain text).\n\n        Parameters:\n            lf: LazyFrame to write\n            path: Output file path (.sam)\n            sort_on_write: If True, sort records by (chrom, start) and set header SO:coordinate.\n                If False (default), set header SO:unsorted.\n\n        !!! Example \"Streaming write SAM\"\n            ```python\n            import polars_bio as pb\n            lf = pb.scan_bam(\"input.bam\").filter(pl.col(\"mapping_quality\") &gt; 20)\n            pb.sink_sam(lf, \"filtered.sam\")\n            ```\n        \"\"\"\n        _write_bam_file(lf, path, OutputFormat.Sam, None, sort_on_write=sort_on_write)\n\n    @staticmethod\n    def write_cram(\n        df: Union[pl.DataFrame, pl.LazyFrame],\n        path: str,\n        reference_path: str,\n        sort_on_write: bool = False,\n    ) -&gt; int:\n        \"\"\"\n        Write a DataFrame to CRAM format.\n\n        CRAM uses reference-based compression, storing only differences from the\n        reference sequence. This achieves 30-60% better compression than BAM.\n\n        Parameters:\n            df: DataFrame or LazyFrame with 11 core BAM columns + optional tag columns\n            path: Output CRAM file path\n            reference_path: Path to reference FASTA file (required). The reference must\n                contain all sequences referenced by the alignment data.\n            sort_on_write: If True, sort records by (chrom, start) and set header SO:coordinate.\n                If False (default), set header SO:unsorted.\n\n        Returns:\n            Number of rows written\n\n        !!! warning \"Known Limitation: MD and NM Tags\"\n            Due to a limitation in the underlying noodles-cram library, **MD and NM tags cannot be read back from CRAM files** after writing, even though they are written to the file. If you need MD/NM tags for downstream analysis, use BAM format instead. Other optional tags (RG, MQ, AM, OQ, AS, etc.) work correctly. See: https://github.com/biodatageeks/datafusion-bio-formats/issues/54\n\n        !!! Example \"Write CRAM files\"\n            ```python\n            import polars_bio as pb\n\n            df = pb.read_bam(\"input.bam\", tag_fields=[\"NM\", \"AS\"])\n\n            # Write CRAM with reference (required)\n            pb.write_cram(df, \"output.cram\", reference_path=\"reference.fasta\")\n\n            # For sorted output\n            pb.write_cram(df, \"output.cram\", reference_path=\"reference.fasta\", sort_on_write=True)\n            ```\n        \"\"\"\n        return _write_bam_file(\n            df, path, OutputFormat.Cram, reference_path, sort_on_write=sort_on_write\n        )\n\n    @staticmethod\n    def sink_cram(\n        lf: pl.LazyFrame,\n        path: str,\n        reference_path: str,\n        sort_on_write: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Streaming write a LazyFrame to CRAM format.\n\n        CRAM uses reference-based compression, storing only differences from the\n        reference sequence. This method streams data without materializing all\n        rows in memory.\n\n        Parameters:\n            lf: LazyFrame to write\n            path: Output CRAM file path\n            reference_path: Path to reference FASTA file (required). The reference must\n                contain all sequences referenced by the alignment data.\n            sort_on_write: If True, sort records by (chrom, start) and set header SO:coordinate.\n                If False (default), set header SO:unsorted.\n\n        !!! warning \"Known Limitation: MD and NM Tags\"\n            Due to a limitation in the underlying noodles-cram library, **MD and NM tags cannot be read back from CRAM files** after writing, even though they are written to the file. If you need MD/NM tags for downstream analysis, use BAM format instead. Other optional tags (RG, MQ, AM, OQ, AS, etc.) work correctly. See: https://github.com/biodatageeks/datafusion-bio-formats/issues/54\n\n        !!! Example \"Streaming write CRAM\"\n            ```python\n            import polars_bio as pb\n            import polars as pl\n\n            lf = pb.scan_bam(\"large_input.bam\")\n            lf = lf.filter(pl.col(\"mapping_quality\") &gt; 30)\n\n            # Write CRAM with reference (required)\n            pb.sink_cram(lf, \"filtered.cram\", reference_path=\"reference.fasta\")\n\n            # For sorted output\n            pb.sink_cram(lf, \"filtered.cram\", reference_path=\"reference.fasta\", sort_on_write=True)\n            ```\n        \"\"\"\n        _write_bam_file(\n            lf, path, OutputFormat.Cram, reference_path, sort_on_write=sort_on_write\n        )\n</code></pre>"},{"location":"api/#polars_bio.data_input.describe_bam","title":"<code>describe_bam(path, sample_size=100, chunk_size=8, concurrent_fetches=1, allow_anonymous=True, enable_request_payer=False, max_retries=5, timeout=300, compression_type='auto', use_zero_based=None)</code>  <code>staticmethod</code>","text":"<p>Get schema information for a BAM file with automatic tag discovery.</p> <p>Samples the first N records to discover all available tags and their types. Returns detailed schema information including column names, data types, nullability, category (standard/tag), SAM type, and descriptions.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the BAM file.</p> required <code>sample_size</code> <code>int</code> <p>Number of records to sample for tag discovery (default: 100). Use higher values for more comprehensive tag discovery.</p> <code>100</code> <code>chunk_size</code> <code>int</code> <p>The size in MB of a chunk when reading from object storage.</p> <code>8</code> <code>concurrent_fetches</code> <code>int</code> <p>The number of concurrent fetches when reading from object storage.</p> <code>1</code> <code>allow_anonymous</code> <code>bool</code> <p>Whether to allow anonymous access to object storage.</p> <code>True</code> <code>enable_request_payer</code> <code>bool</code> <p>Whether to enable request payer for object storage.</p> <code>False</code> <code>max_retries</code> <code>int</code> <p>The maximum number of retries for reading the file.</p> <code>5</code> <code>timeout</code> <code>int</code> <p>The timeout in seconds for reading the file.</p> <code>300</code> <code>compression_type</code> <code>str</code> <p>The compression type of the file. If \"auto\" (default), compression is detected automatically.</p> <code>'auto'</code> <code>use_zero_based</code> <code>Optional[bool]</code> <p>If True, output 0-based coordinates. If False, 1-based coordinates.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with columns:</p> <code>DataFrame</code> <ul> <li>column_name: Name of the column/field</li> </ul> <code>DataFrame</code> <ul> <li>data_type: Arrow data type (e.g., \"Utf8\", \"Int32\")</li> </ul> <code>DataFrame</code> <ul> <li>nullable: Whether the field can be null</li> </ul> <code>DataFrame</code> <ul> <li>category: \"core\" for fixed columns, \"tag\" for optional SAM tags</li> </ul> <code>DataFrame</code> <ul> <li>sam_type: SAM type code (e.g., \"Z\", \"i\") for tags, null for core columns</li> </ul> <code>DataFrame</code> <ul> <li>description: Human-readable description of the field</li> </ul> Example <pre><code>import polars_bio as pb\n\n# Auto-discover all tags present in the file\nschema = pb.describe_bam(\"file.bam\", sample_size=100)\nprint(schema)\n# Output:\n# shape: (15, 6)\n# \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n# \u2502 column_name \u2506 data_type \u2506 nullable \u2506 category \u2506 sam_type \u2506 description          \u2502\n# \u2502 ---         \u2506 ---       \u2506 ---      \u2506 ---      \u2506 ---      \u2506 ---                  \u2502\n# \u2502 str         \u2506 str       \u2506 bool     \u2506 str      \u2506 str      \u2506 str                  \u2502\n# \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n# \u2502 name        \u2506 Utf8      \u2506 true     \u2506 core     \u2506 null     \u2506 Query name           \u2502\n# \u2502 chrom       \u2506 Utf8      \u2506 true     \u2506 core     \u2506 null     \u2506 Reference name       \u2502\n# \u2502 ...         \u2506 ...       \u2506 ...      \u2506 ...      \u2506 ...      \u2506 ...                  \u2502\n# \u2502 NM          \u2506 Int32     \u2506 true     \u2506 tag      \u2506 i        \u2506 Edit distance        \u2502\n# \u2502 AS          \u2506 Int32     \u2506 true     \u2506 tag      \u2506 i        \u2506 Alignment score      \u2502\n# \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> Source code in <code>polars_bio/io.py</code> <pre><code>@staticmethod\ndef describe_bam(\n    path: str,\n    sample_size: int = 100,\n    chunk_size: int = 8,\n    concurrent_fetches: int = 1,\n    allow_anonymous: bool = True,\n    enable_request_payer: bool = False,\n    max_retries: int = 5,\n    timeout: int = 300,\n    compression_type: str = \"auto\",\n    use_zero_based: Optional[bool] = None,\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Get schema information for a BAM file with automatic tag discovery.\n\n    Samples the first N records to discover all available tags and their types.\n    Returns detailed schema information including column names, data types,\n    nullability, category (standard/tag), SAM type, and descriptions.\n\n    Parameters:\n        path: The path to the BAM file.\n        sample_size: Number of records to sample for tag discovery (default: 100).\n            Use higher values for more comprehensive tag discovery.\n        chunk_size: The size in MB of a chunk when reading from object storage.\n        concurrent_fetches: The number of concurrent fetches when reading from object storage.\n        allow_anonymous: Whether to allow anonymous access to object storage.\n        enable_request_payer: Whether to enable request payer for object storage.\n        max_retries: The maximum number of retries for reading the file.\n        timeout: The timeout in seconds for reading the file.\n        compression_type: The compression type of the file. If \"auto\" (default), compression is detected automatically.\n        use_zero_based: If True, output 0-based coordinates. If False, 1-based coordinates.\n\n    Returns:\n        DataFrame with columns:\n        - column_name: Name of the column/field\n        - data_type: Arrow data type (e.g., \"Utf8\", \"Int32\")\n        - nullable: Whether the field can be null\n        - category: \"core\" for fixed columns, \"tag\" for optional SAM tags\n        - sam_type: SAM type code (e.g., \"Z\", \"i\") for tags, null for core columns\n        - description: Human-readable description of the field\n\n    Example:\n        ```python\n        import polars_bio as pb\n\n        # Auto-discover all tags present in the file\n        schema = pb.describe_bam(\"file.bam\", sample_size=100)\n        print(schema)\n        # Output:\n        # shape: (15, 6)\n        # \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        # \u2502 column_name \u2506 data_type \u2506 nullable \u2506 category \u2506 sam_type \u2506 description          \u2502\n        # \u2502 ---         \u2506 ---       \u2506 ---      \u2506 ---      \u2506 ---      \u2506 ---                  \u2502\n        # \u2502 str         \u2506 str       \u2506 bool     \u2506 str      \u2506 str      \u2506 str                  \u2502\n        # \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n        # \u2502 name        \u2506 Utf8      \u2506 true     \u2506 core     \u2506 null     \u2506 Query name           \u2502\n        # \u2502 chrom       \u2506 Utf8      \u2506 true     \u2506 core     \u2506 null     \u2506 Reference name       \u2502\n        # \u2502 ...         \u2506 ...       \u2506 ...      \u2506 ...      \u2506 ...      \u2506 ...                  \u2502\n        # \u2502 NM          \u2506 Int32     \u2506 true     \u2506 tag      \u2506 i        \u2506 Edit distance        \u2502\n        # \u2502 AS          \u2506 Int32     \u2506 true     \u2506 tag      \u2506 i        \u2506 Alignment score      \u2502\n        # \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        ```\n    \"\"\"\n    # Build object storage options\n    object_storage_options = PyObjectStorageOptions(\n        chunk_size=chunk_size,\n        concurrent_fetches=concurrent_fetches,\n        allow_anonymous=allow_anonymous,\n        enable_request_payer=enable_request_payer,\n        max_retries=max_retries,\n        timeout=timeout,\n        compression_type=compression_type,\n    )\n\n    # Resolve zero_based setting\n    zero_based = _resolve_zero_based(use_zero_based)\n\n    # Call Rust function with tag auto-discovery (tag_fields=None)\n    df = py_describe_bam(\n        ctx,  # PyBioSessionContext\n        path,\n        object_storage_options,\n        zero_based,\n        None,  # tag_fields=None enables auto-discovery\n        sample_size,\n    )\n\n    # Convert DataFusion DataFrame to Polars DataFrame\n    return pl.from_arrow(df.to_arrow_table())\n</code></pre>"},{"location":"api/#polars_bio.data_input.describe_cram","title":"<code>describe_cram(path, reference_path=None, sample_size=100, chunk_size=8, concurrent_fetches=1, allow_anonymous=True, enable_request_payer=False, max_retries=5, timeout=300, compression_type='auto', use_zero_based=None)</code>  <code>staticmethod</code>","text":"<p>Get schema information for a CRAM file with automatic tag discovery.</p> <p>Samples the first N records to discover all available tags and their types. Returns detailed schema information including column names, data types, nullability, category (core/tag), SAM type, and descriptions.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the CRAM file.</p> required <code>reference_path</code> <code>str</code> <p>Optional path to external FASTA reference file.</p> <code>None</code> <code>sample_size</code> <code>int</code> <p>Number of records to sample for tag discovery (default: 100).</p> <code>100</code> <code>chunk_size</code> <code>int</code> <p>The size in MB of a chunk when reading from object storage.</p> <code>8</code> <code>concurrent_fetches</code> <code>int</code> <p>The number of concurrent fetches when reading from object storage.</p> <code>1</code> <code>allow_anonymous</code> <code>bool</code> <p>Whether to allow anonymous access to object storage.</p> <code>True</code> <code>enable_request_payer</code> <code>bool</code> <p>Whether to enable request payer for object storage.</p> <code>False</code> <code>max_retries</code> <code>int</code> <p>The maximum number of retries for reading the file.</p> <code>5</code> <code>timeout</code> <code>int</code> <p>The timeout in seconds for reading the file.</p> <code>300</code> <code>compression_type</code> <code>str</code> <p>The compression type of the file. If \"auto\" (default), compression is detected automatically.</p> <code>'auto'</code> <code>use_zero_based</code> <code>Optional[bool]</code> <p>If True, output 0-based coordinates. If False, 1-based coordinates.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with columns:</p> <code>DataFrame</code> <ul> <li>column_name: Name of the column/field</li> </ul> <code>DataFrame</code> <ul> <li>data_type: Arrow data type (e.g., \"Utf8\", \"Int32\")</li> </ul> <code>DataFrame</code> <ul> <li>nullable: Whether the field can be null</li> </ul> <code>DataFrame</code> <ul> <li>category: \"core\" for fixed columns, \"tag\" for optional SAM tags</li> </ul> <code>DataFrame</code> <ul> <li>sam_type: SAM type code (e.g., \"Z\", \"i\") for tags, null for core columns</li> </ul> <code>DataFrame</code> <ul> <li>description: Human-readable description of the field</li> </ul> <p>Known Limitation: MD and NM Tags</p> <p>Due to a limitation in the underlying noodles-cram library, MD (mismatch descriptor) and NM (edit distance) tags are not discoverable from CRAM files, even when stored. Automatic tag discovery will not include MD/NM tags. Other optional tags (RG, MQ, AM, OQ, etc.) are discovered correctly. See: https://github.com/biodatageeks/datafusion-bio-formats/issues/54</p> Example <pre><code>import polars_bio as pb\n\n# Auto-discover all tags present in the file\nschema = pb.describe_cram(\"file.cram\", sample_size=100)\nprint(schema)\n\n# Filter to see only tag columns\ntags = schema.filter(schema[\"category\"] == \"tag\")\nprint(tags[\"column_name\"])\n</code></pre> Source code in <code>polars_bio/io.py</code> <pre><code>@staticmethod\ndef describe_cram(\n    path: str,\n    reference_path: str = None,\n    sample_size: int = 100,\n    chunk_size: int = 8,\n    concurrent_fetches: int = 1,\n    allow_anonymous: bool = True,\n    enable_request_payer: bool = False,\n    max_retries: int = 5,\n    timeout: int = 300,\n    compression_type: str = \"auto\",\n    use_zero_based: Optional[bool] = None,\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Get schema information for a CRAM file with automatic tag discovery.\n\n    Samples the first N records to discover all available tags and their types.\n    Returns detailed schema information including column names, data types,\n    nullability, category (core/tag), SAM type, and descriptions.\n\n    Parameters:\n        path: The path to the CRAM file.\n        reference_path: Optional path to external FASTA reference file.\n        sample_size: Number of records to sample for tag discovery (default: 100).\n        chunk_size: The size in MB of a chunk when reading from object storage.\n        concurrent_fetches: The number of concurrent fetches when reading from object storage.\n        allow_anonymous: Whether to allow anonymous access to object storage.\n        enable_request_payer: Whether to enable request payer for object storage.\n        max_retries: The maximum number of retries for reading the file.\n        timeout: The timeout in seconds for reading the file.\n        compression_type: The compression type of the file. If \"auto\" (default), compression is detected automatically.\n        use_zero_based: If True, output 0-based coordinates. If False, 1-based coordinates.\n\n    Returns:\n        DataFrame with columns:\n        - column_name: Name of the column/field\n        - data_type: Arrow data type (e.g., \"Utf8\", \"Int32\")\n        - nullable: Whether the field can be null\n        - category: \"core\" for fixed columns, \"tag\" for optional SAM tags\n        - sam_type: SAM type code (e.g., \"Z\", \"i\") for tags, null for core columns\n        - description: Human-readable description of the field\n\n    !!! warning \"Known Limitation: MD and NM Tags\"\n        Due to a limitation in the underlying noodles-cram library, **MD (mismatch descriptor) and NM (edit distance) tags are not discoverable** from CRAM files, even when stored. Automatic tag discovery will not include MD/NM tags. Other optional tags (RG, MQ, AM, OQ, etc.) are discovered correctly. See: https://github.com/biodatageeks/datafusion-bio-formats/issues/54\n\n    Example:\n        ```python\n        import polars_bio as pb\n\n        # Auto-discover all tags present in the file\n        schema = pb.describe_cram(\"file.cram\", sample_size=100)\n        print(schema)\n\n        # Filter to see only tag columns\n        tags = schema.filter(schema[\"category\"] == \"tag\")\n        print(tags[\"column_name\"])\n        ```\n    \"\"\"\n    # Build object storage options\n    object_storage_options = PyObjectStorageOptions(\n        chunk_size=chunk_size,\n        concurrent_fetches=concurrent_fetches,\n        allow_anonymous=allow_anonymous,\n        enable_request_payer=enable_request_payer,\n        max_retries=max_retries,\n        timeout=timeout,\n        compression_type=compression_type,\n    )\n\n    # Resolve zero_based setting\n    zero_based = _resolve_zero_based(use_zero_based)\n\n    # Call Rust function with tag auto-discovery (tag_fields=None)\n    df = py_describe_cram(\n        ctx,\n        path,\n        reference_path,\n        object_storage_options,\n        zero_based,\n        None,  # tag_fields=None enables auto-discovery\n        sample_size,\n    )\n\n    # Convert DataFusion DataFrame to Polars DataFrame\n    return pl.from_arrow(df.to_arrow_table())\n</code></pre>"},{"location":"api/#polars_bio.data_input.describe_sam","title":"<code>describe_sam(path, sample_size=100, use_zero_based=None)</code>  <code>staticmethod</code>","text":"<p>Get schema information for a SAM file with automatic tag discovery.</p> <p>Samples the first N records to discover all available tags and their types. Reuses the BAM describe logic, which auto-detects SAM from the file extension.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the SAM file.</p> required <code>sample_size</code> <code>int</code> <p>Number of records to sample for tag discovery (default: 100).</p> <code>100</code> <code>use_zero_based</code> <code>Optional[bool]</code> <p>If True, output 0-based coordinates. If False, 1-based coordinates.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with columns: column_name, data_type, nullable, category, sam_type, description</p> Source code in <code>polars_bio/io.py</code> <pre><code>@staticmethod\ndef describe_sam(\n    path: str,\n    sample_size: int = 100,\n    use_zero_based: Optional[bool] = None,\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Get schema information for a SAM file with automatic tag discovery.\n\n    Samples the first N records to discover all available tags and their types.\n    Reuses the BAM describe logic, which auto-detects SAM from the file extension.\n\n    Parameters:\n        path: The path to the SAM file.\n        sample_size: Number of records to sample for tag discovery (default: 100).\n        use_zero_based: If True, output 0-based coordinates. If False, 1-based coordinates.\n\n    Returns:\n        DataFrame with columns: column_name, data_type, nullable, category, sam_type, description\n    \"\"\"\n    zero_based = _resolve_zero_based(use_zero_based)\n\n    df = py_describe_bam(\n        ctx,\n        path,\n        None,\n        zero_based,\n        None,\n        sample_size,\n    )\n\n    return pl.from_arrow(df.to_arrow_table())\n</code></pre>"},{"location":"api/#polars_bio.data_input.describe_vcf","title":"<code>describe_vcf(path, allow_anonymous=True, enable_request_payer=False, compression_type='auto')</code>  <code>staticmethod</code>","text":"<p>Describe VCF INFO schema.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the VCF file.</p> required <code>allow_anonymous</code> <code>bool</code> <p>Whether to allow anonymous access to object storage (GCS and S3 supported).</p> <code>True</code> <code>enable_request_payer</code> <code>bool</code> <p>Whether to enable request payer for object storage. This is useful for reading files from AWS S3 buckets that require request payer.</p> <code>False</code> <code>compression_type</code> <code>str</code> <p>The compression type of the VCF file. If not specified, it will be detected automatically..</p> <code>'auto'</code> Source code in <code>polars_bio/io.py</code> <pre><code>@staticmethod\ndef describe_vcf(\n    path: str,\n    allow_anonymous: bool = True,\n    enable_request_payer: bool = False,\n    compression_type: str = \"auto\",\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Describe VCF INFO schema.\n\n    Parameters:\n        path: The path to the VCF file.\n        allow_anonymous: Whether to allow anonymous access to object storage (GCS and S3 supported).\n        enable_request_payer: Whether to enable request payer for object storage. This is useful for reading files from AWS S3 buckets that require request payer.\n        compression_type: The compression type of the VCF file. If not specified, it will be detected automatically..\n    \"\"\"\n    object_storage_options = PyObjectStorageOptions(\n        allow_anonymous=allow_anonymous,\n        enable_request_payer=enable_request_payer,\n        chunk_size=8,\n        concurrent_fetches=1,\n        max_retries=1,\n        timeout=10,\n        compression_type=compression_type,\n    )\n    return py_describe_vcf(ctx, path, object_storage_options).to_polars()\n</code></pre>"},{"location":"api/#polars_bio.data_input.from_polars","title":"<code>from_polars(name, df)</code>  <code>staticmethod</code>","text":"<p>Register a Polars DataFrame as a DataFusion table.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the table.</p> required <code>df</code> <code>Union[DataFrame, LazyFrame]</code> <p>The Polars DataFrame.</p> required Source code in <code>polars_bio/io.py</code> <pre><code>@staticmethod\ndef from_polars(name: str, df: Union[pl.DataFrame, pl.LazyFrame]) -&gt; None:\n    \"\"\"\n    Register a Polars DataFrame as a DataFusion table.\n\n    Parameters:\n        name: The name of the table.\n        df: The Polars DataFrame.\n    \"\"\"\n    reader = (\n        df.to_arrow()\n        if isinstance(df, pl.DataFrame)\n        else df.collect().to_arrow().to_reader()\n    )\n    py_from_polars(ctx, name, reader)\n</code></pre>"},{"location":"api/#polars_bio.data_input.read_bam","title":"<code>read_bam(path, tag_fields=None, chunk_size=8, concurrent_fetches=1, allow_anonymous=True, enable_request_payer=False, max_retries=5, timeout=300, projection_pushdown=True, predicate_pushdown=True, use_zero_based=None)</code>  <code>staticmethod</code>","text":"<p>Read a BAM file into a DataFrame.</p> <p>Parallelism &amp; Indexed Reads</p> <p>Indexed parallel reads and predicate pushdown are automatic when a BAI/CSI index is present. See File formats support, Indexed reads, and Automatic parallel partitioning for details.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the BAM file.</p> required <code>tag_fields</code> <code>Union[list[str], None]</code> <p>List of BAM tag names to include as columns (e.g., [\"NM\", \"MD\", \"AS\"]). If None, no optional tags are parsed (default). Common tags include: NM (edit distance), MD (mismatch string), AS (alignment score), XS (secondary alignment score), RG (read group), CB (cell barcode), UB (UMI barcode).</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>The size in MB of a chunk when reading from an object store. The default is 8 MB. For large-scale operations, it is recommended to increase this value to 64.</p> <code>8</code> <code>concurrent_fetches</code> <code>int</code> <p>[GCS] The number of concurrent fetches when reading from an object store. The default is 1. For large-scale operations, it is recommended to increase this value to 8 or even more.</p> <code>1</code> <code>allow_anonymous</code> <code>bool</code> <p>[GCS, AWS S3] Whether to allow anonymous access to object storage.</p> <code>True</code> <code>enable_request_payer</code> <code>bool</code> <p>[AWS S3] Whether to enable request payer for object storage. This is useful for reading files from AWS S3 buckets that require request payer.</p> <code>False</code> <code>max_retries</code> <code>int</code> <p>The maximum number of retries for reading the file from object storage.</p> <code>5</code> <code>timeout</code> <code>int</code> <p>The timeout in seconds for reading the file from object storage.</p> <code>300</code> <code>projection_pushdown</code> <code>bool</code> <p>Enable column projection pushdown to optimize query performance by only reading the necessary columns at the DataFusion level.</p> <code>True</code> <code>predicate_pushdown</code> <code>bool</code> <p>Enable predicate pushdown using index files (BAI/CSI) for efficient region-based filtering. Index files are auto-discovered (e.g., <code>file.bam.bai</code>). Only simple predicates are pushed down (equality, comparisons, IN); complex predicates like <code>.str.contains()</code> or OR logic are filtered client-side. Correctness is always guaranteed.</p> <code>True</code> <code>use_zero_based</code> <code>Optional[bool]</code> <p>If True, output 0-based half-open coordinates. If False, output 1-based closed coordinates. If None (default), uses the global configuration <code>datafusion.bio.coordinate_system_zero_based</code>.</p> <code>None</code> <p>Note</p> <p>By default, coordinates are output in 1-based closed format. Use <code>use_zero_based=True</code> or set <code>pb.set_option(pb.POLARS_BIO_COORDINATE_SYSTEM_ZERO_BASED, True)</code> for 0-based half-open coordinates.</p> Source code in <code>polars_bio/io.py</code> <pre><code>@staticmethod\ndef read_bam(\n    path: str,\n    tag_fields: Union[list[str], None] = None,\n    chunk_size: int = 8,\n    concurrent_fetches: int = 1,\n    allow_anonymous: bool = True,\n    enable_request_payer: bool = False,\n    max_retries: int = 5,\n    timeout: int = 300,\n    projection_pushdown: bool = True,\n    predicate_pushdown: bool = True,\n    use_zero_based: Optional[bool] = None,\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Read a BAM file into a DataFrame.\n\n    !!! hint \"Parallelism &amp; Indexed Reads\"\n        Indexed parallel reads and predicate pushdown are automatic when a BAI/CSI index\n        is present. See [File formats support](/polars-bio/features/#file-formats-support),\n        [Indexed reads](/polars-bio/features/#indexed-reads-predicate-pushdown),\n        and [Automatic parallel partitioning](/polars-bio/features/#automatic-parallel-partitioning) for details.\n\n    Parameters:\n        path: The path to the BAM file.\n        tag_fields: List of BAM tag names to include as columns (e.g., [\"NM\", \"MD\", \"AS\"]). If None, no optional tags are parsed (default). Common tags include: NM (edit distance), MD (mismatch string), AS (alignment score), XS (secondary alignment score), RG (read group), CB (cell barcode), UB (UMI barcode).\n        chunk_size: The size in MB of a chunk when reading from an object store. The default is 8 MB. For large-scale operations, it is recommended to increase this value to 64.\n        concurrent_fetches: [GCS] The number of concurrent fetches when reading from an object store. The default is 1. For large-scale operations, it is recommended to increase this value to 8 or even more.\n        allow_anonymous: [GCS, AWS S3] Whether to allow anonymous access to object storage.\n        enable_request_payer: [AWS S3] Whether to enable request payer for object storage. This is useful for reading files from AWS S3 buckets that require request payer.\n        max_retries:  The maximum number of retries for reading the file from object storage.\n        timeout: The timeout in seconds for reading the file from object storage.\n        projection_pushdown: Enable column projection pushdown to optimize query performance by only reading the necessary columns at the DataFusion level.\n        predicate_pushdown: Enable predicate pushdown using index files (BAI/CSI) for efficient region-based filtering. Index files are auto-discovered (e.g., `file.bam.bai`). Only simple predicates are pushed down (equality, comparisons, IN); complex predicates like `.str.contains()` or OR logic are filtered client-side. Correctness is always guaranteed.\n        use_zero_based: If True, output 0-based half-open coordinates. If False, output 1-based closed coordinates. If None (default), uses the global configuration `datafusion.bio.coordinate_system_zero_based`.\n\n    !!! note\n        By default, coordinates are output in **1-based closed** format. Use `use_zero_based=True` or set `pb.set_option(pb.POLARS_BIO_COORDINATE_SYSTEM_ZERO_BASED, True)` for 0-based half-open coordinates.\n    \"\"\"\n    lf = IOOperations.scan_bam(\n        path,\n        tag_fields,\n        chunk_size,\n        concurrent_fetches,\n        allow_anonymous,\n        enable_request_payer,\n        max_retries,\n        timeout,\n        projection_pushdown,\n        predicate_pushdown,\n        use_zero_based,\n    )\n    # Get metadata before collecting (polars-config-meta doesn't preserve through collect)\n    zero_based = lf.config_meta.get_metadata().get(\"coordinate_system_zero_based\")\n    df = lf.collect()\n    # Set metadata on the collected DataFrame\n    if zero_based is not None:\n        set_coordinate_system(df, zero_based)\n    return df\n</code></pre>"},{"location":"api/#polars_bio.data_input.read_bed","title":"<code>read_bed(path, chunk_size=8, concurrent_fetches=1, allow_anonymous=True, enable_request_payer=False, max_retries=5, timeout=300, compression_type='auto', projection_pushdown=True, use_zero_based=None)</code>  <code>staticmethod</code>","text":"<p>Read a BED file into a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the BED file.</p> required <code>chunk_size</code> <code>int</code> <p>The size in MB of a chunk when reading from an object store. The default is 8 MB. For large scale operations, it is recommended to increase this value to 64.</p> <code>8</code> <code>concurrent_fetches</code> <code>int</code> <p>[GCS] The number of concurrent fetches when reading from an object store. The default is 1. For large scale operations, it is recommended to increase this value to 8 or even more.</p> <code>1</code> <code>allow_anonymous</code> <code>bool</code> <p>[GCS, AWS S3] Whether to allow anonymous access to object storage.</p> <code>True</code> <code>enable_request_payer</code> <code>bool</code> <p>[AWS S3] Whether to enable request payer for object storage. This is useful for reading files from AWS S3 buckets that require request payer.</p> <code>False</code> <code>max_retries</code> <code>int</code> <p>The maximum number of retries for reading the file from object storage.</p> <code>5</code> <code>timeout</code> <code>int</code> <p>The timeout in seconds for reading the file from object storage.</p> <code>300</code> <code>compression_type</code> <code>str</code> <p>The compression type of the BED file. If not specified, it will be detected automatically based on the file extension. BGZF compressions is supported ('bgz').</p> <code>'auto'</code> <code>projection_pushdown</code> <code>bool</code> <p>Enable column projection pushdown to optimize query performance by only reading the necessary columns at the DataFusion level.</p> <code>True</code> <code>use_zero_based</code> <code>Optional[bool]</code> <p>If True, output 0-based half-open coordinates. If False, output 1-based closed coordinates. If None (default), uses the global configuration <code>datafusion.bio.coordinate_system_zero_based</code>.</p> <code>None</code> <p>Note</p> <p>Only BED4 format is supported. It extends the basic BED format (BED3) by adding a name field, resulting in four columns: chromosome, start position, end position, and name. Also unlike other text formats, GZIP compression is not supported.</p> <p>Note</p> <p>By default, coordinates are output in 1-based closed format. Use <code>use_zero_based=True</code> or set <code>pb.set_option(pb.POLARS_BIO_COORDINATE_SYSTEM_ZERO_BASED, True)</code> for 0-based half-open coordinates.</p> Source code in <code>polars_bio/io.py</code> <pre><code>@staticmethod\ndef read_bed(\n    path: str,\n    chunk_size: int = 8,\n    concurrent_fetches: int = 1,\n    allow_anonymous: bool = True,\n    enable_request_payer: bool = False,\n    max_retries: int = 5,\n    timeout: int = 300,\n    compression_type: str = \"auto\",\n    projection_pushdown: bool = True,\n    use_zero_based: Optional[bool] = None,\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Read a BED file into a DataFrame.\n\n    Parameters:\n        path: The path to the BED file.\n        chunk_size: The size in MB of a chunk when reading from an object store. The default is 8 MB. For large scale operations, it is recommended to increase this value to 64.\n        concurrent_fetches: [GCS] The number of concurrent fetches when reading from an object store. The default is 1. For large scale operations, it is recommended to increase this value to 8 or even more.\n        allow_anonymous: [GCS, AWS S3] Whether to allow anonymous access to object storage.\n        enable_request_payer: [AWS S3] Whether to enable request payer for object storage. This is useful for reading files from AWS S3 buckets that require request payer.\n        max_retries:  The maximum number of retries for reading the file from object storage.\n        timeout: The timeout in seconds for reading the file from object storage.\n        compression_type: The compression type of the BED file. If not specified, it will be detected automatically based on the file extension. BGZF compressions is supported ('bgz').\n        projection_pushdown: Enable column projection pushdown to optimize query performance by only reading the necessary columns at the DataFusion level.\n        use_zero_based: If True, output 0-based half-open coordinates. If False, output 1-based closed coordinates. If None (default), uses the global configuration `datafusion.bio.coordinate_system_zero_based`.\n\n    !!! Note\n        Only **BED4** format is supported. It extends the basic BED format (BED3) by adding a name field, resulting in four columns: chromosome, start position, end position, and name.\n        Also unlike other text formats, **GZIP** compression is not supported.\n\n    !!! note\n        By default, coordinates are output in **1-based closed** format. Use `use_zero_based=True` or set `pb.set_option(pb.POLARS_BIO_COORDINATE_SYSTEM_ZERO_BASED, True)` for 0-based half-open coordinates.\n    \"\"\"\n    lf = IOOperations.scan_bed(\n        path,\n        chunk_size,\n        concurrent_fetches,\n        allow_anonymous,\n        enable_request_payer,\n        max_retries,\n        timeout,\n        compression_type,\n        projection_pushdown,\n        use_zero_based,\n    )\n    # Get metadata before collecting (polars-config-meta doesn't preserve through collect)\n    zero_based = lf.config_meta.get_metadata().get(\"coordinate_system_zero_based\")\n    df = lf.collect()\n    # Set metadata on the collected DataFrame\n    if zero_based is not None:\n        set_coordinate_system(df, zero_based)\n    return df\n</code></pre>"},{"location":"api/#polars_bio.data_input.read_cram","title":"<code>read_cram(path, reference_path=None, tag_fields=None, chunk_size=8, concurrent_fetches=1, allow_anonymous=True, enable_request_payer=False, max_retries=5, timeout=300, projection_pushdown=True, predicate_pushdown=True, use_zero_based=None)</code>  <code>staticmethod</code>","text":"<p>Read a CRAM file into a DataFrame.</p> <p>Parallelism &amp; Indexed Reads</p> <p>Indexed parallel reads and predicate pushdown are automatic when a CRAI index is present. See File formats support, Indexed reads, and Automatic parallel partitioning for details.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the CRAM file (local or cloud storage: S3, GCS, Azure Blob).</p> required <code>reference_path</code> <code>str</code> <p>Optional path to external FASTA reference file (local path only, cloud storage not supported). If not provided, the CRAM file must contain embedded reference sequences. The FASTA file must have an accompanying index file (.fai) in the same directory. Create the index using: <code>samtools faidx reference.fasta</code></p> <code>None</code> <code>tag_fields</code> <code>Union[list[str], None]</code> <p>List of CRAM tag names to include as columns (e.g., [\"NM\", \"MD\", \"AS\"]). If None, no optional tags are parsed (default). Common tags include: NM (edit distance), MD (mismatch string), AS (alignment score), XS (secondary alignment score), RG (read group), CB (cell barcode), UB (UMI barcode).</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>The size in MB of a chunk when reading from an object store. The default is 8 MB. For large scale operations, it is recommended to increase this value to 64.</p> <code>8</code> <code>concurrent_fetches</code> <code>int</code> <p>[GCS] The number of concurrent fetches when reading from an object store. The default is 1. For large scale operations, it is recommended to increase this value to 8 or even more.</p> <code>1</code> <code>allow_anonymous</code> <code>bool</code> <p>[GCS, AWS S3] Whether to allow anonymous access to object storage.</p> <code>True</code> <code>enable_request_payer</code> <code>bool</code> <p>[AWS S3] Whether to enable request payer for object storage. This is useful for reading files from AWS S3 buckets that require request payer.</p> <code>False</code> <code>max_retries</code> <code>int</code> <p>The maximum number of retries for reading the file from object storage.</p> <code>5</code> <code>timeout</code> <code>int</code> <p>The timeout in seconds for reading the file from object storage.</p> <code>300</code> <code>projection_pushdown</code> <code>bool</code> <p>Enable column projection pushdown optimization. When True, only requested columns are processed at the DataFusion execution level, improving performance and reducing memory usage.</p> <code>True</code> <code>predicate_pushdown</code> <code>bool</code> <p>Enable predicate pushdown using index files (CRAI) for efficient region-based filtering. Index files are auto-discovered (e.g., <code>file.cram.crai</code>). Only simple predicates are pushed down (equality, comparisons, IN); complex predicates like <code>.str.contains()</code> or OR logic are filtered client-side. Correctness is always guaranteed.</p> <code>True</code> <code>use_zero_based</code> <code>Optional[bool]</code> <p>If True, output 0-based half-open coordinates. If False, output 1-based closed coordinates. If None (default), uses the global configuration <code>datafusion.bio.coordinate_system_zero_based</code>.</p> <code>None</code> <p>Note</p> <p>By default, coordinates are output in 1-based closed format. Use <code>use_zero_based=True</code> or set <code>pb.set_option(pb.POLARS_BIO_COORDINATE_SYSTEM_ZERO_BASED, True)</code> for 0-based half-open coordinates.</p> <p>Known Limitation: MD and NM Tags</p> <p>Due to a limitation in the underlying noodles-cram library, MD (mismatch descriptor) and NM (edit distance) tags are not accessible from CRAM files, even when stored in the file. These tags can be seen with samtools but are not exposed through the noodles-cram record.data() interface.</p> <p>Other optional tags (RG, MQ, AM, OQ, etc.) work correctly. This issue is tracked at: https://github.com/biodatageeks/datafusion-bio-formats/issues/54</p> <p>Workaround: Use BAM format if MD/NM tags are required for your analysis.</p> <p>Using External Reference</p> <pre><code>import polars_bio as pb\n\n# Read CRAM with external reference\ndf = pb.read_cram(\n    \"/path/to/file.cram\",\n    reference_path=\"/path/to/reference.fasta\"\n)\n</code></pre> <p>Public CRAM File Example</p> <p>Download and read a public CRAM file from 42basepairs: </p><pre><code># Download the CRAM file and reference\nwget https://42basepairs.com/download/s3/gatk-test-data/wgs_cram/NA12878_20k_hg38/NA12878.cram\nwget https://storage.googleapis.com/genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.fasta\n\n# Create FASTA index (required)\nsamtools faidx Homo_sapiens_assembly38.fasta\n</code></pre><p></p> <pre><code>import polars_bio as pb\n\n# Read first 5 reads from the CRAM file\ndf = pb.scan_cram(\n    \"NA12878.cram\",\n    reference_path=\"Homo_sapiens_assembly38.fasta\"\n).limit(5).collect()\n\nprint(df.select([\"name\", \"chrom\", \"start\", \"end\", \"cigar\"]))\n</code></pre> <p>Creating CRAM with Embedded Reference</p> <p>To create a CRAM file with embedded reference using samtools: </p><pre><code>samtools view -C -o output.cram --output-fmt-option embed_ref=1 input.bam\n</code></pre><p></p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A Polars DataFrame with the following schema: - name: Read name (String) - chrom: Chromosome/contig name (String) - start: Alignment start position, 1-based (UInt32) - end: Alignment end position, 1-based (UInt32) - flags: SAM flags (UInt32) - cigar: CIGAR string (String) - mapping_quality: Mapping quality (UInt32) - mate_chrom: Mate chromosome/contig name (String) - mate_start: Mate alignment start position, 1-based (UInt32) - sequence: Read sequence (String) - quality_scores: Base quality scores (String)</p> Source code in <code>polars_bio/io.py</code> <pre><code>@staticmethod\ndef read_cram(\n    path: str,\n    reference_path: str = None,\n    tag_fields: Union[list[str], None] = None,\n    chunk_size: int = 8,\n    concurrent_fetches: int = 1,\n    allow_anonymous: bool = True,\n    enable_request_payer: bool = False,\n    max_retries: int = 5,\n    timeout: int = 300,\n    projection_pushdown: bool = True,\n    predicate_pushdown: bool = True,\n    use_zero_based: Optional[bool] = None,\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Read a CRAM file into a DataFrame.\n\n    !!! hint \"Parallelism &amp; Indexed Reads\"\n        Indexed parallel reads and predicate pushdown are automatic when a CRAI index\n        is present. See [File formats support](/polars-bio/features/#file-formats-support),\n        [Indexed reads](/polars-bio/features/#indexed-reads-predicate-pushdown),\n        and [Automatic parallel partitioning](/polars-bio/features/#automatic-parallel-partitioning) for details.\n\n    Parameters:\n        path: The path to the CRAM file (local or cloud storage: S3, GCS, Azure Blob).\n        reference_path: Optional path to external FASTA reference file (**local path only**, cloud storage not supported). If not provided, the CRAM file must contain embedded reference sequences. The FASTA file must have an accompanying index file (.fai) in the same directory. Create the index using: `samtools faidx reference.fasta`\n        tag_fields: List of CRAM tag names to include as columns (e.g., [\"NM\", \"MD\", \"AS\"]). If None, no optional tags are parsed (default). Common tags include: NM (edit distance), MD (mismatch string), AS (alignment score), XS (secondary alignment score), RG (read group), CB (cell barcode), UB (UMI barcode).\n        chunk_size: The size in MB of a chunk when reading from an object store. The default is 8 MB. For large scale operations, it is recommended to increase this value to 64.\n        concurrent_fetches: [GCS] The number of concurrent fetches when reading from an object store. The default is 1. For large scale operations, it is recommended to increase this value to 8 or even more.\n        allow_anonymous: [GCS, AWS S3] Whether to allow anonymous access to object storage.\n        enable_request_payer: [AWS S3] Whether to enable request payer for object storage. This is useful for reading files from AWS S3 buckets that require request payer.\n        max_retries: The maximum number of retries for reading the file from object storage.\n        timeout: The timeout in seconds for reading the file from object storage.\n        projection_pushdown: Enable column projection pushdown optimization. When True, only requested columns are processed at the DataFusion execution level, improving performance and reducing memory usage.\n        predicate_pushdown: Enable predicate pushdown using index files (CRAI) for efficient region-based filtering. Index files are auto-discovered (e.g., `file.cram.crai`). Only simple predicates are pushed down (equality, comparisons, IN); complex predicates like `.str.contains()` or OR logic are filtered client-side. Correctness is always guaranteed.\n        use_zero_based: If True, output 0-based half-open coordinates. If False, output 1-based closed coordinates. If None (default), uses the global configuration `datafusion.bio.coordinate_system_zero_based`.\n\n    !!! note\n        By default, coordinates are output in **1-based closed** format. Use `use_zero_based=True` or set `pb.set_option(pb.POLARS_BIO_COORDINATE_SYSTEM_ZERO_BASED, True)` for 0-based half-open coordinates.\n\n    !!! warning \"Known Limitation: MD and NM Tags\"\n        Due to a limitation in the underlying noodles-cram library, **MD (mismatch descriptor) and NM (edit distance) tags are not accessible** from CRAM files, even when stored in the file. These tags can be seen with samtools but are not exposed through the noodles-cram record.data() interface.\n\n        Other optional tags (RG, MQ, AM, OQ, etc.) work correctly. This issue is tracked at: https://github.com/biodatageeks/datafusion-bio-formats/issues/54\n\n        **Workaround**: Use BAM format if MD/NM tags are required for your analysis.\n\n    !!! example \"Using External Reference\"\n        ```python\n        import polars_bio as pb\n\n        # Read CRAM with external reference\n        df = pb.read_cram(\n            \"/path/to/file.cram\",\n            reference_path=\"/path/to/reference.fasta\"\n        )\n        ```\n\n    !!! example \"Public CRAM File Example\"\n        Download and read a public CRAM file from 42basepairs:\n        ```bash\n        # Download the CRAM file and reference\n        wget https://42basepairs.com/download/s3/gatk-test-data/wgs_cram/NA12878_20k_hg38/NA12878.cram\n        wget https://storage.googleapis.com/genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.fasta\n\n        # Create FASTA index (required)\n        samtools faidx Homo_sapiens_assembly38.fasta\n        ```\n\n        ```python\n        import polars_bio as pb\n\n        # Read first 5 reads from the CRAM file\n        df = pb.scan_cram(\n            \"NA12878.cram\",\n            reference_path=\"Homo_sapiens_assembly38.fasta\"\n        ).limit(5).collect()\n\n        print(df.select([\"name\", \"chrom\", \"start\", \"end\", \"cigar\"]))\n        ```\n\n    !!! example \"Creating CRAM with Embedded Reference\"\n        To create a CRAM file with embedded reference using samtools:\n        ```bash\n        samtools view -C -o output.cram --output-fmt-option embed_ref=1 input.bam\n        ```\n\n    Returns:\n        A Polars DataFrame with the following schema:\n            - name: Read name (String)\n            - chrom: Chromosome/contig name (String)\n            - start: Alignment start position, 1-based (UInt32)\n            - end: Alignment end position, 1-based (UInt32)\n            - flags: SAM flags (UInt32)\n            - cigar: CIGAR string (String)\n            - mapping_quality: Mapping quality (UInt32)\n            - mate_chrom: Mate chromosome/contig name (String)\n            - mate_start: Mate alignment start position, 1-based (UInt32)\n            - sequence: Read sequence (String)\n            - quality_scores: Base quality scores (String)\n    \"\"\"\n    lf = IOOperations.scan_cram(\n        path,\n        reference_path,\n        tag_fields,\n        chunk_size,\n        concurrent_fetches,\n        allow_anonymous,\n        enable_request_payer,\n        max_retries,\n        timeout,\n        projection_pushdown,\n        predicate_pushdown,\n        use_zero_based,\n    )\n    # Get metadata before collecting (polars-config-meta doesn't preserve through collect)\n    zero_based = lf.config_meta.get_metadata().get(\"coordinate_system_zero_based\")\n    df = lf.collect()\n    # Set metadata on the collected DataFrame\n    if zero_based is not None:\n        set_coordinate_system(df, zero_based)\n    return df\n</code></pre>"},{"location":"api/#polars_bio.data_input.read_fasta","title":"<code>read_fasta(path, chunk_size=8, concurrent_fetches=1, allow_anonymous=True, enable_request_payer=False, max_retries=5, timeout=300, compression_type='auto', projection_pushdown=True)</code>  <code>staticmethod</code>","text":"<p>Read a FASTA file into a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the FASTA file.</p> required <code>chunk_size</code> <code>int</code> <p>The size in MB of a chunk when reading from an object store. The default is 8 MB. For large scale operations, it is recommended to increase this value to 64.</p> <code>8</code> <code>concurrent_fetches</code> <code>int</code> <p>[GCS] The number of concurrent fetches when reading from an object store. The default is 1. For large scale operations, it is recommended to increase this value to 8 or even more.</p> <code>1</code> <code>allow_anonymous</code> <code>bool</code> <p>[GCS, AWS S3] Whether to allow anonymous access to object storage.</p> <code>True</code> <code>enable_request_payer</code> <code>bool</code> <p>[AWS S3] Whether to enable request payer for object storage. This is useful for reading files from AWS S3 buckets that require request payer.</p> <code>False</code> <code>max_retries</code> <code>int</code> <p>The maximum number of retries for reading the file from object storage.</p> <code>5</code> <code>timeout</code> <code>int</code> <p>The timeout in seconds for reading the file from object storage.</p> <code>300</code> <code>compression_type</code> <code>str</code> <p>The compression type of the FASTA file. If not specified, it will be detected automatically based on the file extension. BGZF and GZIP compressions are supported ('bgz', 'gz').</p> <code>'auto'</code> <code>projection_pushdown</code> <code>bool</code> <p>Enable column projection pushdown optimization. When True, only requested columns are processed at the DataFusion execution level, improving performance and reducing memory usage.</p> <code>True</code> <p>Example</p> <pre><code>wget https://www.ebi.ac.uk/ena/browser/api/fasta/BK006935.2?download=true -O /tmp/test.fasta\n</code></pre> <p></p><pre><code>import polars_bio as pb\npb.read_fasta(\"/tmp/test.fasta\").limit(1)\n</code></pre> <pre><code> shape: (1, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 name                    \u2506 description                     \u2506 sequence                        \u2502\n\u2502 ---                     \u2506 ---                             \u2506 ---                             \u2502\n\u2502 str                     \u2506 str                             \u2506 str                             \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 ENA|BK006935|BK006935.2 \u2506 TPA_inf: Saccharomyces cerevis\u2026 \u2506 CCACACCACACCCACACACCCACACACCAC\u2026 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre><p></p> Source code in <code>polars_bio/io.py</code> <pre><code>@staticmethod\ndef read_fasta(\n    path: str,\n    chunk_size: int = 8,\n    concurrent_fetches: int = 1,\n    allow_anonymous: bool = True,\n    enable_request_payer: bool = False,\n    max_retries: int = 5,\n    timeout: int = 300,\n    compression_type: str = \"auto\",\n    projection_pushdown: bool = True,\n) -&gt; pl.DataFrame:\n    \"\"\"\n\n    Read a FASTA file into a DataFrame.\n\n    Parameters:\n        path: The path to the FASTA file.\n        chunk_size: The size in MB of a chunk when reading from an object store. The default is 8 MB. For large scale operations, it is recommended to increase this value to 64.\n        concurrent_fetches: [GCS] The number of concurrent fetches when reading from an object store. The default is 1. For large scale operations, it is recommended to increase this value to 8 or even more.\n        allow_anonymous: [GCS, AWS S3] Whether to allow anonymous access to object storage.\n        enable_request_payer: [AWS S3] Whether to enable request payer for object storage. This is useful for reading files from AWS S3 buckets that require request payer.\n        max_retries:  The maximum number of retries for reading the file from object storage.\n        timeout: The timeout in seconds for reading the file from object storage.\n        compression_type: The compression type of the FASTA file. If not specified, it will be detected automatically based on the file extension. BGZF and GZIP compressions are supported ('bgz', 'gz').\n        projection_pushdown: Enable column projection pushdown optimization. When True, only requested columns are processed at the DataFusion execution level, improving performance and reducing memory usage.\n\n    !!! Example\n        ```shell\n        wget https://www.ebi.ac.uk/ena/browser/api/fasta/BK006935.2?download=true -O /tmp/test.fasta\n        ```\n\n        ```python\n        import polars_bio as pb\n        pb.read_fasta(\"/tmp/test.fasta\").limit(1)\n        ```\n        ```shell\n         shape: (1, 3)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 name                    \u2506 description                     \u2506 sequence                        \u2502\n        \u2502 ---                     \u2506 ---                             \u2506 ---                             \u2502\n        \u2502 str                     \u2506 str                             \u2506 str                             \u2502\n        \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n        \u2502 ENA|BK006935|BK006935.2 \u2506 TPA_inf: Saccharomyces cerevis\u2026 \u2506 CCACACCACACCCACACACCCACACACCAC\u2026 \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        ```\n    \"\"\"\n    return IOOperations.scan_fasta(\n        path,\n        chunk_size,\n        concurrent_fetches,\n        allow_anonymous,\n        enable_request_payer,\n        max_retries,\n        timeout,\n        compression_type,\n        projection_pushdown,\n    ).collect()\n</code></pre>"},{"location":"api/#polars_bio.data_input.read_fastq","title":"<code>read_fastq(path, chunk_size=8, concurrent_fetches=1, allow_anonymous=True, enable_request_payer=False, max_retries=5, timeout=300, compression_type='auto', projection_pushdown=True)</code>  <code>staticmethod</code>","text":"<p>Read a FASTQ file into a DataFrame.</p> <p>Parallelism &amp; Compression</p> <p>See File formats support, Compression, and Automatic parallel partitioning for details on parallel reads and supported compression types.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the FASTQ file.</p> required <code>chunk_size</code> <code>int</code> <p>The size in MB of a chunk when reading from an object store. The default is 8 MB. For large scale operations, it is recommended to increase this value to 64.</p> <code>8</code> <code>concurrent_fetches</code> <code>int</code> <p>[GCS] The number of concurrent fetches when reading from an object store. The default is 1. For large scale operations, it is recommended to increase this value to 8 or even more.</p> <code>1</code> <code>allow_anonymous</code> <code>bool</code> <p>[GCS, AWS S3] Whether to allow anonymous access to object storage.</p> <code>True</code> <code>enable_request_payer</code> <code>bool</code> <p>[AWS S3] Whether to enable request payer for object storage. This is useful for reading files from AWS S3 buckets that require request payer.</p> <code>False</code> <code>max_retries</code> <code>int</code> <p>The maximum number of retries for reading the file from object storage.</p> <code>5</code> <code>timeout</code> <code>int</code> <p>The timeout in seconds for reading the file from object storage.</p> <code>300</code> <code>compression_type</code> <code>str</code> <p>The compression type of the FASTQ file. If not specified, it will be detected automatically based on the file extension. BGZF and GZIP compressions are supported ('bgz', 'gz').</p> <code>'auto'</code> <code>projection_pushdown</code> <code>bool</code> <p>Enable column projection pushdown to optimize query performance by only reading the necessary columns at the DataFusion level.</p> <code>True</code> Source code in <code>polars_bio/io.py</code> <pre><code>@staticmethod\ndef read_fastq(\n    path: str,\n    chunk_size: int = 8,\n    concurrent_fetches: int = 1,\n    allow_anonymous: bool = True,\n    enable_request_payer: bool = False,\n    max_retries: int = 5,\n    timeout: int = 300,\n    compression_type: str = \"auto\",\n    projection_pushdown: bool = True,\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Read a FASTQ file into a DataFrame.\n\n    !!! hint \"Parallelism &amp; Compression\"\n        See [File formats support](/polars-bio/features/#file-formats-support),\n        [Compression](/polars-bio/features/#compression),\n        and [Automatic parallel partitioning](/polars-bio/features/#automatic-parallel-partitioning) for details on parallel reads and supported compression types.\n\n    Parameters:\n        path: The path to the FASTQ file.\n        chunk_size: The size in MB of a chunk when reading from an object store. The default is 8 MB. For large scale operations, it is recommended to increase this value to 64.\n        concurrent_fetches: [GCS] The number of concurrent fetches when reading from an object store. The default is 1. For large scale operations, it is recommended to increase this value to 8 or even more.\n        allow_anonymous: [GCS, AWS S3] Whether to allow anonymous access to object storage.\n        enable_request_payer: [AWS S3] Whether to enable request payer for object storage. This is useful for reading files from AWS S3 buckets that require request payer.\n        max_retries:  The maximum number of retries for reading the file from object storage.\n        timeout: The timeout in seconds for reading the file from object storage.\n        compression_type: The compression type of the FASTQ file. If not specified, it will be detected automatically based on the file extension. BGZF and GZIP compressions are supported ('bgz', 'gz').\n        projection_pushdown: Enable column projection pushdown to optimize query performance by only reading the necessary columns at the DataFusion level.\n    \"\"\"\n    return IOOperations.scan_fastq(\n        path,\n        chunk_size,\n        concurrent_fetches,\n        allow_anonymous,\n        enable_request_payer,\n        max_retries,\n        timeout,\n        compression_type,\n        projection_pushdown,\n    ).collect()\n</code></pre>"},{"location":"api/#polars_bio.data_input.read_gff","title":"<code>read_gff(path, attr_fields=None, chunk_size=8, concurrent_fetches=1, allow_anonymous=True, enable_request_payer=False, max_retries=5, timeout=300, compression_type='auto', projection_pushdown=True, predicate_pushdown=True, use_zero_based=None)</code>  <code>staticmethod</code>","text":"<p>Read a GFF file into a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the GFF file.</p> required <code>attr_fields</code> <code>Union[list[str], None]</code> <p>List of attribute field names to extract as separate columns. If None, attributes will be kept as a nested structure. Use this to extract specific attributes like 'ID', 'gene_name', 'gene_type', etc. as direct columns for easier access.</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>The size in MB of a chunk when reading from an object store. The default is 8 MB. For large scale operations, it is recommended to increase this value to 64.</p> <code>8</code> <code>concurrent_fetches</code> <code>int</code> <p>[GCS] The number of concurrent fetches when reading from an object store. The default is 1. For large scale operations, it is recommended to increase this value to 8 or even more.</p> <code>1</code> <code>allow_anonymous</code> <code>bool</code> <p>[GCS, AWS S3] Whether to allow anonymous access to object storage.</p> <code>True</code> <code>enable_request_payer</code> <code>bool</code> <p>[AWS S3] Whether to enable request payer for object storage. This is useful for reading files from AWS S3 buckets that require request payer.</p> <code>False</code> <code>max_retries</code> <code>int</code> <p>The maximum number of retries for reading the file from object storage.</p> <code>5</code> <code>timeout</code> <code>int</code> <p>The timeout in seconds for reading the file from object storage.</p> <code>300</code> <code>compression_type</code> <code>str</code> <p>The compression type of the GFF file. If not specified, it will be detected automatically..</p> <code>'auto'</code> <code>projection_pushdown</code> <code>bool</code> <p>Enable column projection pushdown to optimize query performance by only reading the necessary columns at the DataFusion level.</p> <code>True</code> <code>predicate_pushdown</code> <code>bool</code> <p>Enable predicate pushdown using index files (TBI/CSI) for efficient region-based filtering. Index files are auto-discovered (e.g., <code>file.gff.gz.tbi</code>). Only simple predicates are pushed down (equality, comparisons, IN); complex predicates like <code>.str.contains()</code> or OR logic are filtered client-side. Correctness is always guaranteed.</p> <code>True</code> <code>use_zero_based</code> <code>Optional[bool]</code> <p>If True, output 0-based half-open coordinates. If False, output 1-based closed coordinates. If None (default), uses the global configuration <code>datafusion.bio.coordinate_system_zero_based</code>.</p> <code>None</code> <p>Note</p> <p>By default, coordinates are output in 1-based closed format. Use <code>use_zero_based=True</code> or set <code>pb.set_option(pb.POLARS_BIO_COORDINATE_SYSTEM_ZERO_BASED, True)</code> for 0-based half-open coordinates.</p> Source code in <code>polars_bio/io.py</code> <pre><code>@staticmethod\ndef read_gff(\n    path: str,\n    attr_fields: Union[list[str], None] = None,\n    chunk_size: int = 8,\n    concurrent_fetches: int = 1,\n    allow_anonymous: bool = True,\n    enable_request_payer: bool = False,\n    max_retries: int = 5,\n    timeout: int = 300,\n    compression_type: str = \"auto\",\n    projection_pushdown: bool = True,\n    predicate_pushdown: bool = True,\n    use_zero_based: Optional[bool] = None,\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Read a GFF file into a DataFrame.\n\n    Parameters:\n        path: The path to the GFF file.\n        attr_fields: List of attribute field names to extract as separate columns. If *None*, attributes will be kept as a nested structure. Use this to extract specific attributes like 'ID', 'gene_name', 'gene_type', etc. as direct columns for easier access.\n        chunk_size: The size in MB of a chunk when reading from an object store. The default is 8 MB. For large scale operations, it is recommended to increase this value to 64.\n        concurrent_fetches: [GCS] The number of concurrent fetches when reading from an object store. The default is 1. For large scale operations, it is recommended to increase this value to 8 or even more.\n        allow_anonymous: [GCS, AWS S3] Whether to allow anonymous access to object storage.\n        enable_request_payer: [AWS S3] Whether to enable request payer for object storage. This is useful for reading files from AWS S3 buckets that require request payer.\n        max_retries:  The maximum number of retries for reading the file from object storage.\n        timeout: The timeout in seconds for reading the file from object storage.\n        compression_type: The compression type of the GFF file. If not specified, it will be detected automatically..\n        projection_pushdown: Enable column projection pushdown to optimize query performance by only reading the necessary columns at the DataFusion level.\n        predicate_pushdown: Enable predicate pushdown using index files (TBI/CSI) for efficient region-based filtering. Index files are auto-discovered (e.g., `file.gff.gz.tbi`). Only simple predicates are pushed down (equality, comparisons, IN); complex predicates like `.str.contains()` or OR logic are filtered client-side. Correctness is always guaranteed.\n        use_zero_based: If True, output 0-based half-open coordinates. If False, output 1-based closed coordinates. If None (default), uses the global configuration `datafusion.bio.coordinate_system_zero_based`.\n\n    !!! note\n        By default, coordinates are output in **1-based closed** format. Use `use_zero_based=True` or set `pb.set_option(pb.POLARS_BIO_COORDINATE_SYSTEM_ZERO_BASED, True)` for 0-based half-open coordinates.\n    \"\"\"\n    lf = IOOperations.scan_gff(\n        path,\n        attr_fields,\n        chunk_size,\n        concurrent_fetches,\n        allow_anonymous,\n        enable_request_payer,\n        max_retries,\n        timeout,\n        compression_type,\n        projection_pushdown,\n        predicate_pushdown,\n        use_zero_based,\n    )\n    # Get metadata before collecting (polars-config-meta doesn't preserve through collect)\n    zero_based = lf.config_meta.get_metadata().get(\"coordinate_system_zero_based\")\n    df = lf.collect()\n    # Set metadata on the collected DataFrame\n    if zero_based is not None:\n        set_coordinate_system(df, zero_based)\n    return df\n</code></pre>"},{"location":"api/#polars_bio.data_input.read_pairs","title":"<code>read_pairs(path, chunk_size=8, concurrent_fetches=1, allow_anonymous=True, enable_request_payer=False, max_retries=5, timeout=300, compression_type='auto', projection_pushdown=True, predicate_pushdown=True, use_zero_based=None)</code>  <code>staticmethod</code>","text":"<p>Read a Pairs (Hi-C) file into a DataFrame.</p> <p>The Pairs format (4DN project) stores chromatin contact data with columns: readID, chr1, pos1, chr2, pos2, strand1, strand2.</p> <p>Parallelism &amp; Indexed Reads</p> <p>Indexed parallel reads and predicate pushdown are automatic when a TBI index is present. See File formats support and Indexed reads for details.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the Pairs file (.pairs, .pairs.gz, .pairs.bgz).</p> required <code>chunk_size</code> <code>int</code> <p>The size in MB of a chunk when reading from an object store.</p> <code>8</code> <code>concurrent_fetches</code> <code>int</code> <p>The number of concurrent fetches when reading from an object store.</p> <code>1</code> <code>allow_anonymous</code> <code>bool</code> <p>Whether to allow anonymous access to object storage.</p> <code>True</code> <code>enable_request_payer</code> <code>bool</code> <p>Whether to enable request payer for object storage.</p> <code>False</code> <code>max_retries</code> <code>int</code> <p>The maximum number of retries for reading the file from object storage.</p> <code>5</code> <code>timeout</code> <code>int</code> <p>The timeout in seconds for reading the file from object storage.</p> <code>300</code> <code>compression_type</code> <code>str</code> <p>The compression type. If not specified, it will be detected automatically.</p> <code>'auto'</code> <code>projection_pushdown</code> <code>bool</code> <p>Enable column projection pushdown to optimize query performance.</p> <code>True</code> <code>predicate_pushdown</code> <code>bool</code> <p>Enable predicate pushdown using index files (TBI) for efficient region-based filtering. Index files are auto-discovered (e.g., <code>file.pairs.gz.tbi</code>). Only simple predicates are pushed down (equality, comparisons, IN); complex predicates are filtered client-side. Correctness is always guaranteed.</p> <code>True</code> <code>use_zero_based</code> <code>Optional[bool]</code> <p>If True, output 0-based half-open coordinates. If False, output 1-based closed coordinates. If None (default), uses the global configuration <code>datafusion.bio.coordinate_system_zero_based</code>.</p> <code>None</code> <p>Note</p> <p>By default, coordinates are output in 1-based closed format. Use <code>use_zero_based=True</code> or set <code>pb.set_option(pb.POLARS_BIO_COORDINATE_SYSTEM_ZERO_BASED, True)</code> for 0-based half-open coordinates.</p> Source code in <code>polars_bio/io.py</code> <pre><code>@staticmethod\ndef read_pairs(\n    path: str,\n    chunk_size: int = 8,\n    concurrent_fetches: int = 1,\n    allow_anonymous: bool = True,\n    enable_request_payer: bool = False,\n    max_retries: int = 5,\n    timeout: int = 300,\n    compression_type: str = \"auto\",\n    projection_pushdown: bool = True,\n    predicate_pushdown: bool = True,\n    use_zero_based: Optional[bool] = None,\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Read a Pairs (Hi-C) file into a DataFrame.\n\n    The Pairs format (4DN project) stores chromatin contact data with columns:\n    readID, chr1, pos1, chr2, pos2, strand1, strand2.\n\n    !!! hint \"Parallelism &amp; Indexed Reads\"\n        Indexed parallel reads and predicate pushdown are automatic when a TBI index\n        is present. See [File formats support](/polars-bio/features/#file-formats-support)\n        and [Indexed reads](/polars-bio/features/#indexed-reads-predicate-pushdown) for details.\n\n    Parameters:\n        path: The path to the Pairs file (.pairs, .pairs.gz, .pairs.bgz).\n        chunk_size: The size in MB of a chunk when reading from an object store.\n        concurrent_fetches: The number of concurrent fetches when reading from an object store.\n        allow_anonymous: Whether to allow anonymous access to object storage.\n        enable_request_payer: Whether to enable request payer for object storage.\n        max_retries: The maximum number of retries for reading the file from object storage.\n        timeout: The timeout in seconds for reading the file from object storage.\n        compression_type: The compression type. If not specified, it will be detected automatically.\n        projection_pushdown: Enable column projection pushdown to optimize query performance.\n        predicate_pushdown: Enable predicate pushdown using index files (TBI) for efficient region-based filtering. Index files are auto-discovered (e.g., `file.pairs.gz.tbi`). Only simple predicates are pushed down (equality, comparisons, IN); complex predicates are filtered client-side. Correctness is always guaranteed.\n        use_zero_based: If True, output 0-based half-open coordinates. If False, output 1-based closed coordinates. If None (default), uses the global configuration `datafusion.bio.coordinate_system_zero_based`.\n\n    !!! note\n        By default, coordinates are output in **1-based closed** format. Use `use_zero_based=True` or set `pb.set_option(pb.POLARS_BIO_COORDINATE_SYSTEM_ZERO_BASED, True)` for 0-based half-open coordinates.\n    \"\"\"\n    lf = IOOperations.scan_pairs(\n        path,\n        chunk_size,\n        concurrent_fetches,\n        allow_anonymous,\n        enable_request_payer,\n        max_retries,\n        timeout,\n        compression_type,\n        projection_pushdown,\n        predicate_pushdown,\n        use_zero_based,\n    )\n    zero_based = lf.config_meta.get_metadata().get(\"coordinate_system_zero_based\")\n    df = lf.collect()\n    if zero_based is not None:\n        set_coordinate_system(df, zero_based)\n    return df\n</code></pre>"},{"location":"api/#polars_bio.data_input.read_sam","title":"<code>read_sam(path, tag_fields=None, projection_pushdown=True, use_zero_based=None)</code>  <code>staticmethod</code>","text":"<p>Read a SAM file into a DataFrame.</p> <p>SAM (Sequence Alignment/Map) is the plain-text counterpart of BAM. This function reuses the BAM reader, which auto-detects the format from the file extension.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the SAM file.</p> required <code>tag_fields</code> <code>Union[list[str], None]</code> <p>List of SAM tag names to include as columns (e.g., [\"NM\", \"MD\", \"AS\"]). If None, no optional tags are parsed (default).</p> <code>None</code> <code>projection_pushdown</code> <code>bool</code> <p>Enable column projection pushdown to optimize query performance.</p> <code>True</code> <code>use_zero_based</code> <code>Optional[bool]</code> <p>If True, output 0-based half-open coordinates. If False, output 1-based closed coordinates. If None (default), uses the global configuration.</p> <code>None</code> <p>Note</p> <p>By default, coordinates are output in 1-based closed format.</p> Source code in <code>polars_bio/io.py</code> <pre><code>@staticmethod\ndef read_sam(\n    path: str,\n    tag_fields: Union[list[str], None] = None,\n    projection_pushdown: bool = True,\n    use_zero_based: Optional[bool] = None,\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Read a SAM file into a DataFrame.\n\n    SAM (Sequence Alignment/Map) is the plain-text counterpart of BAM.\n    This function reuses the BAM reader, which auto-detects the format\n    from the file extension.\n\n    Parameters:\n        path: The path to the SAM file.\n        tag_fields: List of SAM tag names to include as columns (e.g., [\"NM\", \"MD\", \"AS\"]).\n            If None, no optional tags are parsed (default).\n        projection_pushdown: Enable column projection pushdown to optimize query performance.\n        use_zero_based: If True, output 0-based half-open coordinates.\n            If False, output 1-based closed coordinates.\n            If None (default), uses the global configuration.\n\n    !!! note\n        By default, coordinates are output in **1-based closed** format.\n    \"\"\"\n    lf = IOOperations.scan_sam(\n        path,\n        tag_fields,\n        projection_pushdown,\n        use_zero_based,\n    )\n    zero_based = lf.config_meta.get_metadata().get(\"coordinate_system_zero_based\")\n    df = lf.collect()\n    if zero_based is not None:\n        set_coordinate_system(df, zero_based)\n    return df\n</code></pre>"},{"location":"api/#polars_bio.data_input.read_table","title":"<code>read_table(path, schema=None, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Read a tab-delimited (i.e. BED) file into a Polars DataFrame.  Tries to be compatible with Bioframe's read_table  but faster. Schema should follow the Bioframe's schema format.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the file.</p> required <code>schema</code> <code>Dict</code> <p>Schema should follow the Bioframe's schema format.</p> <code>None</code> Source code in <code>polars_bio/io.py</code> <pre><code>@staticmethod\ndef read_table(path: str, schema: Dict = None, **kwargs) -&gt; pl.DataFrame:\n    \"\"\"\n     Read a tab-delimited (i.e. BED) file into a Polars DataFrame.\n     Tries to be compatible with Bioframe's [read_table](https://bioframe.readthedocs.io/en/latest/guide-io.html)\n     but faster. Schema should follow the Bioframe's schema [format](https://github.com/open2c/bioframe/blob/2b685eebef393c2c9e6220dcf550b3630d87518e/bioframe/io/schemas.py#L174).\n\n    Parameters:\n        path: The path to the file.\n        schema: Schema should follow the Bioframe's schema [format](https://github.com/open2c/bioframe/blob/2b685eebef393c2c9e6220dcf550b3630d87518e/bioframe/io/schemas.py#L174).\n    \"\"\"\n    return IOOperations.scan_table(path, schema, **kwargs).collect()\n</code></pre>"},{"location":"api/#polars_bio.data_input.read_vcf","title":"<code>read_vcf(path, info_fields=None, format_fields=None, chunk_size=8, concurrent_fetches=1, allow_anonymous=True, enable_request_payer=False, max_retries=5, timeout=300, compression_type='auto', projection_pushdown=True, predicate_pushdown=True, use_zero_based=None)</code>  <code>staticmethod</code>","text":"<p>Read a VCF file into a DataFrame.</p> <p>Parallelism &amp; Indexed Reads</p> <p>Indexed parallel reads and predicate pushdown are automatic when a TBI/CSI index is present. See File formats support, Indexed reads, and Automatic parallel partitioning for details.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the VCF file.</p> required <code>info_fields</code> <code>Union[list[str], None]</code> <p>List of INFO field names to include. If None, all INFO fields from the VCF header are included by default. Use this to limit fields for better performance.</p> <code>None</code> <code>format_fields</code> <code>Union[list[str], None]</code> <p>List of FORMAT field names to include (per-sample genotype data). If None, all FORMAT fields are included by default. For single-sample VCFs, FORMAT fields are top-level columns (e.g., <code>GT</code>, <code>DP</code>). For multi-sample VCFs, FORMAT data is exposed as a nested <code>genotypes</code> column (<code>list&lt;struct&lt;sample_id, values&gt;&gt;</code>).</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>The size in MB of a chunk when reading from an object store. The default is 8 MB. For large scale operations, it is recommended to increase this value to 64.</p> <code>8</code> <code>concurrent_fetches</code> <code>int</code> <p>[GCS] The number of concurrent fetches when reading from an object store. The default is 1. For large scale operations, it is recommended to increase this value to 8 or even more.</p> <code>1</code> <code>allow_anonymous</code> <code>bool</code> <p>[GCS, AWS S3] Whether to allow anonymous access to object storage.</p> <code>True</code> <code>enable_request_payer</code> <code>bool</code> <p>[AWS S3] Whether to enable request payer for object storage. This is useful for reading files from AWS S3 buckets that require request payer.</p> <code>False</code> <code>max_retries</code> <code>int</code> <p>The maximum number of retries for reading the file from object storage.</p> <code>5</code> <code>timeout</code> <code>int</code> <p>The timeout in seconds for reading the file from object storage.</p> <code>300</code> <code>compression_type</code> <code>str</code> <p>The compression type of the VCF file. If not specified, it will be detected automatically..</p> <code>'auto'</code> <code>projection_pushdown</code> <code>bool</code> <p>Enable column projection pushdown to optimize query performance by only reading the necessary columns at the DataFusion level.</p> <code>True</code> <code>predicate_pushdown</code> <code>bool</code> <p>Enable predicate pushdown using index files (TBI/CSI) for efficient region-based filtering. Index files are auto-discovered (e.g., <code>file.vcf.gz.tbi</code>). Only simple predicates are pushed down (equality, comparisons, IN); complex predicates like <code>.str.contains()</code> or OR logic are filtered client-side. Correctness is always guaranteed.</p> <code>True</code> <code>use_zero_based</code> <code>Optional[bool]</code> <p>If True, output 0-based half-open coordinates. If False, output 1-based closed coordinates. If None (default), uses the global configuration <code>datafusion.bio.coordinate_system_zero_based</code>.</p> <code>None</code> <p>Note</p> <p>By default, coordinates are output in 1-based closed format. Use <code>use_zero_based=True</code> or set <code>pb.set_option(pb.POLARS_BIO_COORDINATE_SYSTEM_ZERO_BASED, True)</code> for 0-based half-open coordinates.</p> <p>Reading VCF with INFO and FORMAT fields</p> <pre><code>import polars_bio as pb\n\n# Read VCF with both INFO and FORMAT fields\ndf = pb.read_vcf(\n    \"sample.vcf.gz\",\n    info_fields=[\"END\"],              # INFO field\n    format_fields=[\"GT\", \"DP\", \"GQ\"]  # FORMAT fields\n)\n\n# Single-sample VCF: FORMAT fields are top-level columns (GT, DP, GQ)\nprint(df.select([\"chrom\", \"start\", \"ref\", \"alt\", \"END\", \"GT\", \"DP\", \"GQ\"]))\n# Output:\n# shape: (10, 8)\n# \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n# \u2502 chrom \u2506 start \u2506 ref \u2506 alt \u2506 END  \u2506 GT  \u2506 DP  \u2506 GQ  \u2502\n# \u2502 str   \u2506 u32   \u2506 str \u2506 str \u2506 i32  \u2506 str \u2506 i32 \u2506 i32 \u2502\n# \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n# \u2502 1     \u2506 10009 \u2506 A   \u2506 .   \u2506 null \u2506 0/0 \u2506 10  \u2506 27  \u2502\n# \u2502 1     \u2506 10015 \u2506 A   \u2506 .   \u2506 null \u2506 0/0 \u2506 17  \u2506 35  \u2502\n# \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n\n# Multi-sample VCF: FORMAT data is nested in \"genotypes\"\ndf = pb.read_vcf(\"multisample.vcf\", format_fields=[\"GT\", \"DP\"])\nprint(df.select([\"chrom\", \"start\", \"genotypes\"]))\n</code></pre> Source code in <code>polars_bio/io.py</code> <pre><code>@staticmethod\ndef read_vcf(\n    path: str,\n    info_fields: Union[list[str], None] = None,\n    format_fields: Union[list[str], None] = None,\n    chunk_size: int = 8,\n    concurrent_fetches: int = 1,\n    allow_anonymous: bool = True,\n    enable_request_payer: bool = False,\n    max_retries: int = 5,\n    timeout: int = 300,\n    compression_type: str = \"auto\",\n    projection_pushdown: bool = True,\n    predicate_pushdown: bool = True,\n    use_zero_based: Optional[bool] = None,\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Read a VCF file into a DataFrame.\n\n    !!! hint \"Parallelism &amp; Indexed Reads\"\n        Indexed parallel reads and predicate pushdown are automatic when a TBI/CSI index\n        is present. See [File formats support](/polars-bio/features/#file-formats-support),\n        [Indexed reads](/polars-bio/features/#indexed-reads-predicate-pushdown),\n        and [Automatic parallel partitioning](/polars-bio/features/#automatic-parallel-partitioning) for details.\n\n    Parameters:\n        path: The path to the VCF file.\n        info_fields: List of INFO field names to include. If *None*, all INFO fields from the VCF header are included by default. Use this to limit fields for better performance.\n        format_fields: List of FORMAT field names to include (per-sample genotype data). If *None*, all FORMAT fields are included by default. For **single-sample** VCFs, FORMAT fields are top-level columns (e.g., `GT`, `DP`). For **multi-sample** VCFs, FORMAT data is exposed as a nested `genotypes` column (`list&lt;struct&lt;sample_id, values&gt;&gt;`).\n        chunk_size: The size in MB of a chunk when reading from an object store. The default is 8 MB. For large scale operations, it is recommended to increase this value to 64.\n        concurrent_fetches: [GCS] The number of concurrent fetches when reading from an object store. The default is 1. For large scale operations, it is recommended to increase this value to 8 or even more.\n        allow_anonymous: [GCS, AWS S3] Whether to allow anonymous access to object storage.\n        enable_request_payer: [AWS S3] Whether to enable request payer for object storage. This is useful for reading files from AWS S3 buckets that require request payer.\n        max_retries:  The maximum number of retries for reading the file from object storage.\n        timeout: The timeout in seconds for reading the file from object storage.\n        compression_type: The compression type of the VCF file. If not specified, it will be detected automatically..\n        projection_pushdown: Enable column projection pushdown to optimize query performance by only reading the necessary columns at the DataFusion level.\n        predicate_pushdown: Enable predicate pushdown using index files (TBI/CSI) for efficient region-based filtering. Index files are auto-discovered (e.g., `file.vcf.gz.tbi`). Only simple predicates are pushed down (equality, comparisons, IN); complex predicates like `.str.contains()` or OR logic are filtered client-side. Correctness is always guaranteed.\n        use_zero_based: If True, output 0-based half-open coordinates. If False, output 1-based closed coordinates. If None (default), uses the global configuration `datafusion.bio.coordinate_system_zero_based`.\n\n    !!! note\n        By default, coordinates are output in **1-based closed** format. Use `use_zero_based=True` or set `pb.set_option(pb.POLARS_BIO_COORDINATE_SYSTEM_ZERO_BASED, True)` for 0-based half-open coordinates.\n\n    !!! Example \"Reading VCF with INFO and FORMAT fields\"\n        ```python\n        import polars_bio as pb\n\n        # Read VCF with both INFO and FORMAT fields\n        df = pb.read_vcf(\n            \"sample.vcf.gz\",\n            info_fields=[\"END\"],              # INFO field\n            format_fields=[\"GT\", \"DP\", \"GQ\"]  # FORMAT fields\n        )\n\n        # Single-sample VCF: FORMAT fields are top-level columns (GT, DP, GQ)\n        print(df.select([\"chrom\", \"start\", \"ref\", \"alt\", \"END\", \"GT\", \"DP\", \"GQ\"]))\n        # Output:\n        # shape: (10, 8)\n        # \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n        # \u2502 chrom \u2506 start \u2506 ref \u2506 alt \u2506 END  \u2506 GT  \u2506 DP  \u2506 GQ  \u2502\n        # \u2502 str   \u2506 u32   \u2506 str \u2506 str \u2506 i32  \u2506 str \u2506 i32 \u2506 i32 \u2502\n        # \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2561\n        # \u2502 1     \u2506 10009 \u2506 A   \u2506 .   \u2506 null \u2506 0/0 \u2506 10  \u2506 27  \u2502\n        # \u2502 1     \u2506 10015 \u2506 A   \u2506 .   \u2506 null \u2506 0/0 \u2506 17  \u2506 35  \u2502\n        # \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n\n        # Multi-sample VCF: FORMAT data is nested in \"genotypes\"\n        df = pb.read_vcf(\"multisample.vcf\", format_fields=[\"GT\", \"DP\"])\n        print(df.select([\"chrom\", \"start\", \"genotypes\"]))\n        ```\n    \"\"\"\n    lf = IOOperations.scan_vcf(\n        path,\n        info_fields,\n        format_fields,\n        chunk_size,\n        concurrent_fetches,\n        allow_anonymous,\n        enable_request_payer,\n        max_retries,\n        timeout,\n        compression_type,\n        projection_pushdown,\n        predicate_pushdown,\n        use_zero_based,\n    )\n    # Get metadata before collecting (polars-config-meta doesn't preserve through collect)\n    zero_based = lf.config_meta.get_metadata().get(\"coordinate_system_zero_based\")\n    df = lf.collect()\n    # Set metadata on the collected DataFrame\n    if zero_based is not None:\n        set_coordinate_system(df, zero_based)\n    return df\n</code></pre>"},{"location":"api/#polars_bio.data_input.scan_bam","title":"<code>scan_bam(path, tag_fields=None, chunk_size=8, concurrent_fetches=1, allow_anonymous=True, enable_request_payer=False, max_retries=5, timeout=300, projection_pushdown=True, predicate_pushdown=True, use_zero_based=None)</code>  <code>staticmethod</code>","text":"<p>Lazily read a BAM file into a LazyFrame.</p> <p>Parallelism &amp; Indexed Reads</p> <p>Indexed parallel reads and predicate pushdown are automatic when a BAI/CSI index is present. See File formats support, Indexed reads, and Automatic parallel partitioning for details.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the BAM file.</p> required <code>tag_fields</code> <code>Union[list[str], None]</code> <p>List of BAM tag names to include as columns (e.g., [\"NM\", \"MD\", \"AS\"]). If None, no optional tags are parsed (default). Common tags include: NM (edit distance), MD (mismatch string), AS (alignment score), XS (secondary alignment score), RG (read group), CB (cell barcode), UB (UMI barcode).</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>The size in MB of a chunk when reading from an object store. The default is 8 MB. For large scale operations, it is recommended to increase this value to 64.</p> <code>8</code> <code>concurrent_fetches</code> <code>int</code> <p>[GCS] The number of concurrent fetches when reading from an object store. The default is 1. For large scale operations, it is recommended to increase this value to 8 or even more.</p> <code>1</code> <code>allow_anonymous</code> <code>bool</code> <p>[GCS, AWS S3] Whether to allow anonymous access to object storage.</p> <code>True</code> <code>enable_request_payer</code> <code>bool</code> <p>[AWS S3] Whether to enable request payer for object storage. This is useful for reading files from AWS S3 buckets that require request payer.</p> <code>False</code> <code>max_retries</code> <code>int</code> <p>The maximum number of retries for reading the file from object storage.</p> <code>5</code> <code>timeout</code> <code>int</code> <p>The timeout in seconds for reading the file from object storage.</p> <code>300</code> <code>projection_pushdown</code> <code>bool</code> <p>Enable column projection pushdown to optimize query performance by only reading the necessary columns at the DataFusion level.</p> <code>True</code> <code>predicate_pushdown</code> <code>bool</code> <p>Enable predicate pushdown using index files (BAI/CSI) for efficient region-based filtering. Index files are auto-discovered (e.g., <code>file.bam.bai</code>). Only simple predicates are pushed down (equality, comparisons, IN); complex predicates like <code>.str.contains()</code> or OR logic are filtered client-side. Correctness is always guaranteed.</p> <code>True</code> <code>use_zero_based</code> <code>Optional[bool]</code> <p>If True, output 0-based half-open coordinates. If False, output 1-based closed coordinates. If None (default), uses the global configuration <code>datafusion.bio.coordinate_system_zero_based</code>.</p> <code>None</code> <p>Note</p> <p>By default, coordinates are output in 1-based closed format. Use <code>use_zero_based=True</code> or set <code>pb.set_option(pb.POLARS_BIO_COORDINATE_SYSTEM_ZERO_BASED, True)</code> for 0-based half-open coordinates.</p> Source code in <code>polars_bio/io.py</code> <pre><code>@staticmethod\ndef scan_bam(\n    path: str,\n    tag_fields: Union[list[str], None] = None,\n    chunk_size: int = 8,\n    concurrent_fetches: int = 1,\n    allow_anonymous: bool = True,\n    enable_request_payer: bool = False,\n    max_retries: int = 5,\n    timeout: int = 300,\n    projection_pushdown: bool = True,\n    predicate_pushdown: bool = True,\n    use_zero_based: Optional[bool] = None,\n) -&gt; pl.LazyFrame:\n    \"\"\"\n    Lazily read a BAM file into a LazyFrame.\n\n    !!! hint \"Parallelism &amp; Indexed Reads\"\n        Indexed parallel reads and predicate pushdown are automatic when a BAI/CSI index\n        is present. See [File formats support](/polars-bio/features/#file-formats-support),\n        [Indexed reads](/polars-bio/features/#indexed-reads-predicate-pushdown),\n        and [Automatic parallel partitioning](/polars-bio/features/#automatic-parallel-partitioning) for details.\n\n    Parameters:\n        path: The path to the BAM file.\n        tag_fields: List of BAM tag names to include as columns (e.g., [\"NM\", \"MD\", \"AS\"]). If None, no optional tags are parsed (default). Common tags include: NM (edit distance), MD (mismatch string), AS (alignment score), XS (secondary alignment score), RG (read group), CB (cell barcode), UB (UMI barcode).\n        chunk_size: The size in MB of a chunk when reading from an object store. The default is 8 MB. For large scale operations, it is recommended to increase this value to 64.\n        concurrent_fetches: [GCS] The number of concurrent fetches when reading from an object store. The default is 1. For large scale operations, it is recommended to increase this value to 8 or even more.\n        allow_anonymous: [GCS, AWS S3] Whether to allow anonymous access to object storage.\n        enable_request_payer: [AWS S3] Whether to enable request payer for object storage. This is useful for reading files from AWS S3 buckets that require request payer.\n        max_retries:  The maximum number of retries for reading the file from object storage.\n        timeout: The timeout in seconds for reading the file from object storage.\n        projection_pushdown: Enable column projection pushdown to optimize query performance by only reading the necessary columns at the DataFusion level.\n        predicate_pushdown: Enable predicate pushdown using index files (BAI/CSI) for efficient region-based filtering. Index files are auto-discovered (e.g., `file.bam.bai`). Only simple predicates are pushed down (equality, comparisons, IN); complex predicates like `.str.contains()` or OR logic are filtered client-side. Correctness is always guaranteed.\n        use_zero_based: If True, output 0-based half-open coordinates. If False, output 1-based closed coordinates. If None (default), uses the global configuration `datafusion.bio.coordinate_system_zero_based`.\n\n    !!! note\n        By default, coordinates are output in **1-based closed** format. Use `use_zero_based=True` or set `pb.set_option(pb.POLARS_BIO_COORDINATE_SYSTEM_ZERO_BASED, True)` for 0-based half-open coordinates.\n    \"\"\"\n    object_storage_options = PyObjectStorageOptions(\n        allow_anonymous=allow_anonymous,\n        enable_request_payer=enable_request_payer,\n        chunk_size=chunk_size,\n        concurrent_fetches=concurrent_fetches,\n        max_retries=max_retries,\n        timeout=timeout,\n        compression_type=\"auto\",\n    )\n\n    zero_based = _resolve_zero_based(use_zero_based)\n    bam_read_options = BamReadOptions(\n        object_storage_options=object_storage_options,\n        zero_based=zero_based,\n        tag_fields=tag_fields,\n    )\n    read_options = ReadOptions(bam_read_options=bam_read_options)\n    return _read_file(\n        path,\n        InputFormat.Bam,\n        read_options,\n        projection_pushdown,\n        predicate_pushdown,\n        zero_based=zero_based,\n    )\n</code></pre>"},{"location":"api/#polars_bio.data_input.scan_bed","title":"<code>scan_bed(path, chunk_size=8, concurrent_fetches=1, allow_anonymous=True, enable_request_payer=False, max_retries=5, timeout=300, compression_type='auto', projection_pushdown=True, use_zero_based=None)</code>  <code>staticmethod</code>","text":"<p>Lazily read a BED file into a LazyFrame.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the BED file.</p> required <code>chunk_size</code> <code>int</code> <p>The size in MB of a chunk when reading from an object store. The default is 8 MB. For large scale operations, it is recommended to increase this value to 64.</p> <code>8</code> <code>concurrent_fetches</code> <code>int</code> <p>[GCS] The number of concurrent fetches when reading from an object store. The default is 1. For large scale operations, it is recommended to increase this value to 8 or even more.</p> <code>1</code> <code>allow_anonymous</code> <code>bool</code> <p>[GCS, AWS S3] Whether to allow anonymous access to object storage.</p> <code>True</code> <code>enable_request_payer</code> <code>bool</code> <p>[AWS S3] Whether to enable request payer for object storage. This is useful for reading files from AWS S3 buckets that require request payer.</p> <code>False</code> <code>max_retries</code> <code>int</code> <p>The maximum number of retries for reading the file from object storage.</p> <code>5</code> <code>timeout</code> <code>int</code> <p>The timeout in seconds for reading the file from object storage.</p> <code>300</code> <code>compression_type</code> <code>str</code> <p>The compression type of the BED file. If not specified, it will be detected automatically based on the file extension. BGZF compressions is supported ('bgz').</p> <code>'auto'</code> <code>projection_pushdown</code> <code>bool</code> <p>Enable column projection pushdown to optimize query performance by only reading the necessary columns at the DataFusion level.</p> <code>True</code> <code>use_zero_based</code> <code>Optional[bool]</code> <p>If True, output 0-based half-open coordinates. If False, output 1-based closed coordinates. If None (default), uses the global configuration <code>datafusion.bio.coordinate_system_zero_based</code>.</p> <code>None</code> <p>Note</p> <p>Only BED4 format is supported. It extends the basic BED format (BED3) by adding a name field, resulting in four columns: chromosome, start position, end position, and name. Also unlike other text formats, GZIP compression is not supported.</p> <p>Note</p> <p>By default, coordinates are output in 1-based closed format. Use <code>use_zero_based=True</code> or set <code>pb.set_option(pb.POLARS_BIO_COORDINATE_SYSTEM_ZERO_BASED, True)</code> for 0-based half-open coordinates.</p> Source code in <code>polars_bio/io.py</code> <pre><code>@staticmethod\ndef scan_bed(\n    path: str,\n    chunk_size: int = 8,\n    concurrent_fetches: int = 1,\n    allow_anonymous: bool = True,\n    enable_request_payer: bool = False,\n    max_retries: int = 5,\n    timeout: int = 300,\n    compression_type: str = \"auto\",\n    projection_pushdown: bool = True,\n    use_zero_based: Optional[bool] = None,\n) -&gt; pl.LazyFrame:\n    \"\"\"\n    Lazily read a BED file into a LazyFrame.\n\n    Parameters:\n        path: The path to the BED file.\n        chunk_size: The size in MB of a chunk when reading from an object store. The default is 8 MB. For large scale operations, it is recommended to increase this value to 64.\n        concurrent_fetches: [GCS] The number of concurrent fetches when reading from an object store. The default is 1. For large scale operations, it is recommended to increase this value to 8 or even more.\n        allow_anonymous: [GCS, AWS S3] Whether to allow anonymous access to object storage.\n        enable_request_payer: [AWS S3] Whether to enable request payer for object storage. This is useful for reading files from AWS S3 buckets that require request payer.\n        max_retries:  The maximum number of retries for reading the file from object storage.\n        timeout: The timeout in seconds for reading the file from object storage.\n        compression_type: The compression type of the BED file. If not specified, it will be detected automatically based on the file extension. BGZF compressions is supported ('bgz').\n        projection_pushdown: Enable column projection pushdown to optimize query performance by only reading the necessary columns at the DataFusion level.\n        use_zero_based: If True, output 0-based half-open coordinates. If False, output 1-based closed coordinates. If None (default), uses the global configuration `datafusion.bio.coordinate_system_zero_based`.\n\n    !!! Note\n        Only **BED4** format is supported. It extends the basic BED format (BED3) by adding a name field, resulting in four columns: chromosome, start position, end position, and name.\n        Also unlike other text formats, **GZIP** compression is not supported.\n\n    !!! note\n        By default, coordinates are output in **1-based closed** format. Use `use_zero_based=True` or set `pb.set_option(pb.POLARS_BIO_COORDINATE_SYSTEM_ZERO_BASED, True)` for 0-based half-open coordinates.\n    \"\"\"\n    object_storage_options = PyObjectStorageOptions(\n        allow_anonymous=allow_anonymous,\n        enable_request_payer=enable_request_payer,\n        chunk_size=chunk_size,\n        concurrent_fetches=concurrent_fetches,\n        max_retries=max_retries,\n        timeout=timeout,\n        compression_type=compression_type,\n    )\n\n    zero_based = _resolve_zero_based(use_zero_based)\n    bed_read_options = BedReadOptions(\n        object_storage_options=object_storage_options,\n        zero_based=zero_based,\n    )\n    read_options = ReadOptions(bed_read_options=bed_read_options)\n    return _read_file(\n        path,\n        InputFormat.Bed,\n        read_options,\n        projection_pushdown,\n        zero_based=zero_based,\n    )\n</code></pre>"},{"location":"api/#polars_bio.data_input.scan_cram","title":"<code>scan_cram(path, reference_path=None, tag_fields=None, chunk_size=8, concurrent_fetches=1, allow_anonymous=True, enable_request_payer=False, max_retries=5, timeout=300, projection_pushdown=True, predicate_pushdown=True, use_zero_based=None)</code>  <code>staticmethod</code>","text":"<p>Lazily read a CRAM file into a LazyFrame.</p> <p>Parallelism &amp; Indexed Reads</p> <p>Indexed parallel reads and predicate pushdown are automatic when a CRAI index is present. See File formats support, Indexed reads, and Automatic parallel partitioning for details.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the CRAM file (local or cloud storage: S3, GCS, Azure Blob).</p> required <code>reference_path</code> <code>str</code> <p>Optional path to external FASTA reference file (local path only, cloud storage not supported). If not provided, the CRAM file must contain embedded reference sequences. The FASTA file must have an accompanying index file (.fai) in the same directory. Create the index using: <code>samtools faidx reference.fasta</code></p> <code>None</code> <code>tag_fields</code> <code>Union[list[str], None]</code> <p>List of CRAM tag names to include as columns (e.g., [\"NM\", \"MD\", \"AS\"]). If None, no optional tags are parsed (default). Common tags include: NM (edit distance), MD (mismatch string), AS (alignment score), XS (secondary alignment score), RG (read group), CB (cell barcode), UB (UMI barcode).</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>The size in MB of a chunk when reading from an object store. The default is 8 MB. For large scale operations, it is recommended to increase this value to 64.</p> <code>8</code> <code>concurrent_fetches</code> <code>int</code> <p>[GCS] The number of concurrent fetches when reading from an object store. The default is 1. For large scale operations, it is recommended to increase this value to 8 or even more.</p> <code>1</code> <code>allow_anonymous</code> <code>bool</code> <p>[GCS, AWS S3] Whether to allow anonymous access to object storage.</p> <code>True</code> <code>enable_request_payer</code> <code>bool</code> <p>[AWS S3] Whether to enable request payer for object storage. This is useful for reading files from AWS S3 buckets that require request payer.</p> <code>False</code> <code>max_retries</code> <code>int</code> <p>The maximum number of retries for reading the file from object storage.</p> <code>5</code> <code>timeout</code> <code>int</code> <p>The timeout in seconds for reading the file from object storage.</p> <code>300</code> <code>projection_pushdown</code> <code>bool</code> <p>Enable column projection pushdown optimization. When True, only requested columns are processed at the DataFusion execution level, improving performance and reducing memory usage.</p> <code>True</code> <code>predicate_pushdown</code> <code>bool</code> <p>Enable predicate pushdown using index files (CRAI) for efficient region-based filtering. Index files are auto-discovered (e.g., <code>file.cram.crai</code>). Only simple predicates are pushed down (equality, comparisons, IN); complex predicates like <code>.str.contains()</code> or OR logic are filtered client-side. Correctness is always guaranteed.</p> <code>True</code> <code>use_zero_based</code> <code>Optional[bool]</code> <p>If True, output 0-based half-open coordinates. If False, output 1-based closed coordinates. If None (default), uses the global configuration <code>datafusion.bio.coordinate_system_zero_based</code>.</p> <code>None</code> <p>Note</p> <p>By default, coordinates are output in 1-based closed format. Use <code>use_zero_based=True</code> or set <code>pb.set_option(pb.POLARS_BIO_COORDINATE_SYSTEM_ZERO_BASED, True)</code> for 0-based half-open coordinates.</p> <p>Known Limitation: MD and NM Tags</p> <p>Due to a limitation in the underlying noodles-cram library, MD (mismatch descriptor) and NM (edit distance) tags are not accessible from CRAM files, even when stored in the file. These tags can be seen with samtools but are not exposed through the noodles-cram record.data() interface.</p> <p>Other optional tags (RG, MQ, AM, OQ, etc.) work correctly. This issue is tracked at: https://github.com/biodatageeks/datafusion-bio-formats/issues/54</p> <p>Workaround: Use BAM format if MD/NM tags are required for your analysis.</p> <p>Using External Reference</p> <pre><code>import polars_bio as pb\n\n# Lazy scan CRAM with external reference\nlf = pb.scan_cram(\n    \"/path/to/file.cram\",\n    reference_path=\"/path/to/reference.fasta\"\n)\n\n# Apply transformations and collect\ndf = lf.filter(pl.col(\"chrom\") == \"chr1\").collect()\n</code></pre> <p>Public CRAM File Example</p> <p>Download and read a public CRAM file from 42basepairs: </p><pre><code># Download the CRAM file and reference\nwget https://42basepairs.com/download/s3/gatk-test-data/wgs_cram/NA12878_20k_hg38/NA12878.cram\nwget https://storage.googleapis.com/genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.fasta\n\n# Create FASTA index (required)\nsamtools faidx Homo_sapiens_assembly38.fasta\n</code></pre><p></p> <pre><code>import polars_bio as pb\nimport polars as pl\n\n# Lazy scan and filter for chromosome 20 reads\ndf = pb.scan_cram(\n    \"NA12878.cram\",\n    reference_path=\"Homo_sapiens_assembly38.fasta\"\n).filter(\n    pl.col(\"chrom\") == \"chr20\"\n).select(\n    [\"name\", \"chrom\", \"start\", \"end\", \"mapping_quality\"]\n).limit(10).collect()\n\nprint(df)\n</code></pre> <p>Creating CRAM with Embedded Reference</p> <p>To create a CRAM file with embedded reference using samtools: </p><pre><code>samtools view -C -o output.cram --output-fmt-option embed_ref=1 input.bam\n</code></pre><p></p> <p>Returns:</p> Type Description <code>LazyFrame</code> <p>A Polars LazyFrame with the following schema: - name: Read name (String) - chrom: Chromosome/contig name (String) - start: Alignment start position, 1-based (UInt32) - end: Alignment end position, 1-based (UInt32) - flags: SAM flags (UInt32) - cigar: CIGAR string (String) - mapping_quality: Mapping quality (UInt32) - mate_chrom: Mate chromosome/contig name (String) - mate_start: Mate alignment start position, 1-based (UInt32) - sequence: Read sequence (String) - quality_scores: Base quality scores (String)</p> Source code in <code>polars_bio/io.py</code> <pre><code>@staticmethod\ndef scan_cram(\n    path: str,\n    reference_path: str = None,\n    tag_fields: Union[list[str], None] = None,\n    chunk_size: int = 8,\n    concurrent_fetches: int = 1,\n    allow_anonymous: bool = True,\n    enable_request_payer: bool = False,\n    max_retries: int = 5,\n    timeout: int = 300,\n    projection_pushdown: bool = True,\n    predicate_pushdown: bool = True,\n    use_zero_based: Optional[bool] = None,\n) -&gt; pl.LazyFrame:\n    \"\"\"\n    Lazily read a CRAM file into a LazyFrame.\n\n    !!! hint \"Parallelism &amp; Indexed Reads\"\n        Indexed parallel reads and predicate pushdown are automatic when a CRAI index\n        is present. See [File formats support](/polars-bio/features/#file-formats-support),\n        [Indexed reads](/polars-bio/features/#indexed-reads-predicate-pushdown),\n        and [Automatic parallel partitioning](/polars-bio/features/#automatic-parallel-partitioning) for details.\n\n    Parameters:\n        path: The path to the CRAM file (local or cloud storage: S3, GCS, Azure Blob).\n        reference_path: Optional path to external FASTA reference file (**local path only**, cloud storage not supported). If not provided, the CRAM file must contain embedded reference sequences. The FASTA file must have an accompanying index file (.fai) in the same directory. Create the index using: `samtools faidx reference.fasta`\n        tag_fields: List of CRAM tag names to include as columns (e.g., [\"NM\", \"MD\", \"AS\"]). If None, no optional tags are parsed (default). Common tags include: NM (edit distance), MD (mismatch string), AS (alignment score), XS (secondary alignment score), RG (read group), CB (cell barcode), UB (UMI barcode).\n        chunk_size: The size in MB of a chunk when reading from an object store. The default is 8 MB. For large scale operations, it is recommended to increase this value to 64.\n        concurrent_fetches: [GCS] The number of concurrent fetches when reading from an object store. The default is 1. For large scale operations, it is recommended to increase this value to 8 or even more.\n        allow_anonymous: [GCS, AWS S3] Whether to allow anonymous access to object storage.\n        enable_request_payer: [AWS S3] Whether to enable request payer for object storage. This is useful for reading files from AWS S3 buckets that require request payer.\n        max_retries: The maximum number of retries for reading the file from object storage.\n        timeout: The timeout in seconds for reading the file from object storage.\n        projection_pushdown: Enable column projection pushdown optimization. When True, only requested columns are processed at the DataFusion execution level, improving performance and reducing memory usage.\n        predicate_pushdown: Enable predicate pushdown using index files (CRAI) for efficient region-based filtering. Index files are auto-discovered (e.g., `file.cram.crai`). Only simple predicates are pushed down (equality, comparisons, IN); complex predicates like `.str.contains()` or OR logic are filtered client-side. Correctness is always guaranteed.\n        use_zero_based: If True, output 0-based half-open coordinates. If False, output 1-based closed coordinates. If None (default), uses the global configuration `datafusion.bio.coordinate_system_zero_based`.\n\n    !!! note\n        By default, coordinates are output in **1-based closed** format. Use `use_zero_based=True` or set `pb.set_option(pb.POLARS_BIO_COORDINATE_SYSTEM_ZERO_BASED, True)` for 0-based half-open coordinates.\n\n    !!! warning \"Known Limitation: MD and NM Tags\"\n        Due to a limitation in the underlying noodles-cram library, **MD (mismatch descriptor) and NM (edit distance) tags are not accessible** from CRAM files, even when stored in the file. These tags can be seen with samtools but are not exposed through the noodles-cram record.data() interface.\n\n        Other optional tags (RG, MQ, AM, OQ, etc.) work correctly. This issue is tracked at: https://github.com/biodatageeks/datafusion-bio-formats/issues/54\n\n        **Workaround**: Use BAM format if MD/NM tags are required for your analysis.\n\n    !!! example \"Using External Reference\"\n        ```python\n        import polars_bio as pb\n\n        # Lazy scan CRAM with external reference\n        lf = pb.scan_cram(\n            \"/path/to/file.cram\",\n            reference_path=\"/path/to/reference.fasta\"\n        )\n\n        # Apply transformations and collect\n        df = lf.filter(pl.col(\"chrom\") == \"chr1\").collect()\n        ```\n\n    !!! example \"Public CRAM File Example\"\n        Download and read a public CRAM file from 42basepairs:\n        ```bash\n        # Download the CRAM file and reference\n        wget https://42basepairs.com/download/s3/gatk-test-data/wgs_cram/NA12878_20k_hg38/NA12878.cram\n        wget https://storage.googleapis.com/genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.fasta\n\n        # Create FASTA index (required)\n        samtools faidx Homo_sapiens_assembly38.fasta\n        ```\n\n        ```python\n        import polars_bio as pb\n        import polars as pl\n\n        # Lazy scan and filter for chromosome 20 reads\n        df = pb.scan_cram(\n            \"NA12878.cram\",\n            reference_path=\"Homo_sapiens_assembly38.fasta\"\n        ).filter(\n            pl.col(\"chrom\") == \"chr20\"\n        ).select(\n            [\"name\", \"chrom\", \"start\", \"end\", \"mapping_quality\"]\n        ).limit(10).collect()\n\n        print(df)\n        ```\n\n    !!! example \"Creating CRAM with Embedded Reference\"\n        To create a CRAM file with embedded reference using samtools:\n        ```bash\n        samtools view -C -o output.cram --output-fmt-option embed_ref=1 input.bam\n        ```\n\n    Returns:\n        A Polars LazyFrame with the following schema:\n            - name: Read name (String)\n            - chrom: Chromosome/contig name (String)\n            - start: Alignment start position, 1-based (UInt32)\n            - end: Alignment end position, 1-based (UInt32)\n            - flags: SAM flags (UInt32)\n            - cigar: CIGAR string (String)\n            - mapping_quality: Mapping quality (UInt32)\n            - mate_chrom: Mate chromosome/contig name (String)\n            - mate_start: Mate alignment start position, 1-based (UInt32)\n            - sequence: Read sequence (String)\n            - quality_scores: Base quality scores (String)\n    \"\"\"\n    object_storage_options = PyObjectStorageOptions(\n        allow_anonymous=allow_anonymous,\n        enable_request_payer=enable_request_payer,\n        chunk_size=chunk_size,\n        concurrent_fetches=concurrent_fetches,\n        max_retries=max_retries,\n        timeout=timeout,\n        compression_type=\"auto\",\n    )\n\n    zero_based = _resolve_zero_based(use_zero_based)\n    cram_read_options = CramReadOptions(\n        reference_path=reference_path,\n        object_storage_options=object_storage_options,\n        zero_based=zero_based,\n        tag_fields=tag_fields,\n    )\n    read_options = ReadOptions(cram_read_options=cram_read_options)\n    return _read_file(\n        path,\n        InputFormat.Cram,\n        read_options,\n        projection_pushdown,\n        predicate_pushdown,\n        zero_based=zero_based,\n    )\n</code></pre>"},{"location":"api/#polars_bio.data_input.scan_fasta","title":"<code>scan_fasta(path, chunk_size=8, concurrent_fetches=1, allow_anonymous=True, enable_request_payer=False, max_retries=5, timeout=300, compression_type='auto', projection_pushdown=True)</code>  <code>staticmethod</code>","text":"<p>Lazily read a FASTA file into a LazyFrame.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the FASTA file.</p> required <code>chunk_size</code> <code>int</code> <p>The size in MB of a chunk when reading from an object store. The default is 8 MB. For large scale operations, it is recommended to increase this value to 64.</p> <code>8</code> <code>concurrent_fetches</code> <code>int</code> <p>[GCS] The number of concurrent fetches when reading from an object store. The default is 1. For large scale operations, it is recommended to increase this value to 8 or even more.</p> <code>1</code> <code>allow_anonymous</code> <code>bool</code> <p>[GCS, AWS S3] Whether to allow anonymous access to object storage.</p> <code>True</code> <code>enable_request_payer</code> <code>bool</code> <p>[AWS S3] Whether to enable request payer for object storage. This is useful for reading files from AWS S3 buckets that require request payer.</p> <code>False</code> <code>max_retries</code> <code>int</code> <p>The maximum number of retries for reading the file from object storage.</p> <code>5</code> <code>timeout</code> <code>int</code> <p>The timeout in seconds for reading the file from object storage.</p> <code>300</code> <code>compression_type</code> <code>str</code> <p>The compression type of the FASTA file. If not specified, it will be detected automatically based on the file extension. BGZF and GZIP compressions are supported ('bgz', 'gz').</p> <code>'auto'</code> <code>projection_pushdown</code> <code>bool</code> <p>Enable column projection pushdown to optimize query performance by only reading the necessary columns at the DataFusion level.</p> <code>True</code> <p>Example</p> <pre><code>wget https://www.ebi.ac.uk/ena/browser/api/fasta/BK006935.2?download=true -O /tmp/test.fasta\n</code></pre> <p></p><pre><code>import polars_bio as pb\npb.scan_fasta(\"/tmp/test.fasta\").limit(1).collect()\n</code></pre> <pre><code> shape: (1, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 name                    \u2506 description                     \u2506 sequence                        \u2502\n\u2502 ---                     \u2506 ---                             \u2506 ---                             \u2502\n\u2502 str                     \u2506 str                             \u2506 str                             \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 ENA|BK006935|BK006935.2 \u2506 TPA_inf: Saccharomyces cerevis\u2026 \u2506 CCACACCACACCCACACACCCACACACCAC\u2026 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre><p></p> Source code in <code>polars_bio/io.py</code> <pre><code>@staticmethod\ndef scan_fasta(\n    path: str,\n    chunk_size: int = 8,\n    concurrent_fetches: int = 1,\n    allow_anonymous: bool = True,\n    enable_request_payer: bool = False,\n    max_retries: int = 5,\n    timeout: int = 300,\n    compression_type: str = \"auto\",\n    projection_pushdown: bool = True,\n) -&gt; pl.LazyFrame:\n    \"\"\"\n\n    Lazily read a FASTA file into a LazyFrame.\n\n    Parameters:\n        path: The path to the FASTA file.\n        chunk_size: The size in MB of a chunk when reading from an object store. The default is 8 MB. For large scale operations, it is recommended to increase this value to 64.\n        concurrent_fetches: [GCS] The number of concurrent fetches when reading from an object store. The default is 1. For large scale operations, it is recommended to increase this value to 8 or even more.\n        allow_anonymous: [GCS, AWS S3] Whether to allow anonymous access to object storage.\n        enable_request_payer: [AWS S3] Whether to enable request payer for object storage. This is useful for reading files from AWS S3 buckets that require request payer.\n        max_retries:  The maximum number of retries for reading the file from object storage.\n        timeout: The timeout in seconds for reading the file from object storage.\n        compression_type: The compression type of the FASTA file. If not specified, it will be detected automatically based on the file extension. BGZF and GZIP compressions are supported ('bgz', 'gz').\n        projection_pushdown: Enable column projection pushdown to optimize query performance by only reading the necessary columns at the DataFusion level.\n\n    !!! Example\n        ```shell\n        wget https://www.ebi.ac.uk/ena/browser/api/fasta/BK006935.2?download=true -O /tmp/test.fasta\n        ```\n\n        ```python\n        import polars_bio as pb\n        pb.scan_fasta(\"/tmp/test.fasta\").limit(1).collect()\n        ```\n        ```shell\n         shape: (1, 3)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 name                    \u2506 description                     \u2506 sequence                        \u2502\n        \u2502 ---                     \u2506 ---                             \u2506 ---                             \u2502\n        \u2502 str                     \u2506 str                             \u2506 str                             \u2502\n        \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n        \u2502 ENA|BK006935|BK006935.2 \u2506 TPA_inf: Saccharomyces cerevis\u2026 \u2506 CCACACCACACCCACACACCCACACACCAC\u2026 \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        ```\n    \"\"\"\n    object_storage_options = PyObjectStorageOptions(\n        allow_anonymous=allow_anonymous,\n        enable_request_payer=enable_request_payer,\n        chunk_size=chunk_size,\n        concurrent_fetches=concurrent_fetches,\n        max_retries=max_retries,\n        timeout=timeout,\n        compression_type=compression_type,\n    )\n    fasta_read_options = FastaReadOptions(\n        object_storage_options=object_storage_options\n    )\n    read_options = ReadOptions(fasta_read_options=fasta_read_options)\n    return _read_file(path, InputFormat.Fasta, read_options, projection_pushdown)\n</code></pre>"},{"location":"api/#polars_bio.data_input.scan_fastq","title":"<code>scan_fastq(path, chunk_size=8, concurrent_fetches=1, allow_anonymous=True, enable_request_payer=False, max_retries=5, timeout=300, compression_type='auto', projection_pushdown=True)</code>  <code>staticmethod</code>","text":"<p>Lazily read a FASTQ file into a LazyFrame.</p> <p>Parallelism &amp; Compression</p> <p>See File formats support, Compression, and Automatic parallel partitioning for details on parallel reads and supported compression types.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the FASTQ file.</p> required <code>chunk_size</code> <code>int</code> <p>The size in MB of a chunk when reading from an object store. The default is 8 MB. For large scale operations, it is recommended to increase this value to 64.</p> <code>8</code> <code>concurrent_fetches</code> <code>int</code> <p>[GCS] The number of concurrent fetches when reading from an object store. The default is 1. For large scale operations, it is recommended to increase this value to 8 or even more.</p> <code>1</code> <code>allow_anonymous</code> <code>bool</code> <p>[GCS, AWS S3] Whether to allow anonymous access to object storage.</p> <code>True</code> <code>enable_request_payer</code> <code>bool</code> <p>[AWS S3] Whether to enable request payer for object storage. This is useful for reading files from AWS S3 buckets that require request payer.</p> <code>False</code> <code>max_retries</code> <code>int</code> <p>The maximum number of retries for reading the file from object storage.</p> <code>5</code> <code>timeout</code> <code>int</code> <p>The timeout in seconds for reading the file from object storage.</p> <code>300</code> <code>compression_type</code> <code>str</code> <p>The compression type of the FASTQ file. If not specified, it will be detected automatically based on the file extension. BGZF and GZIP compressions are supported ('bgz', 'gz').</p> <code>'auto'</code> <code>projection_pushdown</code> <code>bool</code> <p>Enable column projection pushdown to optimize query performance by only reading the necessary columns at the DataFusion level.</p> <code>True</code> Source code in <code>polars_bio/io.py</code> <pre><code>@staticmethod\ndef scan_fastq(\n    path: str,\n    chunk_size: int = 8,\n    concurrent_fetches: int = 1,\n    allow_anonymous: bool = True,\n    enable_request_payer: bool = False,\n    max_retries: int = 5,\n    timeout: int = 300,\n    compression_type: str = \"auto\",\n    projection_pushdown: bool = True,\n) -&gt; pl.LazyFrame:\n    \"\"\"\n    Lazily read a FASTQ file into a LazyFrame.\n\n    !!! hint \"Parallelism &amp; Compression\"\n        See [File formats support](/polars-bio/features/#file-formats-support),\n        [Compression](/polars-bio/features/#compression),\n        and [Automatic parallel partitioning](/polars-bio/features/#automatic-parallel-partitioning) for details on parallel reads and supported compression types.\n\n    Parameters:\n        path: The path to the FASTQ file.\n        chunk_size: The size in MB of a chunk when reading from an object store. The default is 8 MB. For large scale operations, it is recommended to increase this value to 64.\n        concurrent_fetches: [GCS] The number of concurrent fetches when reading from an object store. The default is 1. For large scale operations, it is recommended to increase this value to 8 or even more.\n        allow_anonymous: [GCS, AWS S3] Whether to allow anonymous access to object storage.\n        enable_request_payer: [AWS S3] Whether to enable request payer for object storage. This is useful for reading files from AWS S3 buckets that require request payer.\n        max_retries:  The maximum number of retries for reading the file from object storage.\n        timeout: The timeout in seconds for reading the file from object storage.\n        compression_type: The compression type of the FASTQ file. If not specified, it will be detected automatically based on the file extension. BGZF and GZIP compressions are supported ('bgz', 'gz').\n        projection_pushdown: Enable column projection pushdown to optimize query performance by only reading the necessary columns at the DataFusion level.\n    \"\"\"\n    object_storage_options = PyObjectStorageOptions(\n        allow_anonymous=allow_anonymous,\n        enable_request_payer=enable_request_payer,\n        chunk_size=chunk_size,\n        concurrent_fetches=concurrent_fetches,\n        max_retries=max_retries,\n        timeout=timeout,\n        compression_type=compression_type,\n    )\n\n    fastq_read_options = FastqReadOptions(\n        object_storage_options=object_storage_options,\n    )\n    read_options = ReadOptions(fastq_read_options=fastq_read_options)\n    return _read_file(path, InputFormat.Fastq, read_options, projection_pushdown)\n</code></pre>"},{"location":"api/#polars_bio.data_input.scan_gff","title":"<code>scan_gff(path, attr_fields=None, chunk_size=8, concurrent_fetches=1, allow_anonymous=True, enable_request_payer=False, max_retries=5, timeout=300, compression_type='auto', projection_pushdown=True, predicate_pushdown=True, use_zero_based=None)</code>  <code>staticmethod</code>","text":"<p>Lazily read a GFF file into a LazyFrame.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the GFF file.</p> required <code>attr_fields</code> <code>Union[list[str], None]</code> <p>List of attribute field names to extract as separate columns. If None, attributes will be kept as a nested structure. Use this to extract specific attributes like 'ID', 'gene_name', 'gene_type', etc. as direct columns for easier access.</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>The size in MB of a chunk when reading from an object store. The default is 8 MB. For large scale operations, it is recommended to increase this value to 64.</p> <code>8</code> <code>concurrent_fetches</code> <code>int</code> <p>[GCS] The number of concurrent fetches when reading from an object store. The default is 1. For large-scale operations, it is recommended to increase this value to 8 or even more.</p> <code>1</code> <code>allow_anonymous</code> <code>bool</code> <p>[GCS, AWS S3] Whether to allow anonymous access to object storage.</p> <code>True</code> <code>enable_request_payer</code> <code>bool</code> <p>[AWS S3] Whether to enable request payer for object storage. This is useful for reading files from AWS S3 buckets that require request payer.</p> <code>False</code> <code>max_retries</code> <code>int</code> <p>The maximum number of retries for reading the file from object storage.</p> <code>5</code> <code>timeout</code> <code>int</code> <p>The timeout in seconds for reading the file from object storage.</p> <code>300</code> <code>compression_type</code> <code>str</code> <p>The compression type of the GFF file. If not specified, it will be detected automatically.</p> <code>'auto'</code> <code>projection_pushdown</code> <code>bool</code> <p>Enable column projection pushdown to optimize query performance by only reading the necessary columns at the DataFusion level.</p> <code>True</code> <code>predicate_pushdown</code> <code>bool</code> <p>Enable predicate pushdown using index files (TBI/CSI) for efficient region-based filtering. Index files are auto-discovered (e.g., <code>file.gff.gz.tbi</code>). Only simple predicates are pushed down (equality, comparisons, IN); complex predicates like <code>.str.contains()</code> or OR logic are filtered client-side. Correctness is always guaranteed.</p> <code>True</code> <code>use_zero_based</code> <code>Optional[bool]</code> <p>If True, output 0-based half-open coordinates. If False, output 1-based closed coordinates. If None (default), uses the global configuration <code>datafusion.bio.coordinate_system_zero_based</code>.</p> <code>None</code> <p>Note</p> <p>By default, coordinates are output in 1-based closed format. Use <code>use_zero_based=True</code> or set <code>pb.set_option(pb.POLARS_BIO_COORDINATE_SYSTEM_ZERO_BASED, True)</code> for 0-based half-open coordinates.</p> Source code in <code>polars_bio/io.py</code> <pre><code>@staticmethod\ndef scan_gff(\n    path: str,\n    attr_fields: Union[list[str], None] = None,\n    chunk_size: int = 8,\n    concurrent_fetches: int = 1,\n    allow_anonymous: bool = True,\n    enable_request_payer: bool = False,\n    max_retries: int = 5,\n    timeout: int = 300,\n    compression_type: str = \"auto\",\n    projection_pushdown: bool = True,\n    predicate_pushdown: bool = True,\n    use_zero_based: Optional[bool] = None,\n) -&gt; pl.LazyFrame:\n    \"\"\"\n    Lazily read a GFF file into a LazyFrame.\n\n    Parameters:\n        path: The path to the GFF file.\n        attr_fields: List of attribute field names to extract as separate columns. If *None*, attributes will be kept as a nested structure. Use this to extract specific attributes like 'ID', 'gene_name', 'gene_type', etc. as direct columns for easier access.\n        chunk_size: The size in MB of a chunk when reading from an object store. The default is 8 MB. For large scale operations, it is recommended to increase this value to 64.\n        concurrent_fetches: [GCS] The number of concurrent fetches when reading from an object store. The default is 1. For large-scale operations, it is recommended to increase this value to 8 or even more.\n        allow_anonymous: [GCS, AWS S3] Whether to allow anonymous access to object storage.\n        enable_request_payer: [AWS S3] Whether to enable request payer for object storage. This is useful for reading files from AWS S3 buckets that require request payer.\n        max_retries:  The maximum number of retries for reading the file from object storage.\n        timeout: The timeout in seconds for reading the file from object storage.\n        compression_type: The compression type of the GFF file. If not specified, it will be detected automatically.\n        projection_pushdown: Enable column projection pushdown to optimize query performance by only reading the necessary columns at the DataFusion level.\n        predicate_pushdown: Enable predicate pushdown using index files (TBI/CSI) for efficient region-based filtering. Index files are auto-discovered (e.g., `file.gff.gz.tbi`). Only simple predicates are pushed down (equality, comparisons, IN); complex predicates like `.str.contains()` or OR logic are filtered client-side. Correctness is always guaranteed.\n        use_zero_based: If True, output 0-based half-open coordinates. If False, output 1-based closed coordinates. If None (default), uses the global configuration `datafusion.bio.coordinate_system_zero_based`.\n\n    !!! note\n        By default, coordinates are output in **1-based closed** format. Use `use_zero_based=True` or set `pb.set_option(pb.POLARS_BIO_COORDINATE_SYSTEM_ZERO_BASED, True)` for 0-based half-open coordinates.\n    \"\"\"\n    object_storage_options = PyObjectStorageOptions(\n        allow_anonymous=allow_anonymous,\n        enable_request_payer=enable_request_payer,\n        chunk_size=chunk_size,\n        concurrent_fetches=concurrent_fetches,\n        max_retries=max_retries,\n        timeout=timeout,\n        compression_type=compression_type,\n    )\n\n    zero_based = _resolve_zero_based(use_zero_based)\n    gff_read_options = GffReadOptions(\n        attr_fields=attr_fields,\n        object_storage_options=object_storage_options,\n        zero_based=zero_based,\n    )\n    read_options = ReadOptions(gff_read_options=gff_read_options)\n    return _read_file(\n        path,\n        InputFormat.Gff,\n        read_options,\n        projection_pushdown,\n        predicate_pushdown,\n        zero_based=zero_based,\n    )\n</code></pre>"},{"location":"api/#polars_bio.data_input.scan_pairs","title":"<code>scan_pairs(path, chunk_size=8, concurrent_fetches=1, allow_anonymous=True, enable_request_payer=False, max_retries=5, timeout=300, compression_type='auto', projection_pushdown=True, predicate_pushdown=True, use_zero_based=None)</code>  <code>staticmethod</code>","text":"<p>Lazily read a Pairs (Hi-C) file into a LazyFrame.</p> <p>The Pairs format (4DN project) stores chromatin contact data with columns: readID, chr1, pos1, chr2, pos2, strand1, strand2.</p> <p>Parallelism &amp; Indexed Reads</p> <p>Indexed parallel reads and predicate pushdown are automatic when a TBI index is present. See File formats support and Indexed reads for details.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the Pairs file (.pairs, .pairs.gz, .pairs.bgz).</p> required <code>chunk_size</code> <code>int</code> <p>The size in MB of a chunk when reading from an object store.</p> <code>8</code> <code>concurrent_fetches</code> <code>int</code> <p>The number of concurrent fetches when reading from an object store.</p> <code>1</code> <code>allow_anonymous</code> <code>bool</code> <p>Whether to allow anonymous access to object storage.</p> <code>True</code> <code>enable_request_payer</code> <code>bool</code> <p>Whether to enable request payer for object storage.</p> <code>False</code> <code>max_retries</code> <code>int</code> <p>The maximum number of retries for reading the file from object storage.</p> <code>5</code> <code>timeout</code> <code>int</code> <p>The timeout in seconds for reading the file from object storage.</p> <code>300</code> <code>compression_type</code> <code>str</code> <p>The compression type. If not specified, it will be detected automatically.</p> <code>'auto'</code> <code>projection_pushdown</code> <code>bool</code> <p>Enable column projection pushdown to optimize query performance.</p> <code>True</code> <code>predicate_pushdown</code> <code>bool</code> <p>Enable predicate pushdown using index files (TBI) for efficient region-based filtering. Index files are auto-discovered (e.g., <code>file.pairs.gz.tbi</code>). Only simple predicates are pushed down (equality, comparisons, IN); complex predicates are filtered client-side. Correctness is always guaranteed.</p> <code>True</code> <code>use_zero_based</code> <code>Optional[bool]</code> <p>If True, output 0-based half-open coordinates. If False, output 1-based closed coordinates. If None (default), uses the global configuration <code>datafusion.bio.coordinate_system_zero_based</code>.</p> <code>None</code> <p>Note</p> <p>By default, coordinates are output in 1-based closed format. Use <code>use_zero_based=True</code> or set <code>pb.set_option(pb.POLARS_BIO_COORDINATE_SYSTEM_ZERO_BASED, True)</code> for 0-based half-open coordinates.</p> Source code in <code>polars_bio/io.py</code> <pre><code>@staticmethod\ndef scan_pairs(\n    path: str,\n    chunk_size: int = 8,\n    concurrent_fetches: int = 1,\n    allow_anonymous: bool = True,\n    enable_request_payer: bool = False,\n    max_retries: int = 5,\n    timeout: int = 300,\n    compression_type: str = \"auto\",\n    projection_pushdown: bool = True,\n    predicate_pushdown: bool = True,\n    use_zero_based: Optional[bool] = None,\n) -&gt; pl.LazyFrame:\n    \"\"\"\n    Lazily read a Pairs (Hi-C) file into a LazyFrame.\n\n    The Pairs format (4DN project) stores chromatin contact data with columns:\n    readID, chr1, pos1, chr2, pos2, strand1, strand2.\n\n    !!! hint \"Parallelism &amp; Indexed Reads\"\n        Indexed parallel reads and predicate pushdown are automatic when a TBI index\n        is present. See [File formats support](/polars-bio/features/#file-formats-support)\n        and [Indexed reads](/polars-bio/features/#indexed-reads-predicate-pushdown) for details.\n\n    Parameters:\n        path: The path to the Pairs file (.pairs, .pairs.gz, .pairs.bgz).\n        chunk_size: The size in MB of a chunk when reading from an object store.\n        concurrent_fetches: The number of concurrent fetches when reading from an object store.\n        allow_anonymous: Whether to allow anonymous access to object storage.\n        enable_request_payer: Whether to enable request payer for object storage.\n        max_retries: The maximum number of retries for reading the file from object storage.\n        timeout: The timeout in seconds for reading the file from object storage.\n        compression_type: The compression type. If not specified, it will be detected automatically.\n        projection_pushdown: Enable column projection pushdown to optimize query performance.\n        predicate_pushdown: Enable predicate pushdown using index files (TBI) for efficient region-based filtering. Index files are auto-discovered (e.g., `file.pairs.gz.tbi`). Only simple predicates are pushed down (equality, comparisons, IN); complex predicates are filtered client-side. Correctness is always guaranteed.\n        use_zero_based: If True, output 0-based half-open coordinates. If False, output 1-based closed coordinates. If None (default), uses the global configuration `datafusion.bio.coordinate_system_zero_based`.\n\n    !!! note\n        By default, coordinates are output in **1-based closed** format. Use `use_zero_based=True` or set `pb.set_option(pb.POLARS_BIO_COORDINATE_SYSTEM_ZERO_BASED, True)` for 0-based half-open coordinates.\n    \"\"\"\n    object_storage_options = PyObjectStorageOptions(\n        allow_anonymous=allow_anonymous,\n        enable_request_payer=enable_request_payer,\n        chunk_size=chunk_size,\n        concurrent_fetches=concurrent_fetches,\n        max_retries=max_retries,\n        timeout=timeout,\n        compression_type=compression_type,\n    )\n\n    zero_based = _resolve_zero_based(use_zero_based)\n    pairs_read_options = PairsReadOptions(\n        object_storage_options=object_storage_options,\n        zero_based=zero_based,\n    )\n    read_options = ReadOptions(pairs_read_options=pairs_read_options)\n    return _read_file(\n        path,\n        InputFormat.Pairs,\n        read_options,\n        projection_pushdown,\n        predicate_pushdown,\n        zero_based=zero_based,\n    )\n</code></pre>"},{"location":"api/#polars_bio.data_input.scan_sam","title":"<code>scan_sam(path, tag_fields=None, projection_pushdown=True, use_zero_based=None)</code>  <code>staticmethod</code>","text":"<p>Lazily read a SAM file into a LazyFrame.</p> <p>SAM (Sequence Alignment/Map) is the plain-text counterpart of BAM. This function reuses the BAM reader, which auto-detects the format from the file extension.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the SAM file.</p> required <code>tag_fields</code> <code>Union[list[str], None]</code> <p>List of SAM tag names to include as columns (e.g., [\"NM\", \"MD\", \"AS\"]). If None, no optional tags are parsed (default).</p> <code>None</code> <code>projection_pushdown</code> <code>bool</code> <p>Enable column projection pushdown to optimize query performance.</p> <code>True</code> <code>use_zero_based</code> <code>Optional[bool]</code> <p>If True, output 0-based half-open coordinates. If False, output 1-based closed coordinates. If None (default), uses the global configuration.</p> <code>None</code> <p>Note</p> <p>By default, coordinates are output in 1-based closed format.</p> Source code in <code>polars_bio/io.py</code> <pre><code>@staticmethod\ndef scan_sam(\n    path: str,\n    tag_fields: Union[list[str], None] = None,\n    projection_pushdown: bool = True,\n    use_zero_based: Optional[bool] = None,\n) -&gt; pl.LazyFrame:\n    \"\"\"\n    Lazily read a SAM file into a LazyFrame.\n\n    SAM (Sequence Alignment/Map) is the plain-text counterpart of BAM.\n    This function reuses the BAM reader, which auto-detects the format\n    from the file extension.\n\n    Parameters:\n        path: The path to the SAM file.\n        tag_fields: List of SAM tag names to include as columns (e.g., [\"NM\", \"MD\", \"AS\"]).\n            If None, no optional tags are parsed (default).\n        projection_pushdown: Enable column projection pushdown to optimize query performance.\n        use_zero_based: If True, output 0-based half-open coordinates.\n            If False, output 1-based closed coordinates.\n            If None (default), uses the global configuration.\n\n    !!! note\n        By default, coordinates are output in **1-based closed** format.\n    \"\"\"\n    zero_based = _resolve_zero_based(use_zero_based)\n    bam_read_options = BamReadOptions(\n        zero_based=zero_based,\n        tag_fields=tag_fields,\n    )\n    read_options = ReadOptions(bam_read_options=bam_read_options)\n    return _read_file(\n        path,\n        InputFormat.Sam,\n        read_options,\n        projection_pushdown,\n        zero_based=zero_based,\n    )\n</code></pre>"},{"location":"api/#polars_bio.data_input.scan_table","title":"<code>scan_table(path, schema=None, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Lazily read a tab-delimited (i.e. BED) file into a Polars LazyFrame.  Tries to be compatible with Bioframe's read_table  but faster and lazy. Schema should follow the Bioframe's schema format.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the file.</p> required <code>schema</code> <code>Dict</code> <p>Schema should follow the Bioframe's schema format.</p> <code>None</code> Source code in <code>polars_bio/io.py</code> <pre><code>@staticmethod\ndef scan_table(path: str, schema: Dict = None, **kwargs) -&gt; pl.LazyFrame:\n    \"\"\"\n     Lazily read a tab-delimited (i.e. BED) file into a Polars LazyFrame.\n     Tries to be compatible with Bioframe's [read_table](https://bioframe.readthedocs.io/en/latest/guide-io.html)\n     but faster and lazy. Schema should follow the Bioframe's schema [format](https://github.com/open2c/bioframe/blob/2b685eebef393c2c9e6220dcf550b3630d87518e/bioframe/io/schemas.py#L174).\n\n    Parameters:\n        path: The path to the file.\n        schema: Schema should follow the Bioframe's schema [format](https://github.com/open2c/bioframe/blob/2b685eebef393c2c9e6220dcf550b3630d87518e/bioframe/io/schemas.py#L174).\n    \"\"\"\n    df = pl.scan_csv(path, separator=\"\\t\", has_header=False, **kwargs)\n    if schema is not None:\n        columns = SCHEMAS[schema]\n        if len(columns) != len(df.collect_schema()):\n            raise ValueError(\n                f\"Schema incompatible with the input. Expected {len(columns)} columns in a schema, got {len(df.collect_schema())} in the input data file. Please provide a valid schema.\"\n            )\n        for i, c in enumerate(columns):\n            df = df.rename({f\"column_{i+1}\": c})\n    return df\n</code></pre>"},{"location":"api/#polars_bio.data_input.scan_vcf","title":"<code>scan_vcf(path, info_fields=None, format_fields=None, chunk_size=8, concurrent_fetches=1, allow_anonymous=True, enable_request_payer=False, max_retries=5, timeout=300, compression_type='auto', projection_pushdown=True, predicate_pushdown=True, use_zero_based=None)</code>  <code>staticmethod</code>","text":"<p>Lazily read a VCF file into a LazyFrame.</p> <p>Parallelism &amp; Indexed Reads</p> <p>Indexed parallel reads and predicate pushdown are automatic when a TBI/CSI index is present. See File formats support, Indexed reads, and Automatic parallel partitioning for details.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the VCF file.</p> required <code>info_fields</code> <code>Union[list[str], None]</code> <p>List of INFO field names to include. If None, all INFO fields from the VCF header are included by default. Use this to limit fields for better performance.</p> <code>None</code> <code>format_fields</code> <code>Union[list[str], None]</code> <p>List of FORMAT field names to include (per-sample genotype data). If None, all FORMAT fields are included by default. For single-sample VCFs, FORMAT fields are top-level columns (e.g., <code>GT</code>, <code>DP</code>). For multi-sample VCFs, FORMAT data is exposed as a nested <code>genotypes</code> column (<code>list&lt;struct&lt;sample_id, values&gt;&gt;</code>).</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>The size in MB of a chunk when reading from an object store. The default is 8 MB. For large scale operations, it is recommended to increase this value to 64.</p> <code>8</code> <code>concurrent_fetches</code> <code>int</code> <p>[GCS] The number of concurrent fetches when reading from an object store. The default is 1. For large scale operations, it is recommended to increase this value to 8 or even more.</p> <code>1</code> <code>allow_anonymous</code> <code>bool</code> <p>[GCS, AWS S3] Whether to allow anonymous access to object storage.</p> <code>True</code> <code>enable_request_payer</code> <code>bool</code> <p>[AWS S3] Whether to enable request payer for object storage. This is useful for reading files from AWS S3 buckets that require request payer.</p> <code>False</code> <code>max_retries</code> <code>int</code> <p>The maximum number of retries for reading the file from object storage.</p> <code>5</code> <code>timeout</code> <code>int</code> <p>The timeout in seconds for reading the file from object storage.</p> <code>300</code> <code>compression_type</code> <code>str</code> <p>The compression type of the VCF file. If not specified, it will be detected automatically..</p> <code>'auto'</code> <code>projection_pushdown</code> <code>bool</code> <p>Enable column projection pushdown to optimize query performance by only reading the necessary columns at the DataFusion level.</p> <code>True</code> <code>predicate_pushdown</code> <code>bool</code> <p>Enable predicate pushdown using index files (TBI/CSI) for efficient region-based filtering. Index files are auto-discovered (e.g., <code>file.vcf.gz.tbi</code>). Only simple predicates are pushed down (equality, comparisons, IN); complex predicates like <code>.str.contains()</code> or OR logic are filtered client-side. Correctness is always guaranteed.</p> <code>True</code> <code>use_zero_based</code> <code>Optional[bool]</code> <p>If True, output 0-based half-open coordinates. If False, output 1-based closed coordinates. If None (default), uses the global configuration <code>datafusion.bio.coordinate_system_zero_based</code>.</p> <code>None</code> <p>Note</p> <p>By default, coordinates are output in 1-based closed format. Use <code>use_zero_based=True</code> or set <code>pb.set_option(pb.POLARS_BIO_COORDINATE_SYSTEM_ZERO_BASED, True)</code> for 0-based half-open coordinates.</p> <p>Lazy scanning VCF with INFO and FORMAT fields</p> <pre><code>import polars_bio as pb\n\n# Lazily scan VCF with both INFO and FORMAT fields\nlf = pb.scan_vcf(\n    \"sample.vcf.gz\",\n    info_fields=[\"END\"],              # INFO field\n    format_fields=[\"GT\", \"DP\", \"GQ\"]  # FORMAT fields\n)\n\n# Apply filters and collect only what's needed\ndf = lf.filter(pl.col(\"DP\") &gt; 20).select(\n    [\"chrom\", \"start\", \"ref\", \"alt\", \"GT\", \"DP\", \"GQ\"]\n).collect()\n\n# Single-sample VCF: FORMAT fields are top-level columns (GT, DP, GQ)\n# Multi-sample VCF: FORMAT data is nested in \"genotypes\"\n</code></pre> Source code in <code>polars_bio/io.py</code> <pre><code>@staticmethod\ndef scan_vcf(\n    path: str,\n    info_fields: Union[list[str], None] = None,\n    format_fields: Union[list[str], None] = None,\n    chunk_size: int = 8,\n    concurrent_fetches: int = 1,\n    allow_anonymous: bool = True,\n    enable_request_payer: bool = False,\n    max_retries: int = 5,\n    timeout: int = 300,\n    compression_type: str = \"auto\",\n    projection_pushdown: bool = True,\n    predicate_pushdown: bool = True,\n    use_zero_based: Optional[bool] = None,\n) -&gt; pl.LazyFrame:\n    \"\"\"\n    Lazily read a VCF file into a LazyFrame.\n\n    !!! hint \"Parallelism &amp; Indexed Reads\"\n        Indexed parallel reads and predicate pushdown are automatic when a TBI/CSI index\n        is present. See [File formats support](/polars-bio/features/#file-formats-support),\n        [Indexed reads](/polars-bio/features/#indexed-reads-predicate-pushdown),\n        and [Automatic parallel partitioning](/polars-bio/features/#automatic-parallel-partitioning) for details.\n\n    Parameters:\n        path: The path to the VCF file.\n        info_fields: List of INFO field names to include. If *None*, all INFO fields from the VCF header are included by default. Use this to limit fields for better performance.\n        format_fields: List of FORMAT field names to include (per-sample genotype data). If *None*, all FORMAT fields are included by default. For **single-sample** VCFs, FORMAT fields are top-level columns (e.g., `GT`, `DP`). For **multi-sample** VCFs, FORMAT data is exposed as a nested `genotypes` column (`list&lt;struct&lt;sample_id, values&gt;&gt;`).\n        chunk_size: The size in MB of a chunk when reading from an object store. The default is 8 MB. For large scale operations, it is recommended to increase this value to 64.\n        concurrent_fetches: [GCS] The number of concurrent fetches when reading from an object store. The default is 1. For large scale operations, it is recommended to increase this value to 8 or even more.\n        allow_anonymous: [GCS, AWS S3] Whether to allow anonymous access to object storage.\n        enable_request_payer: [AWS S3] Whether to enable request payer for object storage. This is useful for reading files from AWS S3 buckets that require request payer.\n        max_retries:  The maximum number of retries for reading the file from object storage.\n        timeout: The timeout in seconds for reading the file from object storage.\n        compression_type: The compression type of the VCF file. If not specified, it will be detected automatically..\n        projection_pushdown: Enable column projection pushdown to optimize query performance by only reading the necessary columns at the DataFusion level.\n        predicate_pushdown: Enable predicate pushdown using index files (TBI/CSI) for efficient region-based filtering. Index files are auto-discovered (e.g., `file.vcf.gz.tbi`). Only simple predicates are pushed down (equality, comparisons, IN); complex predicates like `.str.contains()` or OR logic are filtered client-side. Correctness is always guaranteed.\n        use_zero_based: If True, output 0-based half-open coordinates. If False, output 1-based closed coordinates. If None (default), uses the global configuration `datafusion.bio.coordinate_system_zero_based`.\n\n    !!! note\n        By default, coordinates are output in **1-based closed** format. Use `use_zero_based=True` or set `pb.set_option(pb.POLARS_BIO_COORDINATE_SYSTEM_ZERO_BASED, True)` for 0-based half-open coordinates.\n\n    !!! Example \"Lazy scanning VCF with INFO and FORMAT fields\"\n        ```python\n        import polars_bio as pb\n\n        # Lazily scan VCF with both INFO and FORMAT fields\n        lf = pb.scan_vcf(\n            \"sample.vcf.gz\",\n            info_fields=[\"END\"],              # INFO field\n            format_fields=[\"GT\", \"DP\", \"GQ\"]  # FORMAT fields\n        )\n\n        # Apply filters and collect only what's needed\n        df = lf.filter(pl.col(\"DP\") &gt; 20).select(\n            [\"chrom\", \"start\", \"ref\", \"alt\", \"GT\", \"DP\", \"GQ\"]\n        ).collect()\n\n        # Single-sample VCF: FORMAT fields are top-level columns (GT, DP, GQ)\n        # Multi-sample VCF: FORMAT data is nested in \"genotypes\"\n        ```\n    \"\"\"\n    object_storage_options = PyObjectStorageOptions(\n        allow_anonymous=allow_anonymous,\n        enable_request_payer=enable_request_payer,\n        chunk_size=chunk_size,\n        concurrent_fetches=concurrent_fetches,\n        max_retries=max_retries,\n        timeout=timeout,\n        compression_type=compression_type,\n    )\n\n    # Upstream VCF reader projects all INFO fields by default when info_fields is None.\n    initial_info_fields = info_fields\n\n    zero_based = _resolve_zero_based(use_zero_based)\n    vcf_read_options = VcfReadOptions(\n        info_fields=initial_info_fields,\n        format_fields=format_fields,\n        object_storage_options=object_storage_options,\n        zero_based=zero_based,\n    )\n    read_options = ReadOptions(vcf_read_options=vcf_read_options)\n    return _read_file(\n        path,\n        InputFormat.Vcf,\n        read_options,\n        projection_pushdown,\n        predicate_pushdown,\n        zero_based=zero_based,\n    )\n</code></pre>"},{"location":"api/#polars_bio.data_input.sink_bam","title":"<code>sink_bam(lf, path, sort_on_write=False)</code>  <code>staticmethod</code>","text":"<p>Streaming write a LazyFrame to BAM/SAM format.</p> <p>For CRAM format, use <code>sink_cram()</code> instead.</p> <p>Parameters:</p> Name Type Description Default <code>lf</code> <code>LazyFrame</code> <p>LazyFrame to write</p> required <code>path</code> <code>str</code> <p>Output file path (.bam or .sam)</p> required <code>sort_on_write</code> <code>bool</code> <p>If True, sort records by (chrom, start) and set header SO:coordinate. If False (default), set header SO:unsorted.</p> <code>False</code> <p>Streaming write BAM</p> <pre><code>import polars_bio as pb\nlf = pb.scan_bam(\"input.bam\").filter(pl.col(\"mapping_quality\") &gt; 20)\npb.sink_bam(lf, \"filtered.bam\")\n</code></pre> Source code in <code>polars_bio/io.py</code> <pre><code>@staticmethod\ndef sink_bam(\n    lf: pl.LazyFrame,\n    path: str,\n    sort_on_write: bool = False,\n) -&gt; None:\n    \"\"\"\n    Streaming write a LazyFrame to BAM/SAM format.\n\n    For CRAM format, use `sink_cram()` instead.\n\n    Parameters:\n        lf: LazyFrame to write\n        path: Output file path (.bam or .sam)\n        sort_on_write: If True, sort records by (chrom, start) and set header SO:coordinate.\n            If False (default), set header SO:unsorted.\n\n    !!! Example \"Streaming write BAM\"\n        ```python\n        import polars_bio as pb\n        lf = pb.scan_bam(\"input.bam\").filter(pl.col(\"mapping_quality\") &gt; 20)\n        pb.sink_bam(lf, \"filtered.bam\")\n        ```\n    \"\"\"\n    _write_bam_file(lf, path, OutputFormat.Bam, None, sort_on_write=sort_on_write)\n</code></pre>"},{"location":"api/#polars_bio.data_input.sink_cram","title":"<code>sink_cram(lf, path, reference_path, sort_on_write=False)</code>  <code>staticmethod</code>","text":"<p>Streaming write a LazyFrame to CRAM format.</p> <p>CRAM uses reference-based compression, storing only differences from the reference sequence. This method streams data without materializing all rows in memory.</p> <p>Parameters:</p> Name Type Description Default <code>lf</code> <code>LazyFrame</code> <p>LazyFrame to write</p> required <code>path</code> <code>str</code> <p>Output CRAM file path</p> required <code>reference_path</code> <code>str</code> <p>Path to reference FASTA file (required). The reference must contain all sequences referenced by the alignment data.</p> required <code>sort_on_write</code> <code>bool</code> <p>If True, sort records by (chrom, start) and set header SO:coordinate. If False (default), set header SO:unsorted.</p> <code>False</code> <p>Known Limitation: MD and NM Tags</p> <p>Due to a limitation in the underlying noodles-cram library, MD and NM tags cannot be read back from CRAM files after writing, even though they are written to the file. If you need MD/NM tags for downstream analysis, use BAM format instead. Other optional tags (RG, MQ, AM, OQ, AS, etc.) work correctly. See: https://github.com/biodatageeks/datafusion-bio-formats/issues/54</p> <p>Streaming write CRAM</p> <pre><code>import polars_bio as pb\nimport polars as pl\n\nlf = pb.scan_bam(\"large_input.bam\")\nlf = lf.filter(pl.col(\"mapping_quality\") &gt; 30)\n\n# Write CRAM with reference (required)\npb.sink_cram(lf, \"filtered.cram\", reference_path=\"reference.fasta\")\n\n# For sorted output\npb.sink_cram(lf, \"filtered.cram\", reference_path=\"reference.fasta\", sort_on_write=True)\n</code></pre> Source code in <code>polars_bio/io.py</code> <pre><code>@staticmethod\ndef sink_cram(\n    lf: pl.LazyFrame,\n    path: str,\n    reference_path: str,\n    sort_on_write: bool = False,\n) -&gt; None:\n    \"\"\"\n    Streaming write a LazyFrame to CRAM format.\n\n    CRAM uses reference-based compression, storing only differences from the\n    reference sequence. This method streams data without materializing all\n    rows in memory.\n\n    Parameters:\n        lf: LazyFrame to write\n        path: Output CRAM file path\n        reference_path: Path to reference FASTA file (required). The reference must\n            contain all sequences referenced by the alignment data.\n        sort_on_write: If True, sort records by (chrom, start) and set header SO:coordinate.\n            If False (default), set header SO:unsorted.\n\n    !!! warning \"Known Limitation: MD and NM Tags\"\n        Due to a limitation in the underlying noodles-cram library, **MD and NM tags cannot be read back from CRAM files** after writing, even though they are written to the file. If you need MD/NM tags for downstream analysis, use BAM format instead. Other optional tags (RG, MQ, AM, OQ, AS, etc.) work correctly. See: https://github.com/biodatageeks/datafusion-bio-formats/issues/54\n\n    !!! Example \"Streaming write CRAM\"\n        ```python\n        import polars_bio as pb\n        import polars as pl\n\n        lf = pb.scan_bam(\"large_input.bam\")\n        lf = lf.filter(pl.col(\"mapping_quality\") &gt; 30)\n\n        # Write CRAM with reference (required)\n        pb.sink_cram(lf, \"filtered.cram\", reference_path=\"reference.fasta\")\n\n        # For sorted output\n        pb.sink_cram(lf, \"filtered.cram\", reference_path=\"reference.fasta\", sort_on_write=True)\n        ```\n    \"\"\"\n    _write_bam_file(\n        lf, path, OutputFormat.Cram, reference_path, sort_on_write=sort_on_write\n    )\n</code></pre>"},{"location":"api/#polars_bio.data_input.sink_fastq","title":"<code>sink_fastq(lf, path)</code>  <code>staticmethod</code>","text":"<p>Streaming write a LazyFrame to FASTQ format.</p> <p>Compression is auto-detected from the file extension.</p> <p>Parameters:</p> Name Type Description Default <code>lf</code> <code>LazyFrame</code> <p>The LazyFrame to write.</p> required <code>path</code> <code>str</code> <p>The output file path. Compression is auto-detected from extension   (.fastq.bgz for BGZF, .fastq.gz for GZIP, .fastq for uncompressed).</p> required <p>Streaming write FASTQ</p> <pre><code>import polars_bio as pb\n\n# Lazy read, filter by quality, then sink\nlf = pb.scan_fastq(\"large_input.fastq.gz\")\npb.sink_fastq(lf.limit(1000), \"sample_output.fastq\")\n</code></pre> Source code in <code>polars_bio/io.py</code> <pre><code>@staticmethod\ndef sink_fastq(\n    lf: pl.LazyFrame,\n    path: str,\n) -&gt; None:\n    \"\"\"\n    Streaming write a LazyFrame to FASTQ format.\n\n    Compression is auto-detected from the file extension.\n\n    Parameters:\n        lf: The LazyFrame to write.\n        path: The output file path. Compression is auto-detected from extension\n              (.fastq.bgz for BGZF, .fastq.gz for GZIP, .fastq for uncompressed).\n\n    !!! Example \"Streaming write FASTQ\"\n        ```python\n        import polars_bio as pb\n\n        # Lazy read, filter by quality, then sink\n        lf = pb.scan_fastq(\"large_input.fastq.gz\")\n        pb.sink_fastq(lf.limit(1000), \"sample_output.fastq\")\n        ```\n    \"\"\"\n    _write_file(lf, path, OutputFormat.Fastq)\n</code></pre>"},{"location":"api/#polars_bio.data_input.sink_sam","title":"<code>sink_sam(lf, path, sort_on_write=False)</code>  <code>staticmethod</code>","text":"<p>Streaming write a LazyFrame to SAM format (plain text).</p> <p>Parameters:</p> Name Type Description Default <code>lf</code> <code>LazyFrame</code> <p>LazyFrame to write</p> required <code>path</code> <code>str</code> <p>Output file path (.sam)</p> required <code>sort_on_write</code> <code>bool</code> <p>If True, sort records by (chrom, start) and set header SO:coordinate. If False (default), set header SO:unsorted.</p> <code>False</code> <p>Streaming write SAM</p> <pre><code>import polars_bio as pb\nlf = pb.scan_bam(\"input.bam\").filter(pl.col(\"mapping_quality\") &gt; 20)\npb.sink_sam(lf, \"filtered.sam\")\n</code></pre> Source code in <code>polars_bio/io.py</code> <pre><code>@staticmethod\ndef sink_sam(\n    lf: pl.LazyFrame,\n    path: str,\n    sort_on_write: bool = False,\n) -&gt; None:\n    \"\"\"\n    Streaming write a LazyFrame to SAM format (plain text).\n\n    Parameters:\n        lf: LazyFrame to write\n        path: Output file path (.sam)\n        sort_on_write: If True, sort records by (chrom, start) and set header SO:coordinate.\n            If False (default), set header SO:unsorted.\n\n    !!! Example \"Streaming write SAM\"\n        ```python\n        import polars_bio as pb\n        lf = pb.scan_bam(\"input.bam\").filter(pl.col(\"mapping_quality\") &gt; 20)\n        pb.sink_sam(lf, \"filtered.sam\")\n        ```\n    \"\"\"\n    _write_bam_file(lf, path, OutputFormat.Sam, None, sort_on_write=sort_on_write)\n</code></pre>"},{"location":"api/#polars_bio.data_input.sink_vcf","title":"<code>sink_vcf(lf, path)</code>  <code>staticmethod</code>","text":"<p>Streaming write a LazyFrame to VCF format.</p> <p>This method executes the LazyFrame immediately and writes the results to the specified path. Unlike <code>write_vcf</code>, it doesn't return the row count.</p> <p>Coordinate system is automatically read from LazyFrame metadata (set during scan_vcf). Compression is auto-detected from the file extension.</p> <p>Parameters:</p> Name Type Description Default <code>lf</code> <code>LazyFrame</code> <p>The LazyFrame to write.</p> required <code>path</code> <code>str</code> <p>The output file path. Compression is auto-detected from extension   (.vcf.bgz for BGZF, .vcf.gz for GZIP, .vcf for uncompressed).</p> required <p>Streaming write VCF</p> <pre><code>import polars_bio as pb\n\n# Lazy read and filter, then sink to VCF\nlf = pb.scan_vcf(\"large_input.vcf\").filter(pl.col(\"qual\") &gt; 30)\npb.sink_vcf(lf, \"filtered_output.vcf.bgz\")\n</code></pre> Source code in <code>polars_bio/io.py</code> <pre><code>@staticmethod\ndef sink_vcf(\n    lf: pl.LazyFrame,\n    path: str,\n) -&gt; None:\n    \"\"\"\n    Streaming write a LazyFrame to VCF format.\n\n    This method executes the LazyFrame immediately and writes the results\n    to the specified path. Unlike `write_vcf`, it doesn't return the row count.\n\n    Coordinate system is automatically read from LazyFrame metadata (set during\n    scan_vcf). Compression is auto-detected from the file extension.\n\n    Parameters:\n        lf: The LazyFrame to write.\n        path: The output file path. Compression is auto-detected from extension\n              (.vcf.bgz for BGZF, .vcf.gz for GZIP, .vcf for uncompressed).\n\n    !!! Example \"Streaming write VCF\"\n        ```python\n        import polars_bio as pb\n\n        # Lazy read and filter, then sink to VCF\n        lf = pb.scan_vcf(\"large_input.vcf\").filter(pl.col(\"qual\") &gt; 30)\n        pb.sink_vcf(lf, \"filtered_output.vcf.bgz\")\n        ```\n    \"\"\"\n    _write_file(lf, path, OutputFormat.Vcf)\n</code></pre>"},{"location":"api/#polars_bio.data_input.write_bam","title":"<code>write_bam(df, path, sort_on_write=False)</code>  <code>staticmethod</code>","text":"<p>Write a DataFrame to BAM/SAM format.</p> <p>Compression is auto-detected from file extension: - .sam \u2192 Uncompressed SAM (plain text) - .bam \u2192 BGZF-compressed BAM</p> <p>For CRAM format, use <code>write_cram()</code> instead.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Union[DataFrame, LazyFrame]</code> <p>DataFrame or LazyFrame with 11 core BAM columns + optional tag columns</p> required <code>path</code> <code>str</code> <p>Output file path (.bam or .sam)</p> required <code>sort_on_write</code> <code>bool</code> <p>If True, sort records by (chrom, start) and set header SO:coordinate. If False (default), set header SO:unsorted.</p> <code>False</code> <p>Returns:</p> Type Description <code>int</code> <p>Number of rows written</p> <p>Write BAM files</p> <pre><code>import polars_bio as pb\ndf = pb.read_bam(\"input.bam\", tag_fields=[\"NM\", \"AS\"])\npb.write_bam(df, \"output.bam\")\npb.write_bam(df, \"output.sam\")\n</code></pre> Source code in <code>polars_bio/io.py</code> <pre><code>@staticmethod\ndef write_bam(\n    df: Union[pl.DataFrame, pl.LazyFrame],\n    path: str,\n    sort_on_write: bool = False,\n) -&gt; int:\n    \"\"\"\n    Write a DataFrame to BAM/SAM format.\n\n    Compression is auto-detected from file extension:\n    - .sam \u2192 Uncompressed SAM (plain text)\n    - .bam \u2192 BGZF-compressed BAM\n\n    For CRAM format, use `write_cram()` instead.\n\n    Parameters:\n        df: DataFrame or LazyFrame with 11 core BAM columns + optional tag columns\n        path: Output file path (.bam or .sam)\n        sort_on_write: If True, sort records by (chrom, start) and set header SO:coordinate.\n            If False (default), set header SO:unsorted.\n\n    Returns:\n        Number of rows written\n\n    !!! Example \"Write BAM files\"\n        ```python\n        import polars_bio as pb\n        df = pb.read_bam(\"input.bam\", tag_fields=[\"NM\", \"AS\"])\n        pb.write_bam(df, \"output.bam\")\n        pb.write_bam(df, \"output.sam\")\n        ```\n    \"\"\"\n    return _write_bam_file(\n        df, path, OutputFormat.Bam, None, sort_on_write=sort_on_write\n    )\n</code></pre>"},{"location":"api/#polars_bio.data_input.write_cram","title":"<code>write_cram(df, path, reference_path, sort_on_write=False)</code>  <code>staticmethod</code>","text":"<p>Write a DataFrame to CRAM format.</p> <p>CRAM uses reference-based compression, storing only differences from the reference sequence. This achieves 30-60% better compression than BAM.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Union[DataFrame, LazyFrame]</code> <p>DataFrame or LazyFrame with 11 core BAM columns + optional tag columns</p> required <code>path</code> <code>str</code> <p>Output CRAM file path</p> required <code>reference_path</code> <code>str</code> <p>Path to reference FASTA file (required). The reference must contain all sequences referenced by the alignment data.</p> required <code>sort_on_write</code> <code>bool</code> <p>If True, sort records by (chrom, start) and set header SO:coordinate. If False (default), set header SO:unsorted.</p> <code>False</code> <p>Returns:</p> Type Description <code>int</code> <p>Number of rows written</p> <p>Known Limitation: MD and NM Tags</p> <p>Due to a limitation in the underlying noodles-cram library, MD and NM tags cannot be read back from CRAM files after writing, even though they are written to the file. If you need MD/NM tags for downstream analysis, use BAM format instead. Other optional tags (RG, MQ, AM, OQ, AS, etc.) work correctly. See: https://github.com/biodatageeks/datafusion-bio-formats/issues/54</p> <p>Write CRAM files</p> <pre><code>import polars_bio as pb\n\ndf = pb.read_bam(\"input.bam\", tag_fields=[\"NM\", \"AS\"])\n\n# Write CRAM with reference (required)\npb.write_cram(df, \"output.cram\", reference_path=\"reference.fasta\")\n\n# For sorted output\npb.write_cram(df, \"output.cram\", reference_path=\"reference.fasta\", sort_on_write=True)\n</code></pre> Source code in <code>polars_bio/io.py</code> <pre><code>@staticmethod\ndef write_cram(\n    df: Union[pl.DataFrame, pl.LazyFrame],\n    path: str,\n    reference_path: str,\n    sort_on_write: bool = False,\n) -&gt; int:\n    \"\"\"\n    Write a DataFrame to CRAM format.\n\n    CRAM uses reference-based compression, storing only differences from the\n    reference sequence. This achieves 30-60% better compression than BAM.\n\n    Parameters:\n        df: DataFrame or LazyFrame with 11 core BAM columns + optional tag columns\n        path: Output CRAM file path\n        reference_path: Path to reference FASTA file (required). The reference must\n            contain all sequences referenced by the alignment data.\n        sort_on_write: If True, sort records by (chrom, start) and set header SO:coordinate.\n            If False (default), set header SO:unsorted.\n\n    Returns:\n        Number of rows written\n\n    !!! warning \"Known Limitation: MD and NM Tags\"\n        Due to a limitation in the underlying noodles-cram library, **MD and NM tags cannot be read back from CRAM files** after writing, even though they are written to the file. If you need MD/NM tags for downstream analysis, use BAM format instead. Other optional tags (RG, MQ, AM, OQ, AS, etc.) work correctly. See: https://github.com/biodatageeks/datafusion-bio-formats/issues/54\n\n    !!! Example \"Write CRAM files\"\n        ```python\n        import polars_bio as pb\n\n        df = pb.read_bam(\"input.bam\", tag_fields=[\"NM\", \"AS\"])\n\n        # Write CRAM with reference (required)\n        pb.write_cram(df, \"output.cram\", reference_path=\"reference.fasta\")\n\n        # For sorted output\n        pb.write_cram(df, \"output.cram\", reference_path=\"reference.fasta\", sort_on_write=True)\n        ```\n    \"\"\"\n    return _write_bam_file(\n        df, path, OutputFormat.Cram, reference_path, sort_on_write=sort_on_write\n    )\n</code></pre>"},{"location":"api/#polars_bio.data_input.write_fastq","title":"<code>write_fastq(df, path)</code>  <code>staticmethod</code>","text":"<p>Write a DataFrame to FASTQ format.</p> <p>Compression is auto-detected from the file extension.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Union[DataFrame, LazyFrame]</code> <p>The DataFrame or LazyFrame to write. Must have columns: - name: Read name/identifier - sequence: DNA sequence - quality_scores: Quality scores string Optional: description (added after name on header line)</p> required <code>path</code> <code>str</code> <p>The output file path. Compression is auto-detected from extension   (.fastq.bgz for BGZF, .fastq.gz for GZIP, .fastq for uncompressed).</p> required <p>Returns:</p> Type Description <code>int</code> <p>The number of rows written.</p> <p>Writing FASTQ files</p> <pre><code>import polars_bio as pb\n\n# Read a FASTQ file\ndf = pb.read_fastq(\"input.fastq\")\n\n# Write to uncompressed FASTQ\npb.write_fastq(df, \"output.fastq\")\n\n# Write to GZIP-compressed FASTQ\npb.write_fastq(df, \"output.fastq.gz\")\n</code></pre> Source code in <code>polars_bio/io.py</code> <pre><code>@staticmethod\ndef write_fastq(\n    df: Union[pl.DataFrame, pl.LazyFrame],\n    path: str,\n) -&gt; int:\n    \"\"\"\n    Write a DataFrame to FASTQ format.\n\n    Compression is auto-detected from the file extension.\n\n    Parameters:\n        df: The DataFrame or LazyFrame to write. Must have columns:\n            - name: Read name/identifier\n            - sequence: DNA sequence\n            - quality_scores: Quality scores string\n            Optional: description (added after name on header line)\n        path: The output file path. Compression is auto-detected from extension\n              (.fastq.bgz for BGZF, .fastq.gz for GZIP, .fastq for uncompressed).\n\n    Returns:\n        The number of rows written.\n\n    !!! Example \"Writing FASTQ files\"\n        ```python\n        import polars_bio as pb\n\n        # Read a FASTQ file\n        df = pb.read_fastq(\"input.fastq\")\n\n        # Write to uncompressed FASTQ\n        pb.write_fastq(df, \"output.fastq\")\n\n        # Write to GZIP-compressed FASTQ\n        pb.write_fastq(df, \"output.fastq.gz\")\n        ```\n    \"\"\"\n    return _write_file(df, path, OutputFormat.Fastq)\n</code></pre>"},{"location":"api/#polars_bio.data_input.write_sam","title":"<code>write_sam(df, path, sort_on_write=False)</code>  <code>staticmethod</code>","text":"<p>Write a DataFrame to SAM format (plain text).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Union[DataFrame, LazyFrame]</code> <p>DataFrame or LazyFrame with 11 core BAM/SAM columns + optional tag columns</p> required <code>path</code> <code>str</code> <p>Output file path (.sam)</p> required <code>sort_on_write</code> <code>bool</code> <p>If True, sort records by (chrom, start) and set header SO:coordinate. If False (default), set header SO:unsorted.</p> <code>False</code> <p>Returns:</p> Type Description <code>int</code> <p>Number of rows written</p> <p>Write SAM files</p> <pre><code>import polars_bio as pb\ndf = pb.read_bam(\"input.bam\", tag_fields=[\"NM\", \"AS\"])\npb.write_sam(df, \"output.sam\")\n</code></pre> Source code in <code>polars_bio/io.py</code> <pre><code>@staticmethod\ndef write_sam(\n    df: Union[pl.DataFrame, pl.LazyFrame],\n    path: str,\n    sort_on_write: bool = False,\n) -&gt; int:\n    \"\"\"\n    Write a DataFrame to SAM format (plain text).\n\n    Parameters:\n        df: DataFrame or LazyFrame with 11 core BAM/SAM columns + optional tag columns\n        path: Output file path (.sam)\n        sort_on_write: If True, sort records by (chrom, start) and set header SO:coordinate.\n            If False (default), set header SO:unsorted.\n\n    Returns:\n        Number of rows written\n\n    !!! Example \"Write SAM files\"\n        ```python\n        import polars_bio as pb\n        df = pb.read_bam(\"input.bam\", tag_fields=[\"NM\", \"AS\"])\n        pb.write_sam(df, \"output.sam\")\n        ```\n    \"\"\"\n    return _write_bam_file(\n        df, path, OutputFormat.Sam, None, sort_on_write=sort_on_write\n    )\n</code></pre>"},{"location":"api/#polars_bio.data_input.write_vcf","title":"<code>write_vcf(df, path)</code>  <code>staticmethod</code>","text":"<p>Write a DataFrame to VCF format.</p> <p>Coordinate system is automatically read from DataFrame metadata (set during read_vcf). Compression is auto-detected from the file extension.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Union[DataFrame, LazyFrame]</code> <p>The DataFrame or LazyFrame to write.</p> required <code>path</code> <code>str</code> <p>The output file path. Compression is auto-detected from extension   (.vcf.bgz for BGZF, .vcf.gz for GZIP, .vcf for uncompressed).</p> required <p>Returns:</p> Type Description <code>int</code> <p>The number of rows written.</p> <p>Writing VCF files</p> <pre><code>import polars_bio as pb\n\n# Read a VCF file\ndf = pb.read_vcf(\"input.vcf\")\n\n# Write to uncompressed VCF\npb.write_vcf(df, \"output.vcf\")\n\n# Write to BGZF-compressed VCF\npb.write_vcf(df, \"output.vcf.bgz\")\n\n# Write to GZIP-compressed VCF\npb.write_vcf(df, \"output.vcf.gz\")\n</code></pre> Source code in <code>polars_bio/io.py</code> <pre><code>@staticmethod\ndef write_vcf(\n    df: Union[pl.DataFrame, pl.LazyFrame],\n    path: str,\n) -&gt; int:\n    \"\"\"\n    Write a DataFrame to VCF format.\n\n    Coordinate system is automatically read from DataFrame metadata (set during\n    read_vcf). Compression is auto-detected from the file extension.\n\n    Parameters:\n        df: The DataFrame or LazyFrame to write.\n        path: The output file path. Compression is auto-detected from extension\n              (.vcf.bgz for BGZF, .vcf.gz for GZIP, .vcf for uncompressed).\n\n    Returns:\n        The number of rows written.\n\n    !!! Example \"Writing VCF files\"\n        ```python\n        import polars_bio as pb\n\n        # Read a VCF file\n        df = pb.read_vcf(\"input.vcf\")\n\n        # Write to uncompressed VCF\n        pb.write_vcf(df, \"output.vcf\")\n\n        # Write to BGZF-compressed VCF\n        pb.write_vcf(df, \"output.vcf.bgz\")\n\n        # Write to GZIP-compressed VCF\n        pb.write_vcf(df, \"output.vcf.gz\")\n        ```\n    \"\"\"\n    return _write_file(df, path, OutputFormat.Vcf)\n</code></pre>"},{"location":"api/#polars_bio.data_processing","title":"<code>data_processing</code>","text":"Source code in <code>polars_bio/sql.py</code> <pre><code>class SQL:\n    @staticmethod\n    def register_vcf(\n        path: str,\n        name: Union[str, None] = None,\n        info_fields: Union[list[str], None] = None,\n        chunk_size: int = 64,\n        concurrent_fetches: int = 8,\n        allow_anonymous: bool = True,\n        max_retries: int = 5,\n        timeout: int = 300,\n        enable_request_payer: bool = False,\n        compression_type: str = \"auto\",\n    ) -&gt; None:\n        \"\"\"\n        Register a VCF file as a Datafusion table.\n\n        Parameters:\n            path: The path to the VCF file.\n            name: The name of the table. If *None*, the name of the table will be generated automatically based on the path.\n            info_fields: List of INFO field names to register. If *None*, all INFO fields will be detected automatically from the VCF header. Use this to limit registration to specific fields for better performance.\n            chunk_size: The size in MB of a chunk when reading from an object store. Default settings are optimized for large scale operations. For small scale (interactive) operations, it is recommended to decrease this value to **8-16**.\n            concurrent_fetches: [GCS] The number of concurrent fetches when reading from an object store. Default settings are optimized for large scale operations. For small scale (interactive) operations, it is recommended to decrease this value to **1-2**.\n            allow_anonymous: [GCS, AWS S3] Whether to allow anonymous access to object storage.\n            enable_request_payer: [AWS S3] Whether to enable request payer for object storage. This is useful for reading files from AWS S3 buckets that require request payer.\n            compression_type: The compression type of the VCF file. If not specified, it will be detected automatically..\n            max_retries:  The maximum number of retries for reading the file from object storage.\n            timeout: The timeout in seconds for reading the file from object storage.\n        !!! note\n            VCF reader uses **1-based** coordinate system for the `start` and `end` columns.\n\n        !!! Example\n              ```python\n              import polars_bio as pb\n              pb.register_vcf(\"/tmp/gnomad.v4.1.sv.sites.vcf.gz\")\n              ```\n             ```shell\n             INFO:polars_bio:Table: gnomad_v4_1_sv_sites_gz registered for path: /tmp/gnomad.v4.1.sv.sites.vcf.gz\n             ```\n        !!! tip\n            `chunk_size` and `concurrent_fetches` can be adjusted according to the network bandwidth and the size of the VCF file. As a rule of thumb for large scale operations (reading a whole VCF), it is recommended to the default values.\n        \"\"\"\n\n        object_storage_options = PyObjectStorageOptions(\n            allow_anonymous=allow_anonymous,\n            enable_request_payer=enable_request_payer,\n            chunk_size=chunk_size,\n            concurrent_fetches=concurrent_fetches,\n            max_retries=max_retries,\n            timeout=timeout,\n            compression_type=compression_type,\n        )\n\n        # Use provided info_fields or autodetect from VCF header\n        if info_fields is not None:\n            all_info_fields = info_fields\n        else:\n            # Get all info fields from VCF header for automatic field detection\n            all_info_fields = None\n            try:\n                from .io import IOOperations\n\n                vcf_schema_df = IOOperations.describe_vcf(\n                    path,\n                    allow_anonymous=allow_anonymous,\n                    enable_request_payer=enable_request_payer,\n                    compression_type=compression_type,\n                )\n                all_info_fields = vcf_schema_df.select(\"name\").to_series().to_list()\n            except Exception:\n                # Fallback to empty list if unable to get info fields\n                all_info_fields = []\n\n        vcf_read_options = VcfReadOptions(\n            info_fields=all_info_fields,\n            object_storage_options=object_storage_options,\n        )\n        read_options = ReadOptions(vcf_read_options=vcf_read_options)\n        py_register_table(ctx, path, name, InputFormat.Vcf, read_options)\n\n    @staticmethod\n    def register_gff(\n        path: str,\n        name: Union[str, None] = None,\n        chunk_size: int = 64,\n        concurrent_fetches: int = 8,\n        allow_anonymous: bool = True,\n        max_retries: int = 5,\n        timeout: int = 300,\n        enable_request_payer: bool = False,\n        compression_type: str = \"auto\",\n    ) -&gt; None:\n        \"\"\"\n        Register a GFF file as a Datafusion table.\n\n        Parameters:\n            path: The path to the GFF file.\n            name: The name of the table. If *None*, the name of the table will be generated automatically based on the path.\n            chunk_size: The size in MB of a chunk when reading from an object store. Default settings are optimized for large scale operations. For small scale (interactive) operations, it is recommended to decrease this value to **8-16**.\n            concurrent_fetches: [GCS] The number of concurrent fetches when reading from an object store. Default settings are optimized for large scale operations. For small scale (interactive) operations, it is recommended to decrease this value to **1-2**.\n            allow_anonymous: [GCS, AWS S3] Whether to allow anonymous access to object storage.\n            enable_request_payer: [AWS S3] Whether to enable request payer for object storage. This is useful for reading files from AWS S3 buckets that require request payer.\n            compression_type: The compression type of the GFF file. If not specified, it will be detected automatically based on the file extension. BGZF and GZIP compression is supported ('bgz' and 'gz').\n            max_retries:  The maximum number of retries for reading the file from object storage.\n            timeout: The timeout in seconds for reading the file from object storage.\n        !!! note\n            GFF reader uses **1-based** coordinate system for the `start` and `end` columns.\n\n        !!! Example\n            ```shell\n            wget https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_38/gencode.v38.annotation.gff3.gz -O /tmp/gencode.v38.annotation.gff3.gz\n            ```\n            ```python\n            import polars_bio as pb\n            pb.register_gff(\"/tmp/gencode.v38.annotation.gff3.gz\", \"gencode_v38_annotation3_bgz\")\n            pb.sql(\"SELECT attributes, count(*) AS cnt FROM gencode_v38_annotation3_bgz GROUP BY attributes\").limit(5).collect()\n            ```\n            ```shell\n\n            shape: (5, 2)\n            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n            \u2502 Parent            \u2506 cnt   \u2502\n            \u2502 ---               \u2506 ---   \u2502\n            \u2502 str               \u2506 i64   \u2502\n            \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n            \u2502 null              \u2506 60649 \u2502\n            \u2502 ENSG00000223972.5 \u2506 2     \u2502\n            \u2502 ENST00000456328.2 \u2506 3     \u2502\n            \u2502 ENST00000450305.2 \u2506 6     \u2502\n            \u2502 ENSG00000227232.5 \u2506 1     \u2502\n            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n            ```\n        !!! tip\n            `chunk_size` and `concurrent_fetches` can be adjusted according to the network bandwidth and the size of the GFF file. As a rule of thumb for large scale operations (reading a whole GFF), it is recommended to the default values.\n        \"\"\"\n\n        object_storage_options = PyObjectStorageOptions(\n            allow_anonymous=allow_anonymous,\n            enable_request_payer=enable_request_payer,\n            chunk_size=chunk_size,\n            concurrent_fetches=concurrent_fetches,\n            max_retries=max_retries,\n            timeout=timeout,\n            compression_type=compression_type,\n        )\n\n        gff_read_options = GffReadOptions(\n            attr_fields=None,\n            object_storage_options=object_storage_options,\n        )\n        read_options = ReadOptions(gff_read_options=gff_read_options)\n        py_register_table(ctx, path, name, InputFormat.Gff, read_options)\n\n    @staticmethod\n    def register_fastq(\n        path: str,\n        name: Union[str, None] = None,\n        chunk_size: int = 64,\n        concurrent_fetches: int = 8,\n        allow_anonymous: bool = True,\n        max_retries: int = 5,\n        timeout: int = 300,\n        enable_request_payer: bool = False,\n        compression_type: str = \"auto\",\n        parallel: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Register a FASTQ file as a Datafusion table.\n\n        Parameters:\n            path: The path to the FASTQ file.\n            name: The name of the table. If *None*, the name of the table will be generated automatically based on the path.\n            chunk_size: The size in MB of a chunk when reading from an object store. Default settings are optimized for large scale operations. For small scale (interactive) operations, it is recommended to decrease this value to **8-16**.\n            concurrent_fetches: [GCS] The number of concurrent fetches when reading from an object store. Default settings are optimized for large scale operations. For small scale (interactive) operations, it is recommended to decrease this value to **1-2**.\n            allow_anonymous: [GCS, AWS S3] Whether to allow anonymous access to object storage.\n            enable_request_payer: [AWS S3] Whether to enable request payer for object storage. This is useful for reading files from AWS S3 buckets that require request payer.\n            compression_type: The compression type of the FASTQ file. If not specified, it will be detected automatically based on the file extension. BGZF and GZIP compression is supported ('bgz' and 'gz').\n            max_retries:  The maximum number of retries for reading the file from object storage.\n            timeout: The timeout in seconds for reading the file from object storage.\n            parallel: Whether to use the parallel reader for BGZF compressed files. Default is False. If a file ends with \".gz\" but is actually BGZF, it will attempt the parallel path and fall back to standard if not BGZF.\n\n        !!! Example\n            ```python\n              import polars_bio as pb\n              pb.register_fastq(\"gs://genomics-public-data/platinum-genomes/fastq/ERR194146.fastq.gz\", \"test_fastq\")\n              pb.sql(\"SELECT name, description FROM test_fastq WHERE name LIKE 'ERR194146%'\").limit(5).collect()\n            ```\n\n            ```shell\n\n              shape: (5, 2)\n            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n            \u2502 name                \u2506 description                     \u2502\n            \u2502 ---                 \u2506 ---                             \u2502\n            \u2502 str                 \u2506 str                             \u2502\n            \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n            \u2502 ERR194146.812444541 \u2506 HSQ1008:141:D0CC8ACXX:2:1204:1\u2026 \u2502\n            \u2502 ERR194146.812444542 \u2506 HSQ1008:141:D0CC8ACXX:4:1206:1\u2026 \u2502\n            \u2502 ERR194146.812444543 \u2506 HSQ1008:141:D0CC8ACXX:3:2104:5\u2026 \u2502\n            \u2502 ERR194146.812444544 \u2506 HSQ1008:141:D0CC8ACXX:3:2204:1\u2026 \u2502\n            \u2502 ERR194146.812444545 \u2506 HSQ1008:141:D0CC8ACXX:3:1304:3\u2026 \u2502\n            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n            ```\n\n\n        !!! tip\n            `chunk_size` and `concurrent_fetches` can be adjusted according to the network bandwidth and the size of the FASTQ file. As a rule of thumb for large scale operations (reading a whole FASTQ), it is recommended to the default values.\n        \"\"\"\n\n        object_storage_options = PyObjectStorageOptions(\n            allow_anonymous=allow_anonymous,\n            enable_request_payer=enable_request_payer,\n            chunk_size=chunk_size,\n            concurrent_fetches=concurrent_fetches,\n            max_retries=max_retries,\n            timeout=timeout,\n            compression_type=compression_type,\n        )\n\n        fastq_read_options = FastqReadOptions(\n            object_storage_options=object_storage_options, parallel=parallel\n        )\n        read_options = ReadOptions(fastq_read_options=fastq_read_options)\n        py_register_table(ctx, path, name, InputFormat.Fastq, read_options)\n\n    @staticmethod\n    def register_bed(\n        path: str,\n        name: Union[str, None] = None,\n        chunk_size: int = 64,\n        concurrent_fetches: int = 8,\n        allow_anonymous: bool = True,\n        max_retries: int = 5,\n        timeout: int = 300,\n        enable_request_payer: bool = False,\n        compression_type: str = \"auto\",\n    ) -&gt; None:\n        \"\"\"\n        Register a BED file as a Datafusion table.\n\n        Parameters:\n            path: The path to the BED file.\n            name: The name of the table. If *None*, the name of the table will be generated automatically based on the path.\n            chunk_size: The size in MB of a chunk when reading from an object store. Default settings are optimized for large scale operations. For small scale (interactive) operations, it is recommended to decrease this value to **8-16**.\n            concurrent_fetches: [GCS] The number of concurrent fetches when reading from an object store. Default settings are optimized for large scale operations. For small scale (interactive) operations, it is recommended to decrease this value to **1-2**.\n            allow_anonymous: [GCS, AWS S3] Whether to allow anonymous access to object storage.\n            enable_request_payer: [AWS S3] Whether to enable request payer for object storage. This is useful for reading files from AWS S3 buckets that require request payer.\n            compression_type: The compression type of the BED file. If not specified, it will be detected automatically..\n            max_retries:  The maximum number of retries for reading the file from object storage.\n            timeout: The timeout in seconds for reading the file from object storage.\n\n        !!! Note\n            Only **BED4** format is supported. It extends the basic BED format (BED3) by adding a name field, resulting in four columns: chromosome, start position, end position, and name.\n            Also unlike other text formats, **GZIP** compression is not supported.\n\n        !!! Example\n            ```shell\n\n             cd /tmp\n             wget https://webs.iiitd.edu.in/raghava/humcfs/fragile_site_bed.zip -O fragile_site_bed.zip\n             unzip fragile_site_bed.zip -x \"__MACOSX/*\" \"*/.DS_Store\"\n            ```\n\n            ```python\n            import polars_bio as pb\n            pb.register_bed(\"/tmp/fragile_site_bed/chr5_fragile_site.bed\", \"test_bed\")\n            b.sql(\"select * FROM test_bed WHERE name LIKE 'FRA5%'\").collect()\n            ```\n\n            ```shell\n\n                shape: (8, 4)\n                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502 chrom \u2506 start     \u2506 end       \u2506 name  \u2502\n                \u2502 ---   \u2506 ---       \u2506 ---       \u2506 ---   \u2502\n                \u2502 str   \u2506 u32       \u2506 u32       \u2506 str   \u2502\n                \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n                \u2502 chr5  \u2506 28900001  \u2506 42500000  \u2506 FRA5A \u2502\n                \u2502 chr5  \u2506 92300001  \u2506 98200000  \u2506 FRA5B \u2502\n                \u2502 chr5  \u2506 130600001 \u2506 136200000 \u2506 FRA5C \u2502\n                \u2502 chr5  \u2506 92300001  \u2506 93916228  \u2506 FRA5D \u2502\n                \u2502 chr5  \u2506 18400001  \u2506 28900000  \u2506 FRA5E \u2502\n                \u2502 chr5  \u2506 98200001  \u2506 109600000 \u2506 FRA5F \u2502\n                \u2502 chr5  \u2506 168500001 \u2506 180915260 \u2506 FRA5G \u2502\n                \u2502 chr5  \u2506 50500001  \u2506 63000000  \u2506 FRA5H \u2502\n                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n            ```\n\n\n        !!! tip\n            `chunk_size` and `concurrent_fetches` can be adjusted according to the network bandwidth and the size of the BED file. As a rule of thumb for large scale operations (reading a whole BED), it is recommended to the default values.\n        \"\"\"\n\n        object_storage_options = PyObjectStorageOptions(\n            allow_anonymous=allow_anonymous,\n            enable_request_payer=enable_request_payer,\n            chunk_size=chunk_size,\n            concurrent_fetches=concurrent_fetches,\n            max_retries=max_retries,\n            timeout=timeout,\n            compression_type=compression_type,\n        )\n\n        bed_read_options = BedReadOptions(\n            object_storage_options=object_storage_options,\n        )\n        read_options = ReadOptions(bed_read_options=bed_read_options)\n        py_register_table(ctx, path, name, InputFormat.Bed, read_options)\n\n    @staticmethod\n    def register_view(name: str, query: str) -&gt; None:\n        \"\"\"\n        Register a query as a Datafusion view. This view can be used in genomic ranges operations,\n        such as overlap, nearest, and count_overlaps. It is useful for filtering, transforming, and aggregating data\n        prior to the range operation. When combined with the range operation, it can be used to perform complex in a streaming fashion end-to-end.\n\n        Parameters:\n            name: The name of the table.\n            query: The SQL query.\n\n        !!! Example\n              ```python\n              import polars_bio as pb\n              pb.register_vcf(\"gs://gcp-public-data--gnomad/release/4.1/vcf/exomes/gnomad.exomes.v4.1.sites.chr21.vcf.bgz\", \"gnomad_sv\")\n              pb.register_view(\"v_gnomad_sv\", \"SELECT replace(chrom,'chr', '') AS chrom, start, end FROM gnomad_sv\")\n              pb.sql(\"SELECT * FROM v_gnomad_sv\").limit(5).collect()\n              ```\n              ```shell\n                shape: (5, 3)\n                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502 chrom \u2506 start   \u2506 end     \u2502\n                \u2502 ---   \u2506 ---     \u2506 ---     \u2502\n                \u2502 str   \u2506 u32     \u2506 u32     \u2502\n                \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n                \u2502 21    \u2506 5031905 \u2506 5031905 \u2502\n                \u2502 21    \u2506 5031905 \u2506 5031905 \u2502\n                \u2502 21    \u2506 5031909 \u2506 5031909 \u2502\n                \u2502 21    \u2506 5031911 \u2506 5031911 \u2502\n                \u2502 21    \u2506 5031911 \u2506 5031911 \u2502\n                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n              ```\n        \"\"\"\n        py_register_view(ctx, name, query)\n\n    @staticmethod\n    def register_bam(\n        path: str,\n        name: Union[str, None] = None,\n        tag_fields: Union[list[str], None] = None,\n        chunk_size: int = 64,\n        concurrent_fetches: int = 8,\n        allow_anonymous: bool = True,\n        max_retries: int = 5,\n        timeout: int = 300,\n        enable_request_payer: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Register a BAM file as a Datafusion table.\n\n        Parameters:\n            path: The path to the BAM file.\n            name: The name of the table. If *None*, the name of the table will be generated automatically based on the path.\n            tag_fields: List of BAM tag names to include as columns (e.g., [\"NM\", \"MD\", \"AS\"]). If None, no optional tags are parsed (default). Common tags include: NM (edit distance), MD (mismatch string), AS (alignment score), XS (secondary alignment score), RG (read group), CB (cell barcode), UB (UMI barcode).\n            chunk_size: The size in MB of a chunk when reading from an object store. Default settings are optimized for large scale operations. For small scale (interactive) operations, it is recommended to decrease this value to **8-16**.\n            concurrent_fetches: [GCS] The number of concurrent fetches when reading from an object store. Default settings are optimized for large scale operations. For small scale (interactive) operations, it is recommended to decrease this value to **1-2**.\n            allow_anonymous: [GCS, AWS S3] Whether to allow anonymous access to object storage.\n            enable_request_payer: [AWS S3] Whether to enable request payer for object storage. This is useful for reading files from AWS S3 buckets that require request payer.\n            max_retries:  The maximum number of retries for reading the file from object storage.\n            timeout: The timeout in seconds for reading the file from object storage.\n        !!! note\n            BAM reader uses **1-based** coordinate system for the `start`, `end`, `mate_start`, `mate_end` columns.\n\n        !!! Example\n\n            ```python\n            import polars_bio as pb\n            pb.register_bam(\"gs://genomics-public-data/1000-genomes/bam/HG00096.mapped.ILLUMINA.bwa.GBR.low_coverage.20120522.bam\", \"HG00096_bam\", concurrent_fetches=1, chunk_size=8)\n            pb.sql(\"SELECT chrom, flags FROM HG00096_bam\").limit(5).collect()\n            ```\n            ```shell\n\n                shape: (5, 2)\n                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502 chrom \u2506 flags \u2502\n                \u2502 ---   \u2506 ---   \u2502\n                \u2502 str   \u2506 u32   \u2502\n                \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n                \u2502 chr1  \u2506 163   \u2502\n                \u2502 chr1  \u2506 163   \u2502\n                \u2502 chr1  \u2506 99    \u2502\n                \u2502 chr1  \u2506 99    \u2502\n                \u2502 chr1  \u2506 99    \u2502\n                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n            ```\n        !!! tip\n            `chunk_size` and `concurrent_fetches` can be adjusted according to the network bandwidth and the size of the BAM file. As a rule of thumb for large scale operations (reading a whole BAM), it is recommended keep the default values.\n            For more interactive inspecting a schema, it is recommended to decrease `chunk_size` to **8-16** and `concurrent_fetches` to **1-2**.\n        \"\"\"\n\n        object_storage_options = PyObjectStorageOptions(\n            allow_anonymous=allow_anonymous,\n            enable_request_payer=enable_request_payer,\n            chunk_size=chunk_size,\n            concurrent_fetches=concurrent_fetches,\n            max_retries=max_retries,\n            timeout=timeout,\n            compression_type=\"auto\",\n        )\n\n        bam_read_options = BamReadOptions(\n            object_storage_options=object_storage_options,\n            tag_fields=tag_fields,\n        )\n        read_options = ReadOptions(bam_read_options=bam_read_options)\n        py_register_table(ctx, path, name, InputFormat.Bam, read_options)\n\n    @staticmethod\n    def register_sam(\n        path: str,\n        name: Union[str, None] = None,\n        tag_fields: Union[list[str], None] = None,\n    ) -&gt; None:\n        \"\"\"\n        Register a SAM file as a Datafusion table.\n\n        SAM (Sequence Alignment/Map) is the plain-text counterpart of BAM.\n        This function reuses the BAM table provider, which auto-detects\n        the format from the file extension.\n\n        Parameters:\n            path: The path to the SAM file.\n            name: The name of the table. If *None*, the name will be generated automatically from the path.\n            tag_fields: List of SAM tag names to include as columns (e.g., [\"NM\", \"MD\", \"AS\"]).\n                If None, no optional tags are parsed (default).\n\n        !!! Example\n            ```python\n            import polars_bio as pb\n            pb.register_sam(\"test.sam\", \"my_sam\")\n            pb.sql(\"SELECT chrom, flags FROM my_sam\").limit(5).collect()\n            ```\n        \"\"\"\n        bam_read_options = BamReadOptions(\n            tag_fields=tag_fields,\n        )\n        read_options = ReadOptions(bam_read_options=bam_read_options)\n        py_register_table(ctx, path, name, InputFormat.Sam, read_options)\n\n    @staticmethod\n    def register_cram(\n        path: str,\n        name: Union[str, None] = None,\n        tag_fields: Union[list[str], None] = None,\n        chunk_size: int = 64,\n        concurrent_fetches: int = 8,\n        allow_anonymous: bool = True,\n        max_retries: int = 5,\n        timeout: int = 300,\n        enable_request_payer: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Register a CRAM file as a Datafusion table.\n\n        !!! warning \"Embedded Reference Required\"\n            Currently, only CRAM files with **embedded reference sequences** are supported.\n            CRAM files requiring external reference FASTA files cannot be registered.\n            Most modern CRAM files include embedded references by default.\n\n            To create a CRAM file with embedded reference using samtools:\n            ```bash\n            samtools view -C -o output.cram --output-fmt-option embed_ref=1 input.bam\n            ```\n\n        Parameters:\n            path: The path to the CRAM file (local or cloud storage: S3, GCS, Azure Blob).\n            name: The name of the table. If *None*, the name of the table will be generated automatically based on the path.\n            tag_fields: List of CRAM tag names to include as columns (e.g., [\"NM\", \"MD\", \"AS\"]). If None, no optional tags are parsed (default). Common tags include: NM (edit distance), MD (mismatch string), AS (alignment score), XS (secondary alignment score), RG (read group), CB (cell barcode), UB (UMI barcode).\n            chunk_size: The size in MB of a chunk when reading from an object store. Default settings are optimized for large scale operations. For small scale (interactive) operations, it is recommended to decrease this value to **8-16**.\n            concurrent_fetches: [GCS] The number of concurrent fetches when reading from an object store. Default settings are optimized for large scale operations. For small scale (interactive) operations, it is recommended to decrease this value to **1-2**.\n            allow_anonymous: [GCS, AWS S3] Whether to allow anonymous access to object storage.\n            enable_request_payer: [AWS S3] Whether to enable request payer for object storage. This is useful for reading files from AWS S3 buckets that require request payer.\n            max_retries:  The maximum number of retries for reading the file from object storage.\n            timeout: The timeout in seconds for reading the file from object storage.\n        !!! note\n            CRAM reader uses **1-based** coordinate system for the `start`, `end`, `mate_start`, `mate_end` columns.\n\n        !!! tip\n            `chunk_size` and `concurrent_fetches` can be adjusted according to the network bandwidth and the size of the CRAM file. As a rule of thumb for large scale operations (reading a whole CRAM), it is recommended to keep the default values.\n            For more interactive inspecting a schema, it is recommended to decrease `chunk_size` to **8-16** and `concurrent_fetches` to **1-2**.\n        \"\"\"\n\n        object_storage_options = PyObjectStorageOptions(\n            allow_anonymous=allow_anonymous,\n            enable_request_payer=enable_request_payer,\n            chunk_size=chunk_size,\n            concurrent_fetches=concurrent_fetches,\n            max_retries=max_retries,\n            timeout=timeout,\n            compression_type=\"auto\",\n        )\n\n        cram_read_options = CramReadOptions(\n            reference_path=None,\n            object_storage_options=object_storage_options,\n            tag_fields=tag_fields,\n        )\n        read_options = ReadOptions(cram_read_options=cram_read_options)\n        py_register_table(ctx, path, name, InputFormat.Cram, read_options)\n\n    @staticmethod\n    def register_pairs(\n        path: str,\n        name: Union[str, None] = None,\n        chunk_size: int = 64,\n        concurrent_fetches: int = 8,\n        allow_anonymous: bool = True,\n        max_retries: int = 5,\n        timeout: int = 300,\n        enable_request_payer: bool = False,\n        compression_type: str = \"auto\",\n    ) -&gt; None:\n        \"\"\"\n        Register a Pairs (Hi-C) file as a Datafusion table.\n\n        The Pairs format (4DN project) stores chromatin contact data with columns:\n        readID, chr1, pos1, chr2, pos2, strand1, strand2.\n\n        Parameters:\n            path: The path to the Pairs file (.pairs, .pairs.gz, .pairs.bgz).\n            name: The name of the table. If *None*, the name will be generated automatically from the path.\n            chunk_size: The size in MB of a chunk when reading from an object store.\n            concurrent_fetches: The number of concurrent fetches when reading from an object store.\n            allow_anonymous: Whether to allow anonymous access to object storage.\n            max_retries: The maximum number of retries for reading the file from object storage.\n            timeout: The timeout in seconds for reading the file from object storage.\n            enable_request_payer: Whether to enable request payer for object storage.\n            compression_type: The compression type. If not specified, it will be detected automatically.\n\n        !!! note\n            Pairs format uses **1-based** coordinate system for pos1 and pos2.\n\n        !!! Example\n            ```python\n            import polars_bio as pb\n            pb.register_pairs(\"contacts.pairs.gz\", \"hic_contacts\")\n            pb.sql(\"SELECT * FROM hic_contacts WHERE chr1 = 'chr1'\").collect()\n            ```\n        \"\"\"\n\n        object_storage_options = PyObjectStorageOptions(\n            allow_anonymous=allow_anonymous,\n            enable_request_payer=enable_request_payer,\n            chunk_size=chunk_size,\n            concurrent_fetches=concurrent_fetches,\n            max_retries=max_retries,\n            timeout=timeout,\n            compression_type=compression_type,\n        )\n\n        pairs_read_options = PairsReadOptions(\n            object_storage_options=object_storage_options,\n        )\n        read_options = ReadOptions(pairs_read_options=pairs_read_options)\n        py_register_table(ctx, path, name, InputFormat.Pairs, read_options)\n\n    @staticmethod\n    def sql(query: str) -&gt; pl.LazyFrame:\n        \"\"\"\n        Execute a SQL query on the registered tables.\n\n        Parameters:\n            query: The SQL query.\n\n        !!! Example\n              ```python\n              import polars_bio as pb\n              pb.register_vcf(\"/tmp/gnomad.v4.1.sv.sites.vcf.gz\", \"gnomad_v4_1_sv\")\n              pb.sql(\"SELECT * FROM gnomad_v4_1_sv LIMIT 5\").collect()\n              ```\n        \"\"\"\n        df = py_read_sql(ctx, query)\n        return _lazy_scan(df)\n</code></pre>"},{"location":"api/#polars_bio.data_processing.register_bam","title":"<code>register_bam(path, name=None, tag_fields=None, chunk_size=64, concurrent_fetches=8, allow_anonymous=True, max_retries=5, timeout=300, enable_request_payer=False)</code>  <code>staticmethod</code>","text":"<p>Register a BAM file as a Datafusion table.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the BAM file.</p> required <code>name</code> <code>Union[str, None]</code> <p>The name of the table. If None, the name of the table will be generated automatically based on the path.</p> <code>None</code> <code>tag_fields</code> <code>Union[list[str], None]</code> <p>List of BAM tag names to include as columns (e.g., [\"NM\", \"MD\", \"AS\"]). If None, no optional tags are parsed (default). Common tags include: NM (edit distance), MD (mismatch string), AS (alignment score), XS (secondary alignment score), RG (read group), CB (cell barcode), UB (UMI barcode).</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>The size in MB of a chunk when reading from an object store. Default settings are optimized for large scale operations. For small scale (interactive) operations, it is recommended to decrease this value to 8-16.</p> <code>64</code> <code>concurrent_fetches</code> <code>int</code> <p>[GCS] The number of concurrent fetches when reading from an object store. Default settings are optimized for large scale operations. For small scale (interactive) operations, it is recommended to decrease this value to 1-2.</p> <code>8</code> <code>allow_anonymous</code> <code>bool</code> <p>[GCS, AWS S3] Whether to allow anonymous access to object storage.</p> <code>True</code> <code>enable_request_payer</code> <code>bool</code> <p>[AWS S3] Whether to enable request payer for object storage. This is useful for reading files from AWS S3 buckets that require request payer.</p> <code>False</code> <code>max_retries</code> <code>int</code> <p>The maximum number of retries for reading the file from object storage.</p> <code>5</code> <code>timeout</code> <code>int</code> <p>The timeout in seconds for reading the file from object storage.</p> <code>300</code> <p>Note</p> <p>BAM reader uses 1-based coordinate system for the <code>start</code>, <code>end</code>, <code>mate_start</code>, <code>mate_end</code> columns.</p> <p>Example</p> <p></p><pre><code>import polars_bio as pb\npb.register_bam(\"gs://genomics-public-data/1000-genomes/bam/HG00096.mapped.ILLUMINA.bwa.GBR.low_coverage.20120522.bam\", \"HG00096_bam\", concurrent_fetches=1, chunk_size=8)\npb.sql(\"SELECT chrom, flags FROM HG00096_bam\").limit(5).collect()\n</code></pre> <pre><code>    shape: (5, 2)\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 chrom \u2506 flags \u2502\n    \u2502 ---   \u2506 ---   \u2502\n    \u2502 str   \u2506 u32   \u2502\n    \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n    \u2502 chr1  \u2506 163   \u2502\n    \u2502 chr1  \u2506 163   \u2502\n    \u2502 chr1  \u2506 99    \u2502\n    \u2502 chr1  \u2506 99    \u2502\n    \u2502 chr1  \u2506 99    \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre><p></p> <p>Tip</p> <p><code>chunk_size</code> and <code>concurrent_fetches</code> can be adjusted according to the network bandwidth and the size of the BAM file. As a rule of thumb for large scale operations (reading a whole BAM), it is recommended keep the default values. For more interactive inspecting a schema, it is recommended to decrease <code>chunk_size</code> to 8-16 and <code>concurrent_fetches</code> to 1-2.</p> Source code in <code>polars_bio/sql.py</code> <pre><code>@staticmethod\ndef register_bam(\n    path: str,\n    name: Union[str, None] = None,\n    tag_fields: Union[list[str], None] = None,\n    chunk_size: int = 64,\n    concurrent_fetches: int = 8,\n    allow_anonymous: bool = True,\n    max_retries: int = 5,\n    timeout: int = 300,\n    enable_request_payer: bool = False,\n) -&gt; None:\n    \"\"\"\n    Register a BAM file as a Datafusion table.\n\n    Parameters:\n        path: The path to the BAM file.\n        name: The name of the table. If *None*, the name of the table will be generated automatically based on the path.\n        tag_fields: List of BAM tag names to include as columns (e.g., [\"NM\", \"MD\", \"AS\"]). If None, no optional tags are parsed (default). Common tags include: NM (edit distance), MD (mismatch string), AS (alignment score), XS (secondary alignment score), RG (read group), CB (cell barcode), UB (UMI barcode).\n        chunk_size: The size in MB of a chunk when reading from an object store. Default settings are optimized for large scale operations. For small scale (interactive) operations, it is recommended to decrease this value to **8-16**.\n        concurrent_fetches: [GCS] The number of concurrent fetches when reading from an object store. Default settings are optimized for large scale operations. For small scale (interactive) operations, it is recommended to decrease this value to **1-2**.\n        allow_anonymous: [GCS, AWS S3] Whether to allow anonymous access to object storage.\n        enable_request_payer: [AWS S3] Whether to enable request payer for object storage. This is useful for reading files from AWS S3 buckets that require request payer.\n        max_retries:  The maximum number of retries for reading the file from object storage.\n        timeout: The timeout in seconds for reading the file from object storage.\n    !!! note\n        BAM reader uses **1-based** coordinate system for the `start`, `end`, `mate_start`, `mate_end` columns.\n\n    !!! Example\n\n        ```python\n        import polars_bio as pb\n        pb.register_bam(\"gs://genomics-public-data/1000-genomes/bam/HG00096.mapped.ILLUMINA.bwa.GBR.low_coverage.20120522.bam\", \"HG00096_bam\", concurrent_fetches=1, chunk_size=8)\n        pb.sql(\"SELECT chrom, flags FROM HG00096_bam\").limit(5).collect()\n        ```\n        ```shell\n\n            shape: (5, 2)\n            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n            \u2502 chrom \u2506 flags \u2502\n            \u2502 ---   \u2506 ---   \u2502\n            \u2502 str   \u2506 u32   \u2502\n            \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n            \u2502 chr1  \u2506 163   \u2502\n            \u2502 chr1  \u2506 163   \u2502\n            \u2502 chr1  \u2506 99    \u2502\n            \u2502 chr1  \u2506 99    \u2502\n            \u2502 chr1  \u2506 99    \u2502\n            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        ```\n    !!! tip\n        `chunk_size` and `concurrent_fetches` can be adjusted according to the network bandwidth and the size of the BAM file. As a rule of thumb for large scale operations (reading a whole BAM), it is recommended keep the default values.\n        For more interactive inspecting a schema, it is recommended to decrease `chunk_size` to **8-16** and `concurrent_fetches` to **1-2**.\n    \"\"\"\n\n    object_storage_options = PyObjectStorageOptions(\n        allow_anonymous=allow_anonymous,\n        enable_request_payer=enable_request_payer,\n        chunk_size=chunk_size,\n        concurrent_fetches=concurrent_fetches,\n        max_retries=max_retries,\n        timeout=timeout,\n        compression_type=\"auto\",\n    )\n\n    bam_read_options = BamReadOptions(\n        object_storage_options=object_storage_options,\n        tag_fields=tag_fields,\n    )\n    read_options = ReadOptions(bam_read_options=bam_read_options)\n    py_register_table(ctx, path, name, InputFormat.Bam, read_options)\n</code></pre>"},{"location":"api/#polars_bio.data_processing.register_bed","title":"<code>register_bed(path, name=None, chunk_size=64, concurrent_fetches=8, allow_anonymous=True, max_retries=5, timeout=300, enable_request_payer=False, compression_type='auto')</code>  <code>staticmethod</code>","text":"<p>Register a BED file as a Datafusion table.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the BED file.</p> required <code>name</code> <code>Union[str, None]</code> <p>The name of the table. If None, the name of the table will be generated automatically based on the path.</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>The size in MB of a chunk when reading from an object store. Default settings are optimized for large scale operations. For small scale (interactive) operations, it is recommended to decrease this value to 8-16.</p> <code>64</code> <code>concurrent_fetches</code> <code>int</code> <p>[GCS] The number of concurrent fetches when reading from an object store. Default settings are optimized for large scale operations. For small scale (interactive) operations, it is recommended to decrease this value to 1-2.</p> <code>8</code> <code>allow_anonymous</code> <code>bool</code> <p>[GCS, AWS S3] Whether to allow anonymous access to object storage.</p> <code>True</code> <code>enable_request_payer</code> <code>bool</code> <p>[AWS S3] Whether to enable request payer for object storage. This is useful for reading files from AWS S3 buckets that require request payer.</p> <code>False</code> <code>compression_type</code> <code>str</code> <p>The compression type of the BED file. If not specified, it will be detected automatically..</p> <code>'auto'</code> <code>max_retries</code> <code>int</code> <p>The maximum number of retries for reading the file from object storage.</p> <code>5</code> <code>timeout</code> <code>int</code> <p>The timeout in seconds for reading the file from object storage.</p> <code>300</code> <p>Note</p> <p>Only BED4 format is supported. It extends the basic BED format (BED3) by adding a name field, resulting in four columns: chromosome, start position, end position, and name. Also unlike other text formats, GZIP compression is not supported.</p> <p>Example</p> <pre><code> cd /tmp\n wget https://webs.iiitd.edu.in/raghava/humcfs/fragile_site_bed.zip -O fragile_site_bed.zip\n unzip fragile_site_bed.zip -x \"__MACOSX/*\" \"*/.DS_Store\"\n</code></pre> <pre><code>import polars_bio as pb\npb.register_bed(\"/tmp/fragile_site_bed/chr5_fragile_site.bed\", \"test_bed\")\nb.sql(\"select * FROM test_bed WHERE name LIKE 'FRA5%'\").collect()\n</code></pre> <pre><code>    shape: (8, 4)\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 chrom \u2506 start     \u2506 end       \u2506 name  \u2502\n    \u2502 ---   \u2506 ---       \u2506 ---       \u2506 ---   \u2502\n    \u2502 str   \u2506 u32       \u2506 u32       \u2506 str   \u2502\n    \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n    \u2502 chr5  \u2506 28900001  \u2506 42500000  \u2506 FRA5A \u2502\n    \u2502 chr5  \u2506 92300001  \u2506 98200000  \u2506 FRA5B \u2502\n    \u2502 chr5  \u2506 130600001 \u2506 136200000 \u2506 FRA5C \u2502\n    \u2502 chr5  \u2506 92300001  \u2506 93916228  \u2506 FRA5D \u2502\n    \u2502 chr5  \u2506 18400001  \u2506 28900000  \u2506 FRA5E \u2502\n    \u2502 chr5  \u2506 98200001  \u2506 109600000 \u2506 FRA5F \u2502\n    \u2502 chr5  \u2506 168500001 \u2506 180915260 \u2506 FRA5G \u2502\n    \u2502 chr5  \u2506 50500001  \u2506 63000000  \u2506 FRA5H \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Tip</p> <p><code>chunk_size</code> and <code>concurrent_fetches</code> can be adjusted according to the network bandwidth and the size of the BED file. As a rule of thumb for large scale operations (reading a whole BED), it is recommended to the default values.</p> Source code in <code>polars_bio/sql.py</code> <pre><code>@staticmethod\ndef register_bed(\n    path: str,\n    name: Union[str, None] = None,\n    chunk_size: int = 64,\n    concurrent_fetches: int = 8,\n    allow_anonymous: bool = True,\n    max_retries: int = 5,\n    timeout: int = 300,\n    enable_request_payer: bool = False,\n    compression_type: str = \"auto\",\n) -&gt; None:\n    \"\"\"\n    Register a BED file as a Datafusion table.\n\n    Parameters:\n        path: The path to the BED file.\n        name: The name of the table. If *None*, the name of the table will be generated automatically based on the path.\n        chunk_size: The size in MB of a chunk when reading from an object store. Default settings are optimized for large scale operations. For small scale (interactive) operations, it is recommended to decrease this value to **8-16**.\n        concurrent_fetches: [GCS] The number of concurrent fetches when reading from an object store. Default settings are optimized for large scale operations. For small scale (interactive) operations, it is recommended to decrease this value to **1-2**.\n        allow_anonymous: [GCS, AWS S3] Whether to allow anonymous access to object storage.\n        enable_request_payer: [AWS S3] Whether to enable request payer for object storage. This is useful for reading files from AWS S3 buckets that require request payer.\n        compression_type: The compression type of the BED file. If not specified, it will be detected automatically..\n        max_retries:  The maximum number of retries for reading the file from object storage.\n        timeout: The timeout in seconds for reading the file from object storage.\n\n    !!! Note\n        Only **BED4** format is supported. It extends the basic BED format (BED3) by adding a name field, resulting in four columns: chromosome, start position, end position, and name.\n        Also unlike other text formats, **GZIP** compression is not supported.\n\n    !!! Example\n        ```shell\n\n         cd /tmp\n         wget https://webs.iiitd.edu.in/raghava/humcfs/fragile_site_bed.zip -O fragile_site_bed.zip\n         unzip fragile_site_bed.zip -x \"__MACOSX/*\" \"*/.DS_Store\"\n        ```\n\n        ```python\n        import polars_bio as pb\n        pb.register_bed(\"/tmp/fragile_site_bed/chr5_fragile_site.bed\", \"test_bed\")\n        b.sql(\"select * FROM test_bed WHERE name LIKE 'FRA5%'\").collect()\n        ```\n\n        ```shell\n\n            shape: (8, 4)\n            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n            \u2502 chrom \u2506 start     \u2506 end       \u2506 name  \u2502\n            \u2502 ---   \u2506 ---       \u2506 ---       \u2506 ---   \u2502\n            \u2502 str   \u2506 u32       \u2506 u32       \u2506 str   \u2502\n            \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n            \u2502 chr5  \u2506 28900001  \u2506 42500000  \u2506 FRA5A \u2502\n            \u2502 chr5  \u2506 92300001  \u2506 98200000  \u2506 FRA5B \u2502\n            \u2502 chr5  \u2506 130600001 \u2506 136200000 \u2506 FRA5C \u2502\n            \u2502 chr5  \u2506 92300001  \u2506 93916228  \u2506 FRA5D \u2502\n            \u2502 chr5  \u2506 18400001  \u2506 28900000  \u2506 FRA5E \u2502\n            \u2502 chr5  \u2506 98200001  \u2506 109600000 \u2506 FRA5F \u2502\n            \u2502 chr5  \u2506 168500001 \u2506 180915260 \u2506 FRA5G \u2502\n            \u2502 chr5  \u2506 50500001  \u2506 63000000  \u2506 FRA5H \u2502\n            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        ```\n\n\n    !!! tip\n        `chunk_size` and `concurrent_fetches` can be adjusted according to the network bandwidth and the size of the BED file. As a rule of thumb for large scale operations (reading a whole BED), it is recommended to the default values.\n    \"\"\"\n\n    object_storage_options = PyObjectStorageOptions(\n        allow_anonymous=allow_anonymous,\n        enable_request_payer=enable_request_payer,\n        chunk_size=chunk_size,\n        concurrent_fetches=concurrent_fetches,\n        max_retries=max_retries,\n        timeout=timeout,\n        compression_type=compression_type,\n    )\n\n    bed_read_options = BedReadOptions(\n        object_storage_options=object_storage_options,\n    )\n    read_options = ReadOptions(bed_read_options=bed_read_options)\n    py_register_table(ctx, path, name, InputFormat.Bed, read_options)\n</code></pre>"},{"location":"api/#polars_bio.data_processing.register_cram","title":"<code>register_cram(path, name=None, tag_fields=None, chunk_size=64, concurrent_fetches=8, allow_anonymous=True, max_retries=5, timeout=300, enable_request_payer=False)</code>  <code>staticmethod</code>","text":"<p>Register a CRAM file as a Datafusion table.</p> <p>Embedded Reference Required</p> <p>Currently, only CRAM files with embedded reference sequences are supported. CRAM files requiring external reference FASTA files cannot be registered. Most modern CRAM files include embedded references by default.</p> <p>To create a CRAM file with embedded reference using samtools: </p><pre><code>samtools view -C -o output.cram --output-fmt-option embed_ref=1 input.bam\n</code></pre><p></p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the CRAM file (local or cloud storage: S3, GCS, Azure Blob).</p> required <code>name</code> <code>Union[str, None]</code> <p>The name of the table. If None, the name of the table will be generated automatically based on the path.</p> <code>None</code> <code>tag_fields</code> <code>Union[list[str], None]</code> <p>List of CRAM tag names to include as columns (e.g., [\"NM\", \"MD\", \"AS\"]). If None, no optional tags are parsed (default). Common tags include: NM (edit distance), MD (mismatch string), AS (alignment score), XS (secondary alignment score), RG (read group), CB (cell barcode), UB (UMI barcode).</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>The size in MB of a chunk when reading from an object store. Default settings are optimized for large scale operations. For small scale (interactive) operations, it is recommended to decrease this value to 8-16.</p> <code>64</code> <code>concurrent_fetches</code> <code>int</code> <p>[GCS] The number of concurrent fetches when reading from an object store. Default settings are optimized for large scale operations. For small scale (interactive) operations, it is recommended to decrease this value to 1-2.</p> <code>8</code> <code>allow_anonymous</code> <code>bool</code> <p>[GCS, AWS S3] Whether to allow anonymous access to object storage.</p> <code>True</code> <code>enable_request_payer</code> <code>bool</code> <p>[AWS S3] Whether to enable request payer for object storage. This is useful for reading files from AWS S3 buckets that require request payer.</p> <code>False</code> <code>max_retries</code> <code>int</code> <p>The maximum number of retries for reading the file from object storage.</p> <code>5</code> <code>timeout</code> <code>int</code> <p>The timeout in seconds for reading the file from object storage.</p> <code>300</code> <p>Note</p> <p>CRAM reader uses 1-based coordinate system for the <code>start</code>, <code>end</code>, <code>mate_start</code>, <code>mate_end</code> columns.</p> <p>Tip</p> <p><code>chunk_size</code> and <code>concurrent_fetches</code> can be adjusted according to the network bandwidth and the size of the CRAM file. As a rule of thumb for large scale operations (reading a whole CRAM), it is recommended to keep the default values. For more interactive inspecting a schema, it is recommended to decrease <code>chunk_size</code> to 8-16 and <code>concurrent_fetches</code> to 1-2.</p> Source code in <code>polars_bio/sql.py</code> <pre><code>@staticmethod\ndef register_cram(\n    path: str,\n    name: Union[str, None] = None,\n    tag_fields: Union[list[str], None] = None,\n    chunk_size: int = 64,\n    concurrent_fetches: int = 8,\n    allow_anonymous: bool = True,\n    max_retries: int = 5,\n    timeout: int = 300,\n    enable_request_payer: bool = False,\n) -&gt; None:\n    \"\"\"\n    Register a CRAM file as a Datafusion table.\n\n    !!! warning \"Embedded Reference Required\"\n        Currently, only CRAM files with **embedded reference sequences** are supported.\n        CRAM files requiring external reference FASTA files cannot be registered.\n        Most modern CRAM files include embedded references by default.\n\n        To create a CRAM file with embedded reference using samtools:\n        ```bash\n        samtools view -C -o output.cram --output-fmt-option embed_ref=1 input.bam\n        ```\n\n    Parameters:\n        path: The path to the CRAM file (local or cloud storage: S3, GCS, Azure Blob).\n        name: The name of the table. If *None*, the name of the table will be generated automatically based on the path.\n        tag_fields: List of CRAM tag names to include as columns (e.g., [\"NM\", \"MD\", \"AS\"]). If None, no optional tags are parsed (default). Common tags include: NM (edit distance), MD (mismatch string), AS (alignment score), XS (secondary alignment score), RG (read group), CB (cell barcode), UB (UMI barcode).\n        chunk_size: The size in MB of a chunk when reading from an object store. Default settings are optimized for large scale operations. For small scale (interactive) operations, it is recommended to decrease this value to **8-16**.\n        concurrent_fetches: [GCS] The number of concurrent fetches when reading from an object store. Default settings are optimized for large scale operations. For small scale (interactive) operations, it is recommended to decrease this value to **1-2**.\n        allow_anonymous: [GCS, AWS S3] Whether to allow anonymous access to object storage.\n        enable_request_payer: [AWS S3] Whether to enable request payer for object storage. This is useful for reading files from AWS S3 buckets that require request payer.\n        max_retries:  The maximum number of retries for reading the file from object storage.\n        timeout: The timeout in seconds for reading the file from object storage.\n    !!! note\n        CRAM reader uses **1-based** coordinate system for the `start`, `end`, `mate_start`, `mate_end` columns.\n\n    !!! tip\n        `chunk_size` and `concurrent_fetches` can be adjusted according to the network bandwidth and the size of the CRAM file. As a rule of thumb for large scale operations (reading a whole CRAM), it is recommended to keep the default values.\n        For more interactive inspecting a schema, it is recommended to decrease `chunk_size` to **8-16** and `concurrent_fetches` to **1-2**.\n    \"\"\"\n\n    object_storage_options = PyObjectStorageOptions(\n        allow_anonymous=allow_anonymous,\n        enable_request_payer=enable_request_payer,\n        chunk_size=chunk_size,\n        concurrent_fetches=concurrent_fetches,\n        max_retries=max_retries,\n        timeout=timeout,\n        compression_type=\"auto\",\n    )\n\n    cram_read_options = CramReadOptions(\n        reference_path=None,\n        object_storage_options=object_storage_options,\n        tag_fields=tag_fields,\n    )\n    read_options = ReadOptions(cram_read_options=cram_read_options)\n    py_register_table(ctx, path, name, InputFormat.Cram, read_options)\n</code></pre>"},{"location":"api/#polars_bio.data_processing.register_fastq","title":"<code>register_fastq(path, name=None, chunk_size=64, concurrent_fetches=8, allow_anonymous=True, max_retries=5, timeout=300, enable_request_payer=False, compression_type='auto', parallel=False)</code>  <code>staticmethod</code>","text":"<p>Register a FASTQ file as a Datafusion table.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the FASTQ file.</p> required <code>name</code> <code>Union[str, None]</code> <p>The name of the table. If None, the name of the table will be generated automatically based on the path.</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>The size in MB of a chunk when reading from an object store. Default settings are optimized for large scale operations. For small scale (interactive) operations, it is recommended to decrease this value to 8-16.</p> <code>64</code> <code>concurrent_fetches</code> <code>int</code> <p>[GCS] The number of concurrent fetches when reading from an object store. Default settings are optimized for large scale operations. For small scale (interactive) operations, it is recommended to decrease this value to 1-2.</p> <code>8</code> <code>allow_anonymous</code> <code>bool</code> <p>[GCS, AWS S3] Whether to allow anonymous access to object storage.</p> <code>True</code> <code>enable_request_payer</code> <code>bool</code> <p>[AWS S3] Whether to enable request payer for object storage. This is useful for reading files from AWS S3 buckets that require request payer.</p> <code>False</code> <code>compression_type</code> <code>str</code> <p>The compression type of the FASTQ file. If not specified, it will be detected automatically based on the file extension. BGZF and GZIP compression is supported ('bgz' and 'gz').</p> <code>'auto'</code> <code>max_retries</code> <code>int</code> <p>The maximum number of retries for reading the file from object storage.</p> <code>5</code> <code>timeout</code> <code>int</code> <p>The timeout in seconds for reading the file from object storage.</p> <code>300</code> <code>parallel</code> <code>bool</code> <p>Whether to use the parallel reader for BGZF compressed files. Default is False. If a file ends with \".gz\" but is actually BGZF, it will attempt the parallel path and fall back to standard if not BGZF.</p> <code>False</code> <p>Example</p> <pre><code>  import polars_bio as pb\n  pb.register_fastq(\"gs://genomics-public-data/platinum-genomes/fastq/ERR194146.fastq.gz\", \"test_fastq\")\n  pb.sql(\"SELECT name, description FROM test_fastq WHERE name LIKE 'ERR194146%'\").limit(5).collect()\n</code></pre> <pre><code>  shape: (5, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 name                \u2506 description                     \u2502\n\u2502 ---                 \u2506 ---                             \u2502\n\u2502 str                 \u2506 str                             \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 ERR194146.812444541 \u2506 HSQ1008:141:D0CC8ACXX:2:1204:1\u2026 \u2502\n\u2502 ERR194146.812444542 \u2506 HSQ1008:141:D0CC8ACXX:4:1206:1\u2026 \u2502\n\u2502 ERR194146.812444543 \u2506 HSQ1008:141:D0CC8ACXX:3:2104:5\u2026 \u2502\n\u2502 ERR194146.812444544 \u2506 HSQ1008:141:D0CC8ACXX:3:2204:1\u2026 \u2502\n\u2502 ERR194146.812444545 \u2506 HSQ1008:141:D0CC8ACXX:3:1304:3\u2026 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Tip</p> <p><code>chunk_size</code> and <code>concurrent_fetches</code> can be adjusted according to the network bandwidth and the size of the FASTQ file. As a rule of thumb for large scale operations (reading a whole FASTQ), it is recommended to the default values.</p> Source code in <code>polars_bio/sql.py</code> <pre><code>@staticmethod\ndef register_fastq(\n    path: str,\n    name: Union[str, None] = None,\n    chunk_size: int = 64,\n    concurrent_fetches: int = 8,\n    allow_anonymous: bool = True,\n    max_retries: int = 5,\n    timeout: int = 300,\n    enable_request_payer: bool = False,\n    compression_type: str = \"auto\",\n    parallel: bool = False,\n) -&gt; None:\n    \"\"\"\n    Register a FASTQ file as a Datafusion table.\n\n    Parameters:\n        path: The path to the FASTQ file.\n        name: The name of the table. If *None*, the name of the table will be generated automatically based on the path.\n        chunk_size: The size in MB of a chunk when reading from an object store. Default settings are optimized for large scale operations. For small scale (interactive) operations, it is recommended to decrease this value to **8-16**.\n        concurrent_fetches: [GCS] The number of concurrent fetches when reading from an object store. Default settings are optimized for large scale operations. For small scale (interactive) operations, it is recommended to decrease this value to **1-2**.\n        allow_anonymous: [GCS, AWS S3] Whether to allow anonymous access to object storage.\n        enable_request_payer: [AWS S3] Whether to enable request payer for object storage. This is useful for reading files from AWS S3 buckets that require request payer.\n        compression_type: The compression type of the FASTQ file. If not specified, it will be detected automatically based on the file extension. BGZF and GZIP compression is supported ('bgz' and 'gz').\n        max_retries:  The maximum number of retries for reading the file from object storage.\n        timeout: The timeout in seconds for reading the file from object storage.\n        parallel: Whether to use the parallel reader for BGZF compressed files. Default is False. If a file ends with \".gz\" but is actually BGZF, it will attempt the parallel path and fall back to standard if not BGZF.\n\n    !!! Example\n        ```python\n          import polars_bio as pb\n          pb.register_fastq(\"gs://genomics-public-data/platinum-genomes/fastq/ERR194146.fastq.gz\", \"test_fastq\")\n          pb.sql(\"SELECT name, description FROM test_fastq WHERE name LIKE 'ERR194146%'\").limit(5).collect()\n        ```\n\n        ```shell\n\n          shape: (5, 2)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 name                \u2506 description                     \u2502\n        \u2502 ---                 \u2506 ---                             \u2502\n        \u2502 str                 \u2506 str                             \u2502\n        \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n        \u2502 ERR194146.812444541 \u2506 HSQ1008:141:D0CC8ACXX:2:1204:1\u2026 \u2502\n        \u2502 ERR194146.812444542 \u2506 HSQ1008:141:D0CC8ACXX:4:1206:1\u2026 \u2502\n        \u2502 ERR194146.812444543 \u2506 HSQ1008:141:D0CC8ACXX:3:2104:5\u2026 \u2502\n        \u2502 ERR194146.812444544 \u2506 HSQ1008:141:D0CC8ACXX:3:2204:1\u2026 \u2502\n        \u2502 ERR194146.812444545 \u2506 HSQ1008:141:D0CC8ACXX:3:1304:3\u2026 \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n        ```\n\n\n    !!! tip\n        `chunk_size` and `concurrent_fetches` can be adjusted according to the network bandwidth and the size of the FASTQ file. As a rule of thumb for large scale operations (reading a whole FASTQ), it is recommended to the default values.\n    \"\"\"\n\n    object_storage_options = PyObjectStorageOptions(\n        allow_anonymous=allow_anonymous,\n        enable_request_payer=enable_request_payer,\n        chunk_size=chunk_size,\n        concurrent_fetches=concurrent_fetches,\n        max_retries=max_retries,\n        timeout=timeout,\n        compression_type=compression_type,\n    )\n\n    fastq_read_options = FastqReadOptions(\n        object_storage_options=object_storage_options, parallel=parallel\n    )\n    read_options = ReadOptions(fastq_read_options=fastq_read_options)\n    py_register_table(ctx, path, name, InputFormat.Fastq, read_options)\n</code></pre>"},{"location":"api/#polars_bio.data_processing.register_gff","title":"<code>register_gff(path, name=None, chunk_size=64, concurrent_fetches=8, allow_anonymous=True, max_retries=5, timeout=300, enable_request_payer=False, compression_type='auto')</code>  <code>staticmethod</code>","text":"<p>Register a GFF file as a Datafusion table.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the GFF file.</p> required <code>name</code> <code>Union[str, None]</code> <p>The name of the table. If None, the name of the table will be generated automatically based on the path.</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>The size in MB of a chunk when reading from an object store. Default settings are optimized for large scale operations. For small scale (interactive) operations, it is recommended to decrease this value to 8-16.</p> <code>64</code> <code>concurrent_fetches</code> <code>int</code> <p>[GCS] The number of concurrent fetches when reading from an object store. Default settings are optimized for large scale operations. For small scale (interactive) operations, it is recommended to decrease this value to 1-2.</p> <code>8</code> <code>allow_anonymous</code> <code>bool</code> <p>[GCS, AWS S3] Whether to allow anonymous access to object storage.</p> <code>True</code> <code>enable_request_payer</code> <code>bool</code> <p>[AWS S3] Whether to enable request payer for object storage. This is useful for reading files from AWS S3 buckets that require request payer.</p> <code>False</code> <code>compression_type</code> <code>str</code> <p>The compression type of the GFF file. If not specified, it will be detected automatically based on the file extension. BGZF and GZIP compression is supported ('bgz' and 'gz').</p> <code>'auto'</code> <code>max_retries</code> <code>int</code> <p>The maximum number of retries for reading the file from object storage.</p> <code>5</code> <code>timeout</code> <code>int</code> <p>The timeout in seconds for reading the file from object storage.</p> <code>300</code> <p>Note</p> <p>GFF reader uses 1-based coordinate system for the <code>start</code> and <code>end</code> columns.</p> <p>Example</p> <p></p><pre><code>wget https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_38/gencode.v38.annotation.gff3.gz -O /tmp/gencode.v38.annotation.gff3.gz\n</code></pre> <pre><code>import polars_bio as pb\npb.register_gff(\"/tmp/gencode.v38.annotation.gff3.gz\", \"gencode_v38_annotation3_bgz\")\npb.sql(\"SELECT attributes, count(*) AS cnt FROM gencode_v38_annotation3_bgz GROUP BY attributes\").limit(5).collect()\n</code></pre> <pre><code>shape: (5, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Parent            \u2506 cnt   \u2502\n\u2502 ---               \u2506 ---   \u2502\n\u2502 str               \u2506 i64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 null              \u2506 60649 \u2502\n\u2502 ENSG00000223972.5 \u2506 2     \u2502\n\u2502 ENST00000456328.2 \u2506 3     \u2502\n\u2502 ENST00000450305.2 \u2506 6     \u2502\n\u2502 ENSG00000227232.5 \u2506 1     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre><p></p> <p>Tip</p> <p><code>chunk_size</code> and <code>concurrent_fetches</code> can be adjusted according to the network bandwidth and the size of the GFF file. As a rule of thumb for large scale operations (reading a whole GFF), it is recommended to the default values.</p> Source code in <code>polars_bio/sql.py</code> <pre><code>@staticmethod\ndef register_gff(\n    path: str,\n    name: Union[str, None] = None,\n    chunk_size: int = 64,\n    concurrent_fetches: int = 8,\n    allow_anonymous: bool = True,\n    max_retries: int = 5,\n    timeout: int = 300,\n    enable_request_payer: bool = False,\n    compression_type: str = \"auto\",\n) -&gt; None:\n    \"\"\"\n    Register a GFF file as a Datafusion table.\n\n    Parameters:\n        path: The path to the GFF file.\n        name: The name of the table. If *None*, the name of the table will be generated automatically based on the path.\n        chunk_size: The size in MB of a chunk when reading from an object store. Default settings are optimized for large scale operations. For small scale (interactive) operations, it is recommended to decrease this value to **8-16**.\n        concurrent_fetches: [GCS] The number of concurrent fetches when reading from an object store. Default settings are optimized for large scale operations. For small scale (interactive) operations, it is recommended to decrease this value to **1-2**.\n        allow_anonymous: [GCS, AWS S3] Whether to allow anonymous access to object storage.\n        enable_request_payer: [AWS S3] Whether to enable request payer for object storage. This is useful for reading files from AWS S3 buckets that require request payer.\n        compression_type: The compression type of the GFF file. If not specified, it will be detected automatically based on the file extension. BGZF and GZIP compression is supported ('bgz' and 'gz').\n        max_retries:  The maximum number of retries for reading the file from object storage.\n        timeout: The timeout in seconds for reading the file from object storage.\n    !!! note\n        GFF reader uses **1-based** coordinate system for the `start` and `end` columns.\n\n    !!! Example\n        ```shell\n        wget https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_38/gencode.v38.annotation.gff3.gz -O /tmp/gencode.v38.annotation.gff3.gz\n        ```\n        ```python\n        import polars_bio as pb\n        pb.register_gff(\"/tmp/gencode.v38.annotation.gff3.gz\", \"gencode_v38_annotation3_bgz\")\n        pb.sql(\"SELECT attributes, count(*) AS cnt FROM gencode_v38_annotation3_bgz GROUP BY attributes\").limit(5).collect()\n        ```\n        ```shell\n\n        shape: (5, 2)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 Parent            \u2506 cnt   \u2502\n        \u2502 ---               \u2506 ---   \u2502\n        \u2502 str               \u2506 i64   \u2502\n        \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n        \u2502 null              \u2506 60649 \u2502\n        \u2502 ENSG00000223972.5 \u2506 2     \u2502\n        \u2502 ENST00000456328.2 \u2506 3     \u2502\n        \u2502 ENST00000450305.2 \u2506 6     \u2502\n        \u2502 ENSG00000227232.5 \u2506 1     \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n        ```\n    !!! tip\n        `chunk_size` and `concurrent_fetches` can be adjusted according to the network bandwidth and the size of the GFF file. As a rule of thumb for large scale operations (reading a whole GFF), it is recommended to the default values.\n    \"\"\"\n\n    object_storage_options = PyObjectStorageOptions(\n        allow_anonymous=allow_anonymous,\n        enable_request_payer=enable_request_payer,\n        chunk_size=chunk_size,\n        concurrent_fetches=concurrent_fetches,\n        max_retries=max_retries,\n        timeout=timeout,\n        compression_type=compression_type,\n    )\n\n    gff_read_options = GffReadOptions(\n        attr_fields=None,\n        object_storage_options=object_storage_options,\n    )\n    read_options = ReadOptions(gff_read_options=gff_read_options)\n    py_register_table(ctx, path, name, InputFormat.Gff, read_options)\n</code></pre>"},{"location":"api/#polars_bio.data_processing.register_pairs","title":"<code>register_pairs(path, name=None, chunk_size=64, concurrent_fetches=8, allow_anonymous=True, max_retries=5, timeout=300, enable_request_payer=False, compression_type='auto')</code>  <code>staticmethod</code>","text":"<p>Register a Pairs (Hi-C) file as a Datafusion table.</p> <p>The Pairs format (4DN project) stores chromatin contact data with columns: readID, chr1, pos1, chr2, pos2, strand1, strand2.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the Pairs file (.pairs, .pairs.gz, .pairs.bgz).</p> required <code>name</code> <code>Union[str, None]</code> <p>The name of the table. If None, the name will be generated automatically from the path.</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>The size in MB of a chunk when reading from an object store.</p> <code>64</code> <code>concurrent_fetches</code> <code>int</code> <p>The number of concurrent fetches when reading from an object store.</p> <code>8</code> <code>allow_anonymous</code> <code>bool</code> <p>Whether to allow anonymous access to object storage.</p> <code>True</code> <code>max_retries</code> <code>int</code> <p>The maximum number of retries for reading the file from object storage.</p> <code>5</code> <code>timeout</code> <code>int</code> <p>The timeout in seconds for reading the file from object storage.</p> <code>300</code> <code>enable_request_payer</code> <code>bool</code> <p>Whether to enable request payer for object storage.</p> <code>False</code> <code>compression_type</code> <code>str</code> <p>The compression type. If not specified, it will be detected automatically.</p> <code>'auto'</code> <p>Note</p> <p>Pairs format uses 1-based coordinate system for pos1 and pos2.</p> <p>Example</p> <pre><code>import polars_bio as pb\npb.register_pairs(\"contacts.pairs.gz\", \"hic_contacts\")\npb.sql(\"SELECT * FROM hic_contacts WHERE chr1 = 'chr1'\").collect()\n</code></pre> Source code in <code>polars_bio/sql.py</code> <pre><code>@staticmethod\ndef register_pairs(\n    path: str,\n    name: Union[str, None] = None,\n    chunk_size: int = 64,\n    concurrent_fetches: int = 8,\n    allow_anonymous: bool = True,\n    max_retries: int = 5,\n    timeout: int = 300,\n    enable_request_payer: bool = False,\n    compression_type: str = \"auto\",\n) -&gt; None:\n    \"\"\"\n    Register a Pairs (Hi-C) file as a Datafusion table.\n\n    The Pairs format (4DN project) stores chromatin contact data with columns:\n    readID, chr1, pos1, chr2, pos2, strand1, strand2.\n\n    Parameters:\n        path: The path to the Pairs file (.pairs, .pairs.gz, .pairs.bgz).\n        name: The name of the table. If *None*, the name will be generated automatically from the path.\n        chunk_size: The size in MB of a chunk when reading from an object store.\n        concurrent_fetches: The number of concurrent fetches when reading from an object store.\n        allow_anonymous: Whether to allow anonymous access to object storage.\n        max_retries: The maximum number of retries for reading the file from object storage.\n        timeout: The timeout in seconds for reading the file from object storage.\n        enable_request_payer: Whether to enable request payer for object storage.\n        compression_type: The compression type. If not specified, it will be detected automatically.\n\n    !!! note\n        Pairs format uses **1-based** coordinate system for pos1 and pos2.\n\n    !!! Example\n        ```python\n        import polars_bio as pb\n        pb.register_pairs(\"contacts.pairs.gz\", \"hic_contacts\")\n        pb.sql(\"SELECT * FROM hic_contacts WHERE chr1 = 'chr1'\").collect()\n        ```\n    \"\"\"\n\n    object_storage_options = PyObjectStorageOptions(\n        allow_anonymous=allow_anonymous,\n        enable_request_payer=enable_request_payer,\n        chunk_size=chunk_size,\n        concurrent_fetches=concurrent_fetches,\n        max_retries=max_retries,\n        timeout=timeout,\n        compression_type=compression_type,\n    )\n\n    pairs_read_options = PairsReadOptions(\n        object_storage_options=object_storage_options,\n    )\n    read_options = ReadOptions(pairs_read_options=pairs_read_options)\n    py_register_table(ctx, path, name, InputFormat.Pairs, read_options)\n</code></pre>"},{"location":"api/#polars_bio.data_processing.register_sam","title":"<code>register_sam(path, name=None, tag_fields=None)</code>  <code>staticmethod</code>","text":"<p>Register a SAM file as a Datafusion table.</p> <p>SAM (Sequence Alignment/Map) is the plain-text counterpart of BAM. This function reuses the BAM table provider, which auto-detects the format from the file extension.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the SAM file.</p> required <code>name</code> <code>Union[str, None]</code> <p>The name of the table. If None, the name will be generated automatically from the path.</p> <code>None</code> <code>tag_fields</code> <code>Union[list[str], None]</code> <p>List of SAM tag names to include as columns (e.g., [\"NM\", \"MD\", \"AS\"]). If None, no optional tags are parsed (default).</p> <code>None</code> <p>Example</p> <pre><code>import polars_bio as pb\npb.register_sam(\"test.sam\", \"my_sam\")\npb.sql(\"SELECT chrom, flags FROM my_sam\").limit(5).collect()\n</code></pre> Source code in <code>polars_bio/sql.py</code> <pre><code>@staticmethod\ndef register_sam(\n    path: str,\n    name: Union[str, None] = None,\n    tag_fields: Union[list[str], None] = None,\n) -&gt; None:\n    \"\"\"\n    Register a SAM file as a Datafusion table.\n\n    SAM (Sequence Alignment/Map) is the plain-text counterpart of BAM.\n    This function reuses the BAM table provider, which auto-detects\n    the format from the file extension.\n\n    Parameters:\n        path: The path to the SAM file.\n        name: The name of the table. If *None*, the name will be generated automatically from the path.\n        tag_fields: List of SAM tag names to include as columns (e.g., [\"NM\", \"MD\", \"AS\"]).\n            If None, no optional tags are parsed (default).\n\n    !!! Example\n        ```python\n        import polars_bio as pb\n        pb.register_sam(\"test.sam\", \"my_sam\")\n        pb.sql(\"SELECT chrom, flags FROM my_sam\").limit(5).collect()\n        ```\n    \"\"\"\n    bam_read_options = BamReadOptions(\n        tag_fields=tag_fields,\n    )\n    read_options = ReadOptions(bam_read_options=bam_read_options)\n    py_register_table(ctx, path, name, InputFormat.Sam, read_options)\n</code></pre>"},{"location":"api/#polars_bio.data_processing.register_vcf","title":"<code>register_vcf(path, name=None, info_fields=None, chunk_size=64, concurrent_fetches=8, allow_anonymous=True, max_retries=5, timeout=300, enable_request_payer=False, compression_type='auto')</code>  <code>staticmethod</code>","text":"<p>Register a VCF file as a Datafusion table.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the VCF file.</p> required <code>name</code> <code>Union[str, None]</code> <p>The name of the table. If None, the name of the table will be generated automatically based on the path.</p> <code>None</code> <code>info_fields</code> <code>Union[list[str], None]</code> <p>List of INFO field names to register. If None, all INFO fields will be detected automatically from the VCF header. Use this to limit registration to specific fields for better performance.</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>The size in MB of a chunk when reading from an object store. Default settings are optimized for large scale operations. For small scale (interactive) operations, it is recommended to decrease this value to 8-16.</p> <code>64</code> <code>concurrent_fetches</code> <code>int</code> <p>[GCS] The number of concurrent fetches when reading from an object store. Default settings are optimized for large scale operations. For small scale (interactive) operations, it is recommended to decrease this value to 1-2.</p> <code>8</code> <code>allow_anonymous</code> <code>bool</code> <p>[GCS, AWS S3] Whether to allow anonymous access to object storage.</p> <code>True</code> <code>enable_request_payer</code> <code>bool</code> <p>[AWS S3] Whether to enable request payer for object storage. This is useful for reading files from AWS S3 buckets that require request payer.</p> <code>False</code> <code>compression_type</code> <code>str</code> <p>The compression type of the VCF file. If not specified, it will be detected automatically..</p> <code>'auto'</code> <code>max_retries</code> <code>int</code> <p>The maximum number of retries for reading the file from object storage.</p> <code>5</code> <code>timeout</code> <code>int</code> <p>The timeout in seconds for reading the file from object storage.</p> <code>300</code> <p>Note</p> <p>VCF reader uses 1-based coordinate system for the <code>start</code> and <code>end</code> columns.</p> <p>Example</p> <p></p><pre><code>import polars_bio as pb\npb.register_vcf(\"/tmp/gnomad.v4.1.sv.sites.vcf.gz\")\n</code></pre> <pre><code>INFO:polars_bio:Table: gnomad_v4_1_sv_sites_gz registered for path: /tmp/gnomad.v4.1.sv.sites.vcf.gz\n</code></pre><p></p> <p>Tip</p> <p><code>chunk_size</code> and <code>concurrent_fetches</code> can be adjusted according to the network bandwidth and the size of the VCF file. As a rule of thumb for large scale operations (reading a whole VCF), it is recommended to the default values.</p> Source code in <code>polars_bio/sql.py</code> <pre><code>@staticmethod\ndef register_vcf(\n    path: str,\n    name: Union[str, None] = None,\n    info_fields: Union[list[str], None] = None,\n    chunk_size: int = 64,\n    concurrent_fetches: int = 8,\n    allow_anonymous: bool = True,\n    max_retries: int = 5,\n    timeout: int = 300,\n    enable_request_payer: bool = False,\n    compression_type: str = \"auto\",\n) -&gt; None:\n    \"\"\"\n    Register a VCF file as a Datafusion table.\n\n    Parameters:\n        path: The path to the VCF file.\n        name: The name of the table. If *None*, the name of the table will be generated automatically based on the path.\n        info_fields: List of INFO field names to register. If *None*, all INFO fields will be detected automatically from the VCF header. Use this to limit registration to specific fields for better performance.\n        chunk_size: The size in MB of a chunk when reading from an object store. Default settings are optimized for large scale operations. For small scale (interactive) operations, it is recommended to decrease this value to **8-16**.\n        concurrent_fetches: [GCS] The number of concurrent fetches when reading from an object store. Default settings are optimized for large scale operations. For small scale (interactive) operations, it is recommended to decrease this value to **1-2**.\n        allow_anonymous: [GCS, AWS S3] Whether to allow anonymous access to object storage.\n        enable_request_payer: [AWS S3] Whether to enable request payer for object storage. This is useful for reading files from AWS S3 buckets that require request payer.\n        compression_type: The compression type of the VCF file. If not specified, it will be detected automatically..\n        max_retries:  The maximum number of retries for reading the file from object storage.\n        timeout: The timeout in seconds for reading the file from object storage.\n    !!! note\n        VCF reader uses **1-based** coordinate system for the `start` and `end` columns.\n\n    !!! Example\n          ```python\n          import polars_bio as pb\n          pb.register_vcf(\"/tmp/gnomad.v4.1.sv.sites.vcf.gz\")\n          ```\n         ```shell\n         INFO:polars_bio:Table: gnomad_v4_1_sv_sites_gz registered for path: /tmp/gnomad.v4.1.sv.sites.vcf.gz\n         ```\n    !!! tip\n        `chunk_size` and `concurrent_fetches` can be adjusted according to the network bandwidth and the size of the VCF file. As a rule of thumb for large scale operations (reading a whole VCF), it is recommended to the default values.\n    \"\"\"\n\n    object_storage_options = PyObjectStorageOptions(\n        allow_anonymous=allow_anonymous,\n        enable_request_payer=enable_request_payer,\n        chunk_size=chunk_size,\n        concurrent_fetches=concurrent_fetches,\n        max_retries=max_retries,\n        timeout=timeout,\n        compression_type=compression_type,\n    )\n\n    # Use provided info_fields or autodetect from VCF header\n    if info_fields is not None:\n        all_info_fields = info_fields\n    else:\n        # Get all info fields from VCF header for automatic field detection\n        all_info_fields = None\n        try:\n            from .io import IOOperations\n\n            vcf_schema_df = IOOperations.describe_vcf(\n                path,\n                allow_anonymous=allow_anonymous,\n                enable_request_payer=enable_request_payer,\n                compression_type=compression_type,\n            )\n            all_info_fields = vcf_schema_df.select(\"name\").to_series().to_list()\n        except Exception:\n            # Fallback to empty list if unable to get info fields\n            all_info_fields = []\n\n    vcf_read_options = VcfReadOptions(\n        info_fields=all_info_fields,\n        object_storage_options=object_storage_options,\n    )\n    read_options = ReadOptions(vcf_read_options=vcf_read_options)\n    py_register_table(ctx, path, name, InputFormat.Vcf, read_options)\n</code></pre>"},{"location":"api/#polars_bio.data_processing.register_view","title":"<code>register_view(name, query)</code>  <code>staticmethod</code>","text":"<p>Register a query as a Datafusion view. This view can be used in genomic ranges operations, such as overlap, nearest, and count_overlaps. It is useful for filtering, transforming, and aggregating data prior to the range operation. When combined with the range operation, it can be used to perform complex in a streaming fashion end-to-end.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the table.</p> required <code>query</code> <code>str</code> <p>The SQL query.</p> required <p>Example</p> <p></p><pre><code>import polars_bio as pb\npb.register_vcf(\"gs://gcp-public-data--gnomad/release/4.1/vcf/exomes/gnomad.exomes.v4.1.sites.chr21.vcf.bgz\", \"gnomad_sv\")\npb.register_view(\"v_gnomad_sv\", \"SELECT replace(chrom,'chr', '') AS chrom, start, end FROM gnomad_sv\")\npb.sql(\"SELECT * FROM v_gnomad_sv\").limit(5).collect()\n</code></pre> <pre><code>  shape: (5, 3)\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502 chrom \u2506 start   \u2506 end     \u2502\n  \u2502 ---   \u2506 ---     \u2506 ---     \u2502\n  \u2502 str   \u2506 u32     \u2506 u32     \u2502\n  \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n  \u2502 21    \u2506 5031905 \u2506 5031905 \u2502\n  \u2502 21    \u2506 5031905 \u2506 5031905 \u2502\n  \u2502 21    \u2506 5031909 \u2506 5031909 \u2502\n  \u2502 21    \u2506 5031911 \u2506 5031911 \u2502\n  \u2502 21    \u2506 5031911 \u2506 5031911 \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre><p></p> Source code in <code>polars_bio/sql.py</code> <pre><code>@staticmethod\ndef register_view(name: str, query: str) -&gt; None:\n    \"\"\"\n    Register a query as a Datafusion view. This view can be used in genomic ranges operations,\n    such as overlap, nearest, and count_overlaps. It is useful for filtering, transforming, and aggregating data\n    prior to the range operation. When combined with the range operation, it can be used to perform complex in a streaming fashion end-to-end.\n\n    Parameters:\n        name: The name of the table.\n        query: The SQL query.\n\n    !!! Example\n          ```python\n          import polars_bio as pb\n          pb.register_vcf(\"gs://gcp-public-data--gnomad/release/4.1/vcf/exomes/gnomad.exomes.v4.1.sites.chr21.vcf.bgz\", \"gnomad_sv\")\n          pb.register_view(\"v_gnomad_sv\", \"SELECT replace(chrom,'chr', '') AS chrom, start, end FROM gnomad_sv\")\n          pb.sql(\"SELECT * FROM v_gnomad_sv\").limit(5).collect()\n          ```\n          ```shell\n            shape: (5, 3)\n            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n            \u2502 chrom \u2506 start   \u2506 end     \u2502\n            \u2502 ---   \u2506 ---     \u2506 ---     \u2502\n            \u2502 str   \u2506 u32     \u2506 u32     \u2502\n            \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n            \u2502 21    \u2506 5031905 \u2506 5031905 \u2502\n            \u2502 21    \u2506 5031905 \u2506 5031905 \u2502\n            \u2502 21    \u2506 5031909 \u2506 5031909 \u2502\n            \u2502 21    \u2506 5031911 \u2506 5031911 \u2502\n            \u2502 21    \u2506 5031911 \u2506 5031911 \u2502\n            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n          ```\n    \"\"\"\n    py_register_view(ctx, name, query)\n</code></pre>"},{"location":"api/#polars_bio.data_processing.sql","title":"<code>sql(query)</code>  <code>staticmethod</code>","text":"<p>Execute a SQL query on the registered tables.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The SQL query.</p> required <p>Example</p> <pre><code>import polars_bio as pb\npb.register_vcf(\"/tmp/gnomad.v4.1.sv.sites.vcf.gz\", \"gnomad_v4_1_sv\")\npb.sql(\"SELECT * FROM gnomad_v4_1_sv LIMIT 5\").collect()\n</code></pre> Source code in <code>polars_bio/sql.py</code> <pre><code>@staticmethod\ndef sql(query: str) -&gt; pl.LazyFrame:\n    \"\"\"\n    Execute a SQL query on the registered tables.\n\n    Parameters:\n        query: The SQL query.\n\n    !!! Example\n          ```python\n          import polars_bio as pb\n          pb.register_vcf(\"/tmp/gnomad.v4.1.sv.sites.vcf.gz\", \"gnomad_v4_1_sv\")\n          pb.sql(\"SELECT * FROM gnomad_v4_1_sv LIMIT 5\").collect()\n          ```\n    \"\"\"\n    df = py_read_sql(ctx, query)\n    return _lazy_scan(df)\n</code></pre>"},{"location":"api/#polars_bio.pileup_operations","title":"<code>pileup_operations</code>","text":"<p>Per-base read depth (pileup) operations on alignment files.</p> <p>Computes per-position depth from BAM/SAM/CRAM files by walking CIGAR operations, producing mosdepth-compatible coverage blocks.</p> Source code in <code>polars_bio/pileup_op.py</code> <pre><code>class PileupOperations:\n    \"\"\"Per-base read depth (pileup) operations on alignment files.\n\n    Computes per-position depth from BAM/SAM/CRAM files by walking CIGAR\n    operations, producing mosdepth-compatible coverage blocks.\n    \"\"\"\n\n    @staticmethod\n    def depth(\n        path: str,\n        filter_flag: int = 1796,\n        min_mapping_quality: int = 0,\n        binary_cigar: bool = True,\n        dense_mode: str = \"auto\",\n        use_zero_based: Optional[bool] = None,\n        per_base: bool = False,\n        output_type: str = \"polars.LazyFrame\",\n    ) -&gt; Union[pl.LazyFrame, pl.DataFrame, \"pd.DataFrame\"]:\n        \"\"\"Compute per-base read depth (pileup) from a BAM/SAM/CRAM file.\n\n        Walks CIGAR operations to produce coverage blocks -- similar to\n        mosdepth / samtools depth.\n\n        Args:\n            path: Path to alignment file (.bam, .sam, or .cram).\n                Index files (BAI/CSI/CRAI) are auto-discovered.\n            filter_flag: SAM flag mask -- reads with any of these flags are\n                excluded. Default 1796 (unmapped, secondary, failed QC,\n                duplicate).\n            min_mapping_quality: Minimum MAPQ threshold. Default 0\n                (no filter).\n            binary_cigar: Use binary CIGAR parsing (faster). Default True.\n            dense_mode: Accumulation strategy:\n\n                - ``\"auto\"`` -- use dense accumulation when contig lengths are\n                  available in schema metadata (default).\n                - ``\"force\"`` -- always use dense accumulation.\n                - ``\"disable\"`` -- always use sparse (event-list) accumulation.\n            use_zero_based: Coordinate system for output positions.\n\n                - ``None`` (default) -- use global config (``pb.options``),\n                  which defaults to 1-based.\n                - ``True`` -- 0-based half-open coordinates.\n                - ``False`` -- 1-based closed coordinates.\n            per_base: If True, emit one row per genomic position (like\n                ``samtools depth -a``) instead of RLE coverage blocks.\n                Requires dense mode (BAM header with contig lengths).\n                Default False.\n            output_type: One of ``\"polars.LazyFrame\"``,\n                ``\"polars.DataFrame\"``, or ``\"pandas.DataFrame\"``.\n\n        Returns:\n            DataFrame with columns depending on ``per_base``:\n\n            - Block mode (default): ``contig`` (Utf8), ``pos_start`` (Int32),\n              ``pos_end`` (Int32), ``coverage`` (Int16).\n            - Per-base mode: ``contig`` (Utf8), ``pos`` (Int32),\n              ``coverage`` (Int16).\n\n        Example:\n            ```python\n            import polars_bio as pb\n\n            # Basic depth computation (RLE blocks)\n            df = pb.depth(\"alignments.bam\").collect()\n\n            # Per-base output (one row per position)\n            df = pb.depth(\"alignments.bam\", per_base=True).collect()\n\n            # With MAPQ filter\n            df = pb.depth(\"alignments.bam\", min_mapping_quality=20).collect()\n\n            # As pandas DataFrame\n            pdf = pb.depth(\"alignments.bam\", output_type=\"pandas.DataFrame\")\n            ```\n        \"\"\"\n        from polars_bio.polars_bio import (\n            PileupOptions,\n            py_get_table_schema,\n            py_register_pileup_table,\n        )\n\n        zero_based = _resolve_zero_based(use_zero_based)\n\n        opts = PileupOptions(\n            filter_flag=filter_flag,\n            min_mapping_quality=min_mapping_quality,\n            binary_cigar=binary_cigar,\n            dense_mode=dense_mode,\n            zero_based=zero_based,\n            per_base=per_base,\n        )\n\n        # 1. Register table (no execution)\n        table_name = py_register_pileup_table(ctx, path, opts)\n\n        # 2. Get schema without materializing data\n        schema = py_get_table_schema(ctx, table_name)\n        empty_table = pa.table(\n            {field.name: pa.array([], type=field.type) for field in schema}\n        )\n        polars_schema = dict(pl.from_arrow(empty_table).schema)\n\n        # 3. Define streaming callback (executed only on .collect())\n        def _pileup_source(\n            with_columns: Union[pl.Expr, None],\n            predicate: Union[pl.Expr, None],\n            n_rows: Union[int, None],\n            _batch_size: Union[int, None],\n        ) -&gt; Iterator[pl.DataFrame]:\n            from polars_bio.polars_bio import py_read_table\n\n            from .context import ctx as _ctx\n\n            query_df = py_read_table(_ctx, table_name)\n\n            # Projection pushdown\n            projection_applied = False\n            if with_columns is not None:\n                requested_cols = _extract_column_names_from_expr(with_columns)\n                if requested_cols:\n                    try:\n                        select_exprs = [\n                            query_df.parse_sql_expr(f'\"{c}\"') for c in requested_cols\n                        ]\n                        query_df = query_df.select(*select_exprs)\n                        projection_applied = True\n                    except Exception as e:\n                        logger.debug(\"Projection pushdown failed: %s\", e)\n\n            # Predicate pushdown (reuses pattern from _overlap_source in io.py)\n            predicate_pushed_down = False\n            if predicate is not None:\n                try:\n                    from .predicate_translator import (\n                        datafusion_expr_to_sql,\n                        translate_predicate,\n                    )\n\n                    df_expr = translate_predicate(\n                        predicate,\n                        string_cols={\"contig\"},\n                        uint32_cols={\"pos\", \"pos_start\", \"pos_end\", \"coverage\"},\n                    )\n                    sql_predicate = datafusion_expr_to_sql(df_expr)\n                    native_expr = query_df.parse_sql_expr(sql_predicate)\n                    query_df = query_df.filter(native_expr)\n                    predicate_pushed_down = True\n                except Exception as e:\n                    logger.debug(\"Pileup predicate pushdown failed: %s\", e)\n\n            # Limit pushdown\n            if n_rows and n_rows &gt; 0:\n                query_df = query_df.limit(int(n_rows))\n\n            # Stream batches\n            df_stream = query_df.execute_stream()\n            progress_bar = tqdm(unit=\"rows\")\n            remaining = int(n_rows) if n_rows is not None else None\n\n            for batch in df_stream:\n                out = pl.DataFrame(batch.to_pyarrow())\n\n                # Client-side predicate filtering (fallback)\n                if predicate is not None and not predicate_pushed_down:\n                    out = out.filter(predicate)\n\n                # Client-side projection fallback\n                if with_columns is not None and not projection_applied:\n                    out = out.select(with_columns)\n\n                if remaining is not None:\n                    if remaining &lt;= 0:\n                        break\n                    if len(out) &gt; remaining:\n                        out = out.head(remaining)\n                    remaining -= len(out)\n\n                progress_bar.update(len(out))\n                yield out\n\n                if remaining is not None and remaining &lt;= 0:\n                    return\n\n            # Clean up registered table to free memory\n            try:\n                _ctx.deregister_table(table_name)\n            except Exception:\n                pass\n\n        # 4. Create lazy frame\n        lf = register_io_source(_pileup_source, schema=polars_schema)\n        set_coordinate_system(lf, zero_based)\n\n        # 5. Handle output_type\n        if output_type == \"polars.LazyFrame\":\n            return lf\n        elif output_type == \"polars.DataFrame\":\n            return lf.collect()\n        elif output_type == \"pandas.DataFrame\":\n            if pd is None:\n                raise ImportError(\n                    \"pandas is not installed. Please run `pip install pandas` \"\n                    \"or `pip install polars-bio[pandas]`.\"\n                )\n            return lf.collect().to_pandas()\n        else:\n            raise ValueError(f\"Invalid output_type: {output_type!r}\")\n</code></pre>"},{"location":"api/#polars_bio.pileup_operations.depth","title":"<code>depth(path, filter_flag=1796, min_mapping_quality=0, binary_cigar=True, dense_mode='auto', use_zero_based=None, per_base=False, output_type='polars.LazyFrame')</code>  <code>staticmethod</code>","text":"<p>Compute per-base read depth (pileup) from a BAM/SAM/CRAM file.</p> <p>Walks CIGAR operations to produce coverage blocks -- similar to mosdepth / samtools depth.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to alignment file (.bam, .sam, or .cram). Index files (BAI/CSI/CRAI) are auto-discovered.</p> required <code>filter_flag</code> <code>int</code> <p>SAM flag mask -- reads with any of these flags are excluded. Default 1796 (unmapped, secondary, failed QC, duplicate).</p> <code>1796</code> <code>min_mapping_quality</code> <code>int</code> <p>Minimum MAPQ threshold. Default 0 (no filter).</p> <code>0</code> <code>binary_cigar</code> <code>bool</code> <p>Use binary CIGAR parsing (faster). Default True.</p> <code>True</code> <code>dense_mode</code> <code>str</code> <p>Accumulation strategy:</p> <ul> <li><code>\"auto\"</code> -- use dense accumulation when contig lengths are   available in schema metadata (default).</li> <li><code>\"force\"</code> -- always use dense accumulation.</li> <li><code>\"disable\"</code> -- always use sparse (event-list) accumulation.</li> </ul> <code>'auto'</code> <code>use_zero_based</code> <code>Optional[bool]</code> <p>Coordinate system for output positions.</p> <ul> <li><code>None</code> (default) -- use global config (<code>pb.options</code>),   which defaults to 1-based.</li> <li><code>True</code> -- 0-based half-open coordinates.</li> <li><code>False</code> -- 1-based closed coordinates.</li> </ul> <code>None</code> <code>per_base</code> <code>bool</code> <p>If True, emit one row per genomic position (like <code>samtools depth -a</code>) instead of RLE coverage blocks. Requires dense mode (BAM header with contig lengths). Default False.</p> <code>False</code> <code>output_type</code> <code>str</code> <p>One of <code>\"polars.LazyFrame\"</code>, <code>\"polars.DataFrame\"</code>, or <code>\"pandas.DataFrame\"</code>.</p> <code>'polars.LazyFrame'</code> <p>Returns:</p> Type Description <code>Union[LazyFrame, DataFrame, DataFrame]</code> <p>DataFrame with columns depending on <code>per_base</code>:</p> <code>Union[LazyFrame, DataFrame, DataFrame]</code> <ul> <li>Block mode (default): <code>contig</code> (Utf8), <code>pos_start</code> (Int32), <code>pos_end</code> (Int32), <code>coverage</code> (Int16).</li> </ul> <code>Union[LazyFrame, DataFrame, DataFrame]</code> <ul> <li>Per-base mode: <code>contig</code> (Utf8), <code>pos</code> (Int32), <code>coverage</code> (Int16).</li> </ul> Example <pre><code>import polars_bio as pb\n\n# Basic depth computation (RLE blocks)\ndf = pb.depth(\"alignments.bam\").collect()\n\n# Per-base output (one row per position)\ndf = pb.depth(\"alignments.bam\", per_base=True).collect()\n\n# With MAPQ filter\ndf = pb.depth(\"alignments.bam\", min_mapping_quality=20).collect()\n\n# As pandas DataFrame\npdf = pb.depth(\"alignments.bam\", output_type=\"pandas.DataFrame\")\n</code></pre> Source code in <code>polars_bio/pileup_op.py</code> <pre><code>@staticmethod\ndef depth(\n    path: str,\n    filter_flag: int = 1796,\n    min_mapping_quality: int = 0,\n    binary_cigar: bool = True,\n    dense_mode: str = \"auto\",\n    use_zero_based: Optional[bool] = None,\n    per_base: bool = False,\n    output_type: str = \"polars.LazyFrame\",\n) -&gt; Union[pl.LazyFrame, pl.DataFrame, \"pd.DataFrame\"]:\n    \"\"\"Compute per-base read depth (pileup) from a BAM/SAM/CRAM file.\n\n    Walks CIGAR operations to produce coverage blocks -- similar to\n    mosdepth / samtools depth.\n\n    Args:\n        path: Path to alignment file (.bam, .sam, or .cram).\n            Index files (BAI/CSI/CRAI) are auto-discovered.\n        filter_flag: SAM flag mask -- reads with any of these flags are\n            excluded. Default 1796 (unmapped, secondary, failed QC,\n            duplicate).\n        min_mapping_quality: Minimum MAPQ threshold. Default 0\n            (no filter).\n        binary_cigar: Use binary CIGAR parsing (faster). Default True.\n        dense_mode: Accumulation strategy:\n\n            - ``\"auto\"`` -- use dense accumulation when contig lengths are\n              available in schema metadata (default).\n            - ``\"force\"`` -- always use dense accumulation.\n            - ``\"disable\"`` -- always use sparse (event-list) accumulation.\n        use_zero_based: Coordinate system for output positions.\n\n            - ``None`` (default) -- use global config (``pb.options``),\n              which defaults to 1-based.\n            - ``True`` -- 0-based half-open coordinates.\n            - ``False`` -- 1-based closed coordinates.\n        per_base: If True, emit one row per genomic position (like\n            ``samtools depth -a``) instead of RLE coverage blocks.\n            Requires dense mode (BAM header with contig lengths).\n            Default False.\n        output_type: One of ``\"polars.LazyFrame\"``,\n            ``\"polars.DataFrame\"``, or ``\"pandas.DataFrame\"``.\n\n    Returns:\n        DataFrame with columns depending on ``per_base``:\n\n        - Block mode (default): ``contig`` (Utf8), ``pos_start`` (Int32),\n          ``pos_end`` (Int32), ``coverage`` (Int16).\n        - Per-base mode: ``contig`` (Utf8), ``pos`` (Int32),\n          ``coverage`` (Int16).\n\n    Example:\n        ```python\n        import polars_bio as pb\n\n        # Basic depth computation (RLE blocks)\n        df = pb.depth(\"alignments.bam\").collect()\n\n        # Per-base output (one row per position)\n        df = pb.depth(\"alignments.bam\", per_base=True).collect()\n\n        # With MAPQ filter\n        df = pb.depth(\"alignments.bam\", min_mapping_quality=20).collect()\n\n        # As pandas DataFrame\n        pdf = pb.depth(\"alignments.bam\", output_type=\"pandas.DataFrame\")\n        ```\n    \"\"\"\n    from polars_bio.polars_bio import (\n        PileupOptions,\n        py_get_table_schema,\n        py_register_pileup_table,\n    )\n\n    zero_based = _resolve_zero_based(use_zero_based)\n\n    opts = PileupOptions(\n        filter_flag=filter_flag,\n        min_mapping_quality=min_mapping_quality,\n        binary_cigar=binary_cigar,\n        dense_mode=dense_mode,\n        zero_based=zero_based,\n        per_base=per_base,\n    )\n\n    # 1. Register table (no execution)\n    table_name = py_register_pileup_table(ctx, path, opts)\n\n    # 2. Get schema without materializing data\n    schema = py_get_table_schema(ctx, table_name)\n    empty_table = pa.table(\n        {field.name: pa.array([], type=field.type) for field in schema}\n    )\n    polars_schema = dict(pl.from_arrow(empty_table).schema)\n\n    # 3. Define streaming callback (executed only on .collect())\n    def _pileup_source(\n        with_columns: Union[pl.Expr, None],\n        predicate: Union[pl.Expr, None],\n        n_rows: Union[int, None],\n        _batch_size: Union[int, None],\n    ) -&gt; Iterator[pl.DataFrame]:\n        from polars_bio.polars_bio import py_read_table\n\n        from .context import ctx as _ctx\n\n        query_df = py_read_table(_ctx, table_name)\n\n        # Projection pushdown\n        projection_applied = False\n        if with_columns is not None:\n            requested_cols = _extract_column_names_from_expr(with_columns)\n            if requested_cols:\n                try:\n                    select_exprs = [\n                        query_df.parse_sql_expr(f'\"{c}\"') for c in requested_cols\n                    ]\n                    query_df = query_df.select(*select_exprs)\n                    projection_applied = True\n                except Exception as e:\n                    logger.debug(\"Projection pushdown failed: %s\", e)\n\n        # Predicate pushdown (reuses pattern from _overlap_source in io.py)\n        predicate_pushed_down = False\n        if predicate is not None:\n            try:\n                from .predicate_translator import (\n                    datafusion_expr_to_sql,\n                    translate_predicate,\n                )\n\n                df_expr = translate_predicate(\n                    predicate,\n                    string_cols={\"contig\"},\n                    uint32_cols={\"pos\", \"pos_start\", \"pos_end\", \"coverage\"},\n                )\n                sql_predicate = datafusion_expr_to_sql(df_expr)\n                native_expr = query_df.parse_sql_expr(sql_predicate)\n                query_df = query_df.filter(native_expr)\n                predicate_pushed_down = True\n            except Exception as e:\n                logger.debug(\"Pileup predicate pushdown failed: %s\", e)\n\n        # Limit pushdown\n        if n_rows and n_rows &gt; 0:\n            query_df = query_df.limit(int(n_rows))\n\n        # Stream batches\n        df_stream = query_df.execute_stream()\n        progress_bar = tqdm(unit=\"rows\")\n        remaining = int(n_rows) if n_rows is not None else None\n\n        for batch in df_stream:\n            out = pl.DataFrame(batch.to_pyarrow())\n\n            # Client-side predicate filtering (fallback)\n            if predicate is not None and not predicate_pushed_down:\n                out = out.filter(predicate)\n\n            # Client-side projection fallback\n            if with_columns is not None and not projection_applied:\n                out = out.select(with_columns)\n\n            if remaining is not None:\n                if remaining &lt;= 0:\n                    break\n                if len(out) &gt; remaining:\n                    out = out.head(remaining)\n                remaining -= len(out)\n\n            progress_bar.update(len(out))\n            yield out\n\n            if remaining is not None and remaining &lt;= 0:\n                return\n\n        # Clean up registered table to free memory\n        try:\n            _ctx.deregister_table(table_name)\n        except Exception:\n            pass\n\n    # 4. Create lazy frame\n    lf = register_io_source(_pileup_source, schema=polars_schema)\n    set_coordinate_system(lf, zero_based)\n\n    # 5. Handle output_type\n    if output_type == \"polars.LazyFrame\":\n        return lf\n    elif output_type == \"polars.DataFrame\":\n        return lf.collect()\n    elif output_type == \"pandas.DataFrame\":\n        if pd is None:\n            raise ImportError(\n                \"pandas is not installed. Please run `pip install pandas` \"\n                \"or `pip install polars-bio[pandas]`.\"\n            )\n        return lf.collect().to_pandas()\n    else:\n        raise ValueError(f\"Invalid output_type: {output_type!r}\")\n</code></pre>"},{"location":"api/#polars_bio.range_operations","title":"<code>range_operations</code>","text":"Source code in <code>polars_bio/range_op.py</code> <pre><code>class IntervalOperations:\n\n    @staticmethod\n    def overlap(\n        df1: Union[str, pl.DataFrame, pl.LazyFrame, \"pd.DataFrame\"],\n        df2: Union[str, pl.DataFrame, pl.LazyFrame, \"pd.DataFrame\"],\n        suffixes: tuple[str, str] = (\"_1\", \"_2\"),\n        on_cols: Union[list[str], None] = None,\n        cols1: Union[list[str], None] = [\"chrom\", \"start\", \"end\"],\n        cols2: Union[list[str], None] = [\"chrom\", \"start\", \"end\"],\n        algorithm: str = \"Coitrees\",\n        low_memory: bool = False,\n        output_type: str = \"polars.LazyFrame\",\n        read_options1: Union[ReadOptions, None] = None,\n        read_options2: Union[ReadOptions, None] = None,\n        projection_pushdown: bool = True,\n    ) -&gt; Union[pl.LazyFrame, pl.DataFrame, \"pd.DataFrame\", datafusion.DataFrame]:\n        \"\"\"\n        Find pairs of overlapping genomic intervals.\n        Bioframe inspired API.\n\n        The coordinate system (0-based or 1-based) is automatically detected from\n        DataFrame metadata set at I/O time. Both inputs must have the same coordinate\n        system.\n\n        Parameters:\n            df1: Can be a path to a file, a polars DataFrame, or a pandas DataFrame or a registered table (see [register_vcf](api.md#polars_bio.register_vcf)). CSV with a header, BED and Parquet are supported.\n            df2: Can be a path to a file, a polars DataFrame, or a pandas DataFrame or a registered table. CSV with a header, BED  and Parquet are supported.\n            cols1: The names of columns containing the chromosome, start and end of the\n                genomic intervals, provided separately for each set.\n            cols2:  The names of columns containing the chromosome, start and end of the\n                genomic intervals, provided separately for each set.\n            suffixes: Suffixes for the columns of the two overlapped sets.\n            on_cols: List of additional column names to join on. default is None.\n            algorithm: The algorithm to use for the overlap operation. Available options: Coitrees, IntervalTree, ArrayIntervalTree, Lapper, SuperIntervals\n            low_memory: If True, use low memory method for output generation. This caps the output batch size, trading some performance for significantly lower peak memory consumption. Recommended for operations that produce very large result sets.\n            output_type: Type of the output. default is \"polars.LazyFrame\", \"polars.DataFrame\", or \"pandas.DataFrame\" or \"datafusion.DataFrame\" are also supported.\n            read_options1: Additional options for reading the input files.\n            read_options2: Additional options for reading the input files.\n            projection_pushdown: Enable column projection pushdown to optimize query performance by only reading the necessary columns at the DataFusion level.\n\n        Returns:\n            **polars.LazyFrame** or polars.DataFrame or pandas.DataFrame of the overlapping intervals.\n\n        Raises:\n            MissingCoordinateSystemError: If either input lacks coordinate system metadata\n                and `datafusion.bio.coordinate_system_check` is \"true\" (default). Use polars-bio\n                I/O functions (scan_*, read_*) which automatically set metadata, or set it manually\n                on Polars DataFrames via `df.config_meta.set(coordinate_system_zero_based=True/False)`\n                or on Pandas DataFrames via `df.attrs[\"coordinate_system_zero_based\"] = True/False`.\n                Set `pb.set_option(\"datafusion.bio.coordinate_system_check\", False)` to disable\n                strict checking and fall back to global coordinate system setting.\n            CoordinateSystemMismatchError: If inputs have different coordinate systems.\n\n        Note:\n            1. The default output format, i.e.  [LazyFrame](https://docs.pola.rs/api/python/stable/reference/lazyframe/index.html), is recommended for large datasets as it supports output streaming and lazy evaluation.\n            This enables efficient processing of large datasets without loading the entire output dataset into memory.\n            2. Streaming is only supported for polars.LazyFrame output.\n\n        Example:\n            ```python\n            import polars_bio as pb\n            import pandas as pd\n\n            df1 = pd.DataFrame([\n                ['chr1', 1, 5],\n                ['chr1', 3, 8],\n                ['chr1', 8, 10],\n                ['chr1', 12, 14]],\n            columns=['chrom', 'start', 'end']\n            )\n            df1.attrs[\"coordinate_system_zero_based\"] = False  # 1-based coordinates\n\n            df2 = pd.DataFrame(\n            [['chr1', 4, 8],\n             ['chr1', 10, 11]],\n            columns=['chrom', 'start', 'end' ]\n            )\n            df2.attrs[\"coordinate_system_zero_based\"] = False  # 1-based coordinates\n\n            overlapping_intervals = pb.overlap(df1, df2, output_type=\"pandas.DataFrame\")\n\n            overlapping_intervals\n                chrom_1         start_1     end_1 chrom_2       start_2  end_2\n            0     chr1            1          5     chr1            4          8\n            1     chr1            3          8     chr1            4          8\n\n            ```\n\n        Todo:\n             Support for on_cols.\n        \"\"\"\n\n        _validate_overlap_input(cols1, cols2, on_cols, suffixes, output_type)\n\n        # Get filter_op from DataFrame metadata\n        filter_op = _get_filter_op_from_metadata(df1, df2)\n\n        cols1 = DEFAULT_INTERVAL_COLUMNS if cols1 is None else cols1\n        cols2 = DEFAULT_INTERVAL_COLUMNS if cols2 is None else cols2\n        range_options = RangeOptions(\n            range_op=RangeOp.Overlap,\n            filter_op=filter_op,\n            suffixes=suffixes,\n            columns_1=cols1,\n            columns_2=cols2,\n            overlap_alg=algorithm,\n            overlap_low_memory=low_memory,\n        )\n\n        return range_operation(\n            df1,\n            df2,\n            range_options,\n            output_type,\n            ctx,\n            read_options1,\n            read_options2,\n            projection_pushdown,\n        )\n\n    @staticmethod\n    def nearest(\n        df1: Union[str, pl.DataFrame, pl.LazyFrame, \"pd.DataFrame\"],\n        df2: Union[str, pl.DataFrame, pl.LazyFrame, \"pd.DataFrame\"],\n        suffixes: tuple[str, str] = (\"_1\", \"_2\"),\n        on_cols: Union[list[str], None] = None,\n        cols1: Union[list[str], None] = [\"chrom\", \"start\", \"end\"],\n        cols2: Union[list[str], None] = [\"chrom\", \"start\", \"end\"],\n        k: int = 1,\n        overlap: bool = True,\n        distance: bool = True,\n        output_type: str = \"polars.LazyFrame\",\n        read_options: Union[ReadOptions, None] = None,\n        projection_pushdown: bool = True,\n    ) -&gt; Union[pl.LazyFrame, pl.DataFrame, \"pd.DataFrame\", datafusion.DataFrame]:\n        \"\"\"\n        Find pairs of closest genomic intervals.\n        Bioframe inspired API.\n\n        The coordinate system (0-based or 1-based) is automatically detected from\n        DataFrame metadata set at I/O time. Both inputs must have the same coordinate\n        system.\n\n        Parameters:\n            df1: Can be a path to a file, a polars DataFrame, or a pandas DataFrame or a registered table (see [register_vcf](api.md#polars_bio.register_vcf)). CSV with a header, BED and Parquet are supported.\n            df2: Can be a path to a file, a polars DataFrame, or a pandas DataFrame or a registered table. CSV with a header, BED  and Parquet are supported.\n            cols1: The names of columns containing the chromosome, start and end of the\n                genomic intervals, provided separately for each set.\n            cols2:  The names of columns containing the chromosome, start and end of the\n                genomic intervals, provided separately for each set.\n            suffixes: Suffixes for the columns of the two overlapped sets.\n            on_cols: List of additional column names to join on. default is None.\n            k: Number of nearest neighbors to return per query interval. Default is 1.\n            overlap: If True (default), include overlapping intervals in results. If False, only return non-overlapping nearest neighbors.\n            distance: If True (default), include a `distance` column in the output. If False, omit it.\n            output_type: Type of the output. default is \"polars.LazyFrame\", \"polars.DataFrame\", or \"pandas.DataFrame\" or \"datafusion.DataFrame\" are also supported.\n            read_options: Additional options for reading the input files.\n            projection_pushdown: Enable column projection pushdown to optimize query performance by only reading the necessary columns at the DataFusion level.\n\n        Returns:\n            **polars.LazyFrame** or polars.DataFrame or pandas.DataFrame of the overlapping intervals.\n\n        Raises:\n            MissingCoordinateSystemError: If either input lacks coordinate system metadata\n                and `datafusion.bio.coordinate_system_check` is \"true\" (default).\n            CoordinateSystemMismatchError: If inputs have different coordinate systems.\n\n        Note:\n            The default output format, i.e. [LazyFrame](https://docs.pola.rs/api/python/stable/reference/lazyframe/index.html), is recommended for large datasets as it supports output streaming and lazy evaluation.\n            This enables efficient processing of large datasets without loading the entire output dataset into memory.\n\n        Example:\n\n        Todo:\n            Support for on_cols.\n        \"\"\"\n\n        _validate_overlap_input(cols1, cols2, on_cols, suffixes, output_type)\n\n        # Get filter_op from DataFrame metadata\n        filter_op = _get_filter_op_from_metadata(df1, df2)\n\n        cols1 = DEFAULT_INTERVAL_COLUMNS if cols1 is None else cols1\n        cols2 = DEFAULT_INTERVAL_COLUMNS if cols2 is None else cols2\n        range_options = RangeOptions(\n            range_op=RangeOp.Nearest,\n            filter_op=filter_op,\n            suffixes=suffixes,\n            columns_1=cols1,\n            columns_2=cols2,\n            nearest_k=k,\n            include_overlaps=overlap,\n            compute_distance=distance,\n        )\n        return range_operation(\n            df1,\n            df2,\n            range_options,\n            output_type,\n            ctx,\n            read_options,\n            projection_pushdown=projection_pushdown,\n        )\n\n    @staticmethod\n    def coverage(\n        df1: Union[str, pl.DataFrame, pl.LazyFrame, \"pd.DataFrame\"],\n        df2: Union[str, pl.DataFrame, pl.LazyFrame, \"pd.DataFrame\"],\n        suffixes: tuple[str, str] = (\"_1\", \"_2\"),\n        on_cols: Union[list[str], None] = None,\n        cols1: Union[list[str], None] = [\"chrom\", \"start\", \"end\"],\n        cols2: Union[list[str], None] = [\"chrom\", \"start\", \"end\"],\n        output_type: str = \"polars.LazyFrame\",\n        read_options: Union[ReadOptions, None] = None,\n        projection_pushdown: bool = True,\n    ) -&gt; Union[pl.LazyFrame, pl.DataFrame, \"pd.DataFrame\", datafusion.DataFrame]:\n        \"\"\"\n        Calculate intervals coverage.\n        Bioframe inspired API.\n\n        The coordinate system (0-based or 1-based) is automatically detected from\n        DataFrame metadata set at I/O time. Both inputs must have the same coordinate\n        system.\n\n        Parameters:\n            df1: Can be a path to a file, a polars DataFrame, or a pandas DataFrame or a registered table (see [register_vcf](api.md#polars_bio.register_vcf)). CSV with a header, BED and Parquet are supported.\n            df2: Can be a path to a file, a polars DataFrame, or a pandas DataFrame or a registered table. CSV with a header, BED  and Parquet are supported.\n            cols1: The names of columns containing the chromosome, start and end of the\n                genomic intervals, provided separately for each set.\n            cols2:  The names of columns containing the chromosome, start and end of the\n                genomic intervals, provided separately for each set.\n            suffixes: Suffixes for the columns of the two overlapped sets.\n            on_cols: List of additional column names to join on. default is None.\n            output_type: Type of the output. default is \"polars.LazyFrame\", \"polars.DataFrame\", or \"pandas.DataFrame\" or \"datafusion.DataFrame\" are also supported.\n            read_options: Additional options for reading the input files.\n            projection_pushdown: Enable column projection pushdown to optimize query performance by only reading the necessary columns at the DataFusion level.\n\n        Returns:\n            **polars.LazyFrame** or polars.DataFrame or pandas.DataFrame of the overlapping intervals.\n\n        Raises:\n            MissingCoordinateSystemError: If either input lacks coordinate system metadata\n                and `datafusion.bio.coordinate_system_check` is \"true\" (default).\n            CoordinateSystemMismatchError: If inputs have different coordinate systems.\n\n        Note:\n            The default output format, i.e. [LazyFrame](https://docs.pola.rs/api/python/stable/reference/lazyframe/index.html), is recommended for large datasets as it supports output streaming and lazy evaluation.\n            This enables efficient processing of large datasets without loading the entire output dataset into memory.\n\n        Example:\n\n        Todo:\n            Support for on_cols.\n        \"\"\"\n\n        _validate_overlap_input(cols1, cols2, on_cols, suffixes, output_type)\n\n        # Get filter_op from DataFrame metadata\n        filter_op = _get_filter_op_from_metadata(df1, df2)\n\n        cols1 = DEFAULT_INTERVAL_COLUMNS if cols1 is None else cols1\n        cols2 = DEFAULT_INTERVAL_COLUMNS if cols2 is None else cols2\n        range_options = RangeOptions(\n            range_op=RangeOp.Coverage,\n            filter_op=filter_op,\n            suffixes=suffixes,\n            columns_1=cols1,\n            columns_2=cols2,\n        )\n        return range_operation(\n            df2,\n            df1,\n            range_options,\n            output_type,\n            ctx,\n            read_options,\n            projection_pushdown=projection_pushdown,\n        )\n\n    @staticmethod\n    def count_overlaps(\n        df1: Union[str, pl.DataFrame, pl.LazyFrame, \"pd.DataFrame\"],\n        df2: Union[str, pl.DataFrame, pl.LazyFrame, \"pd.DataFrame\"],\n        suffixes: tuple[str, str] = (\"\", \"_\"),\n        cols1: Union[list[str], None] = [\"chrom\", \"start\", \"end\"],\n        cols2: Union[list[str], None] = [\"chrom\", \"start\", \"end\"],\n        on_cols: Union[list[str], None] = None,\n        output_type: str = \"polars.LazyFrame\",\n        naive_query: bool = True,\n        projection_pushdown: bool = True,\n    ) -&gt; Union[pl.LazyFrame, pl.DataFrame, \"pd.DataFrame\", datafusion.DataFrame]:\n        \"\"\"\n        Count pairs of overlapping genomic intervals.\n        Bioframe inspired API.\n\n        The coordinate system (0-based or 1-based) is automatically detected from\n        DataFrame metadata set at I/O time. Both inputs must have the same coordinate\n        system.\n\n        Parameters:\n            df1: Can be a path to a file, a polars DataFrame, or a pandas DataFrame or a registered table (see [register_vcf](api.md#polars_bio.register_vcf)). CSV with a header, BED and Parquet are supported.\n            df2: Can be a path to a file, a polars DataFrame, or a pandas DataFrame or a registered table. CSV with a header, BED  and Parquet are supported.\n            suffixes: Suffixes for the columns of the two overlapped sets.\n            cols1: The names of columns containing the chromosome, start and end of the\n                genomic intervals, provided separately for each set.\n            cols2:  The names of columns containing the chromosome, start and end of the\n                genomic intervals, provided separately for each set.\n            on_cols: List of additional column names to join on. default is None.\n            output_type: Type of the output. default is \"polars.LazyFrame\", \"polars.DataFrame\", or \"pandas.DataFrame\" or \"datafusion.DataFrame\" are also supported.\n            naive_query: If True, use naive query for counting overlaps based on overlaps.\n            projection_pushdown: Enable column projection pushdown to optimize query performance by only reading the necessary columns at the DataFusion level.\n\n        Returns:\n            **polars.LazyFrame** or polars.DataFrame or pandas.DataFrame of the overlapping intervals.\n\n        Raises:\n            MissingCoordinateSystemError: If either input lacks coordinate system metadata\n                and `datafusion.bio.coordinate_system_check` is \"true\" (default).\n            CoordinateSystemMismatchError: If inputs have different coordinate systems.\n\n        Example:\n            ```python\n            import polars_bio as pb\n            import pandas as pd\n\n            df1 = pd.DataFrame([\n                ['chr1', 1, 5],\n                ['chr1', 3, 8],\n                ['chr1', 8, 10],\n                ['chr1', 12, 14]],\n            columns=['chrom', 'start', 'end']\n            )\n            df1.attrs[\"coordinate_system_zero_based\"] = False  # 1-based coordinates\n\n            df2 = pd.DataFrame(\n            [['chr1', 4, 8],\n             ['chr1', 10, 11]],\n            columns=['chrom', 'start', 'end' ]\n            )\n            df2.attrs[\"coordinate_system_zero_based\"] = False  # 1-based coordinates\n\n            counts = pb.count_overlaps(df1, df2, output_type=\"pandas.DataFrame\")\n\n            counts\n\n            chrom  start  end  count\n            0  chr1      1    5      1\n            1  chr1      3    8      1\n            2  chr1      8   10      0\n            3  chr1     12   14      0\n            ```\n\n        Todo:\n             Support return_input.\n        \"\"\"\n        _validate_overlap_input(cols1, cols2, on_cols, suffixes, output_type)\n\n        # Get filter_op and zero_based from DataFrame metadata\n        zero_based = validate_coordinate_systems(df1, df2, ctx)\n        filter_op = FilterOp.Strict if zero_based else FilterOp.Weak\n\n        my_ctx = get_py_ctx()\n        on_cols = [] if on_cols is None else on_cols\n        cols1 = DEFAULT_INTERVAL_COLUMNS if cols1 is None else cols1\n        cols2 = DEFAULT_INTERVAL_COLUMNS if cols2 is None else cols2\n        if naive_query:\n            range_options = RangeOptions(\n                range_op=RangeOp.CountOverlapsNaive,\n                filter_op=filter_op,\n                suffixes=suffixes,\n                columns_1=cols1,\n                columns_2=cols2,\n            )\n            return range_operation(df2, df1, range_options, output_type, ctx)\n        df1 = read_df_to_datafusion(my_ctx, df1)\n        df2 = read_df_to_datafusion(my_ctx, df2)\n\n        curr_cols = set(df1.schema().names) | set(df2.schema().names)\n        s1start_s2end = prevent_column_collision(\"s1starts2end\", curr_cols)\n        s1end_s2start = prevent_column_collision(\"s1ends2start\", curr_cols)\n        contig = prevent_column_collision(\"contig\", curr_cols)\n        count = prevent_column_collision(\"count\", curr_cols)\n        starts = prevent_column_collision(\"starts\", curr_cols)\n        ends = prevent_column_collision(\"ends\", curr_cols)\n        is_s1 = prevent_column_collision(\"is_s1\", curr_cols)\n        suff, _ = suffixes\n        df1, df2 = df2, df1\n        df1 = df1.select(\n            *(\n                [\n                    literal(1).alias(is_s1),\n                    col(cols1[1]).alias(s1start_s2end),\n                    col(cols1[2]).alias(s1end_s2start),\n                    col(cols1[0]).alias(contig),\n                ]\n                + on_cols\n            )\n        )\n        df2 = df2.select(\n            *(\n                [\n                    literal(0).alias(is_s1),\n                    col(cols2[2]).alias(s1end_s2start),\n                    col(cols2[1]).alias(s1start_s2end),\n                    col(cols2[0]).alias(contig),\n                ]\n                + on_cols\n            )\n        )\n\n        df = df1.union(df2)\n\n        partitioning = [col(contig)] + [col(c) for c in on_cols]\n        df = df.select(\n            *(\n                [\n                    s1start_s2end,\n                    s1end_s2start,\n                    contig,\n                    is_s1,\n                    datafusion.functions.sum(col(is_s1))\n                    .over(\n                        datafusion.expr.Window(\n                            partition_by=partitioning,\n                            order_by=[\n                                col(s1start_s2end).sort(),\n                                col(is_s1).sort(ascending=zero_based),\n                            ],\n                        )\n                    )\n                    .alias(starts),\n                    datafusion.functions.sum(col(is_s1))\n                    .over(\n                        datafusion.expr.Window(\n                            partition_by=partitioning,\n                            order_by=[\n                                col(s1end_s2start).sort(),\n                                col(is_s1).sort(ascending=(not zero_based)),\n                            ],\n                        )\n                    )\n                    .alias(ends),\n                ]\n                + on_cols\n            )\n        )\n        df = df.filter(col(is_s1) == 0)\n        df = df.select(\n            *(\n                [\n                    col(contig).alias(cols1[0] + suff),\n                    col(s1end_s2start).alias(cols1[1] + suff),\n                    col(s1start_s2end).alias(cols1[2] + suff),\n                ]\n                + on_cols\n                + [(col(starts) - col(ends)).alias(count)]\n            )\n        )\n\n        return convert_result(df, output_type)\n\n    @staticmethod\n    def merge(\n        df: Union[str, pl.DataFrame, pl.LazyFrame, \"pd.DataFrame\"],\n        min_dist: int = 0,\n        cols: Union[list[str], None] = [\"chrom\", \"start\", \"end\"],\n        on_cols: Union[list[str], None] = None,\n        output_type: str = \"polars.LazyFrame\",\n        projection_pushdown: bool = True,\n    ) -&gt; Union[pl.LazyFrame, pl.DataFrame, \"pd.DataFrame\", datafusion.DataFrame]:\n        \"\"\"\n        Merge overlapping intervals. It is assumed that start &lt; end.\n\n        The coordinate system (0-based or 1-based) is automatically detected from\n        DataFrame metadata set at I/O time.\n\n        Parameters:\n            df: Can be a path to a file, a polars DataFrame, or a pandas DataFrame. CSV with a header, BED  and Parquet are supported.\n            min_dist: Minimum distance (integer) between intervals to merge. Default is 0.\n            cols: The names of columns containing the chromosome, start and end of the\n                genomic intervals, provided separately for each set.\n            on_cols: List of additional column names for clustering. default is None.\n            output_type: Type of the output. default is \"polars.LazyFrame\", \"polars.DataFrame\", or \"pandas.DataFrame\" or \"datafusion.DataFrame\" are also supported.\n            projection_pushdown: Enable column projection pushdown to optimize query performance by only reading the necessary columns at the DataFusion level.\n\n        Returns:\n            **polars.LazyFrame** or polars.DataFrame or pandas.DataFrame of the overlapping intervals.\n\n        Raises:\n            MissingCoordinateSystemError: If input lacks coordinate system metadata\n                and `datafusion.bio.coordinate_system_check` is \"true\" (default).\n\n        Example:\n\n        Todo:\n            Support for on_cols.\n        \"\"\"\n        suffixes = (\"_1\", \"_2\")\n        _validate_overlap_input(cols, cols, on_cols, suffixes, output_type)\n\n        # Get filter_op from DataFrame metadata\n        filter_op = _get_filter_op_from_metadata_single(df)\n\n        cols = DEFAULT_INTERVAL_COLUMNS if cols is None else cols\n        range_options = RangeOptions(\n            range_op=RangeOp.Merge,\n            filter_op=filter_op,\n            columns_1=cols,\n            columns_2=cols,\n            min_dist=min_dist,\n        )\n\n        return range_operation(\n            df,\n            df,\n            range_options,\n            output_type,\n            ctx,\n            projection_pushdown=projection_pushdown,\n        )\n\n    @staticmethod\n    def cluster(\n        df: Union[str, pl.DataFrame, pl.LazyFrame, \"pd.DataFrame\"],\n        min_dist: int = 0,\n        cols: Union[list[str], None] = [\"chrom\", \"start\", \"end\"],\n        output_type: str = \"polars.LazyFrame\",\n        projection_pushdown: bool = True,\n    ) -&gt; Union[pl.LazyFrame, pl.DataFrame, \"pd.DataFrame\", datafusion.DataFrame]:\n        \"\"\"\n        Assign cluster IDs to overlapping or nearby genomic intervals.\n\n        Groups intervals that overlap or are within ``min_dist`` of each other\n        into clusters. Each row is annotated with a cluster ID and the\n        cluster's merged start/end boundaries.\n\n        Bioframe inspired API.\n\n        The coordinate system (0-based or 1-based) is automatically detected from\n        DataFrame metadata set at I/O time.\n\n        Parameters:\n            df: Can be a path to a file, a polars DataFrame, or a pandas DataFrame. CSV with a header, BED and Parquet are supported.\n            min_dist: Minimum distance (integer) between intervals to cluster. Default is 0.\n            cols: The names of columns containing the chromosome, start and end of the\n                genomic intervals.\n            output_type: Type of the output. default is \"polars.LazyFrame\", \"polars.DataFrame\", or \"pandas.DataFrame\" or \"datafusion.DataFrame\" are also supported.\n            projection_pushdown: Enable column projection pushdown.\n\n        Returns:\n            **polars.LazyFrame** or polars.DataFrame or pandas.DataFrame with original\n            interval columns plus ``cluster``, ``cluster_start``, ``cluster_end``.\n\n        Raises:\n            MissingCoordinateSystemError: If input lacks coordinate system metadata\n                and ``datafusion.bio.coordinate_system_check`` is \"true\" (default).\n        \"\"\"\n        suffixes = (\"_1\", \"_2\")\n        _validate_overlap_input(cols, cols, None, suffixes, output_type)\n\n        filter_op = _get_filter_op_from_metadata_single(df)\n\n        cols = DEFAULT_INTERVAL_COLUMNS if cols is None else cols\n        range_options = RangeOptions(\n            range_op=RangeOp.Cluster,\n            filter_op=filter_op,\n            columns_1=cols,\n            columns_2=cols,\n            min_dist=min_dist,\n        )\n\n        return range_operation(\n            df,\n            df,\n            range_options,\n            output_type,\n            ctx,\n            projection_pushdown=projection_pushdown,\n        )\n\n    @staticmethod\n    def complement(\n        df: Union[str, pl.DataFrame, pl.LazyFrame, \"pd.DataFrame\"],\n        view_df: Union[pl.DataFrame, pl.LazyFrame, \"pd.DataFrame\", None] = None,\n        cols: Union[list[str], None] = [\"chrom\", \"start\", \"end\"],\n        view_cols: Union[list[str], None] = None,\n        output_type: str = \"polars.LazyFrame\",\n        projection_pushdown: bool = True,\n    ) -&gt; Union[pl.LazyFrame, pl.DataFrame, \"pd.DataFrame\", datafusion.DataFrame]:\n        \"\"\"\n        Compute the complement of genomic intervals \u2014 the gaps between them.\n\n        Returns intervals that represent the genomic regions **not** covered\n        by the input intervals. If ``view_df`` is provided, gaps are computed\n        within the boundaries of the view (e.g., chromosome sizes); otherwise\n        each contig spans ``[0, i64::MAX)``.\n\n        Bioframe inspired API.\n\n        The coordinate system (0-based or 1-based) is automatically detected from\n        DataFrame metadata set at I/O time.\n\n        Parameters:\n            df: Can be a path to a file, a polars DataFrame, or a pandas DataFrame. CSV with a header, BED and Parquet are supported.\n            view_df: Optional DataFrame defining contig boundaries (e.g., chromosome sizes). Each row should have contig, start, end columns.\n            cols: The names of columns containing the chromosome, start and end of the\n                genomic intervals.\n            view_cols: Column names for the view table. Defaults to ``cols`` when not specified.\n            output_type: Type of the output. default is \"polars.LazyFrame\", \"polars.DataFrame\", or \"pandas.DataFrame\" or \"datafusion.DataFrame\" are also supported.\n            projection_pushdown: Enable column projection pushdown.\n\n        Returns:\n            **polars.LazyFrame** or polars.DataFrame or pandas.DataFrame of complement\n            intervals (contig, start, end).\n\n        Raises:\n            MissingCoordinateSystemError: If input lacks coordinate system metadata\n                and ``datafusion.bio.coordinate_system_check`` is \"true\" (default).\n        \"\"\"\n        suffixes = (\"_1\", \"_2\")\n        _validate_overlap_input(cols, cols, None, suffixes, output_type)\n\n        filter_op = _get_filter_op_from_metadata_single(df)\n\n        cols = DEFAULT_INTERVAL_COLUMNS if cols is None else cols\n        view_cols = cols if view_cols is None else view_cols\n\n        # Register view table in DataFusion if provided\n        view_table_name = None\n        if view_df is not None:\n            view_table_name = _register_view_table(view_df, view_cols[0])\n        else:\n            logger.warning(\n                \"No view_df provided \u2014 complement will span [0, i64::MAX) per contig. \"\n                \"Pass a view_df with contig boundaries (e.g., chromosome sizes) \"\n                \"for meaningful results.\"\n            )\n\n        range_options = RangeOptions(\n            range_op=RangeOp.Complement,\n            filter_op=filter_op,\n            columns_1=cols,\n            columns_2=cols,\n            view_table=view_table_name,\n            view_columns=view_cols,\n        )\n\n        return range_operation(\n            df,\n            df,\n            range_options,\n            output_type,\n            ctx,\n            projection_pushdown=projection_pushdown,\n        )\n\n    @staticmethod\n    def subtract(\n        df1: Union[str, pl.DataFrame, pl.LazyFrame, \"pd.DataFrame\"],\n        df2: Union[str, pl.DataFrame, pl.LazyFrame, \"pd.DataFrame\"],\n        cols1: Union[list[str], None] = [\"chrom\", \"start\", \"end\"],\n        cols2: Union[list[str], None] = [\"chrom\", \"start\", \"end\"],\n        output_type: str = \"polars.LazyFrame\",\n        projection_pushdown: bool = True,\n    ) -&gt; Union[pl.LazyFrame, pl.DataFrame, \"pd.DataFrame\", datafusion.DataFrame]:\n        \"\"\"\n        Subtract the second set of intervals from the first.\n\n        For each interval in ``df1``, removes any portion that overlaps with\n        intervals in ``df2``. The result contains the remaining fragments.\n\n        Bioframe inspired API.\n\n        The coordinate system (0-based or 1-based) is automatically detected from\n        DataFrame metadata set at I/O time. Both inputs must have the same coordinate\n        system.\n\n        Parameters:\n            df1: Can be a path to a file, a polars DataFrame, or a pandas DataFrame. CSV with a header, BED and Parquet are supported.\n            df2: Can be a path to a file, a polars DataFrame, or a pandas DataFrame. CSV with a header, BED and Parquet are supported.\n            cols1: The names of columns containing the chromosome, start and end of the\n                genomic intervals for the first set.\n            cols2: The names of columns containing the chromosome, start and end of the\n                genomic intervals for the second set.\n            output_type: Type of the output. default is \"polars.LazyFrame\", \"polars.DataFrame\", or \"pandas.DataFrame\" or \"datafusion.DataFrame\" are also supported.\n            projection_pushdown: Enable column projection pushdown.\n\n        Returns:\n            **polars.LazyFrame** or polars.DataFrame or pandas.DataFrame of the\n            remaining interval fragments (contig, start, end).\n\n        Raises:\n            MissingCoordinateSystemError: If either input lacks coordinate system metadata\n                and ``datafusion.bio.coordinate_system_check`` is \"true\" (default).\n            CoordinateSystemMismatchError: If inputs have different coordinate systems.\n        \"\"\"\n        suffixes = (\"_1\", \"_2\")\n        _validate_overlap_input(cols1, cols2, None, suffixes, output_type)\n\n        filter_op = _get_filter_op_from_metadata(df1, df2)\n\n        cols1 = DEFAULT_INTERVAL_COLUMNS if cols1 is None else cols1\n        cols2 = DEFAULT_INTERVAL_COLUMNS if cols2 is None else cols2\n        range_options = RangeOptions(\n            range_op=RangeOp.Subtract,\n            filter_op=filter_op,\n            columns_1=cols1,\n            columns_2=cols2,\n        )\n\n        return range_operation(\n            df1,\n            df2,\n            range_options,\n            output_type,\n            ctx,\n            projection_pushdown=projection_pushdown,\n        )\n</code></pre>"},{"location":"api/#polars_bio.range_operations.cluster","title":"<code>cluster(df, min_dist=0, cols=['chrom', 'start', 'end'], output_type='polars.LazyFrame', projection_pushdown=True)</code>  <code>staticmethod</code>","text":"<p>Assign cluster IDs to overlapping or nearby genomic intervals.</p> <p>Groups intervals that overlap or are within <code>min_dist</code> of each other into clusters. Each row is annotated with a cluster ID and the cluster's merged start/end boundaries.</p> <p>Bioframe inspired API.</p> <p>The coordinate system (0-based or 1-based) is automatically detected from DataFrame metadata set at I/O time.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Union[str, DataFrame, LazyFrame, 'pd.DataFrame']</code> <p>Can be a path to a file, a polars DataFrame, or a pandas DataFrame. CSV with a header, BED and Parquet are supported.</p> required <code>min_dist</code> <code>int</code> <p>Minimum distance (integer) between intervals to cluster. Default is 0.</p> <code>0</code> <code>cols</code> <code>Union[list[str], None]</code> <p>The names of columns containing the chromosome, start and end of the genomic intervals.</p> <code>['chrom', 'start', 'end']</code> <code>output_type</code> <code>str</code> <p>Type of the output. default is \"polars.LazyFrame\", \"polars.DataFrame\", or \"pandas.DataFrame\" or \"datafusion.DataFrame\" are also supported.</p> <code>'polars.LazyFrame'</code> <code>projection_pushdown</code> <code>bool</code> <p>Enable column projection pushdown.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[LazyFrame, DataFrame, 'pd.DataFrame', DataFrame]</code> <p>polars.LazyFrame or polars.DataFrame or pandas.DataFrame with original</p> <code>Union[LazyFrame, DataFrame, 'pd.DataFrame', DataFrame]</code> <p>interval columns plus <code>cluster</code>, <code>cluster_start</code>, <code>cluster_end</code>.</p> <p>Raises:</p> Type Description <code>MissingCoordinateSystemError</code> <p>If input lacks coordinate system metadata and <code>datafusion.bio.coordinate_system_check</code> is \"true\" (default).</p> Source code in <code>polars_bio/range_op.py</code> <pre><code>@staticmethod\ndef cluster(\n    df: Union[str, pl.DataFrame, pl.LazyFrame, \"pd.DataFrame\"],\n    min_dist: int = 0,\n    cols: Union[list[str], None] = [\"chrom\", \"start\", \"end\"],\n    output_type: str = \"polars.LazyFrame\",\n    projection_pushdown: bool = True,\n) -&gt; Union[pl.LazyFrame, pl.DataFrame, \"pd.DataFrame\", datafusion.DataFrame]:\n    \"\"\"\n    Assign cluster IDs to overlapping or nearby genomic intervals.\n\n    Groups intervals that overlap or are within ``min_dist`` of each other\n    into clusters. Each row is annotated with a cluster ID and the\n    cluster's merged start/end boundaries.\n\n    Bioframe inspired API.\n\n    The coordinate system (0-based or 1-based) is automatically detected from\n    DataFrame metadata set at I/O time.\n\n    Parameters:\n        df: Can be a path to a file, a polars DataFrame, or a pandas DataFrame. CSV with a header, BED and Parquet are supported.\n        min_dist: Minimum distance (integer) between intervals to cluster. Default is 0.\n        cols: The names of columns containing the chromosome, start and end of the\n            genomic intervals.\n        output_type: Type of the output. default is \"polars.LazyFrame\", \"polars.DataFrame\", or \"pandas.DataFrame\" or \"datafusion.DataFrame\" are also supported.\n        projection_pushdown: Enable column projection pushdown.\n\n    Returns:\n        **polars.LazyFrame** or polars.DataFrame or pandas.DataFrame with original\n        interval columns plus ``cluster``, ``cluster_start``, ``cluster_end``.\n\n    Raises:\n        MissingCoordinateSystemError: If input lacks coordinate system metadata\n            and ``datafusion.bio.coordinate_system_check`` is \"true\" (default).\n    \"\"\"\n    suffixes = (\"_1\", \"_2\")\n    _validate_overlap_input(cols, cols, None, suffixes, output_type)\n\n    filter_op = _get_filter_op_from_metadata_single(df)\n\n    cols = DEFAULT_INTERVAL_COLUMNS if cols is None else cols\n    range_options = RangeOptions(\n        range_op=RangeOp.Cluster,\n        filter_op=filter_op,\n        columns_1=cols,\n        columns_2=cols,\n        min_dist=min_dist,\n    )\n\n    return range_operation(\n        df,\n        df,\n        range_options,\n        output_type,\n        ctx,\n        projection_pushdown=projection_pushdown,\n    )\n</code></pre>"},{"location":"api/#polars_bio.range_operations.complement","title":"<code>complement(df, view_df=None, cols=['chrom', 'start', 'end'], view_cols=None, output_type='polars.LazyFrame', projection_pushdown=True)</code>  <code>staticmethod</code>","text":"<p>Compute the complement of genomic intervals \u2014 the gaps between them.</p> <p>Returns intervals that represent the genomic regions not covered by the input intervals. If <code>view_df</code> is provided, gaps are computed within the boundaries of the view (e.g., chromosome sizes); otherwise each contig spans <code>[0, i64::MAX)</code>.</p> <p>Bioframe inspired API.</p> <p>The coordinate system (0-based or 1-based) is automatically detected from DataFrame metadata set at I/O time.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Union[str, DataFrame, LazyFrame, 'pd.DataFrame']</code> <p>Can be a path to a file, a polars DataFrame, or a pandas DataFrame. CSV with a header, BED and Parquet are supported.</p> required <code>view_df</code> <code>Union[DataFrame, LazyFrame, 'pd.DataFrame', None]</code> <p>Optional DataFrame defining contig boundaries (e.g., chromosome sizes). Each row should have contig, start, end columns.</p> <code>None</code> <code>cols</code> <code>Union[list[str], None]</code> <p>The names of columns containing the chromosome, start and end of the genomic intervals.</p> <code>['chrom', 'start', 'end']</code> <code>view_cols</code> <code>Union[list[str], None]</code> <p>Column names for the view table. Defaults to <code>cols</code> when not specified.</p> <code>None</code> <code>output_type</code> <code>str</code> <p>Type of the output. default is \"polars.LazyFrame\", \"polars.DataFrame\", or \"pandas.DataFrame\" or \"datafusion.DataFrame\" are also supported.</p> <code>'polars.LazyFrame'</code> <code>projection_pushdown</code> <code>bool</code> <p>Enable column projection pushdown.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[LazyFrame, DataFrame, 'pd.DataFrame', DataFrame]</code> <p>polars.LazyFrame or polars.DataFrame or pandas.DataFrame of complement</p> <code>Union[LazyFrame, DataFrame, 'pd.DataFrame', DataFrame]</code> <p>intervals (contig, start, end).</p> <p>Raises:</p> Type Description <code>MissingCoordinateSystemError</code> <p>If input lacks coordinate system metadata and <code>datafusion.bio.coordinate_system_check</code> is \"true\" (default).</p> Source code in <code>polars_bio/range_op.py</code> <pre><code>@staticmethod\ndef complement(\n    df: Union[str, pl.DataFrame, pl.LazyFrame, \"pd.DataFrame\"],\n    view_df: Union[pl.DataFrame, pl.LazyFrame, \"pd.DataFrame\", None] = None,\n    cols: Union[list[str], None] = [\"chrom\", \"start\", \"end\"],\n    view_cols: Union[list[str], None] = None,\n    output_type: str = \"polars.LazyFrame\",\n    projection_pushdown: bool = True,\n) -&gt; Union[pl.LazyFrame, pl.DataFrame, \"pd.DataFrame\", datafusion.DataFrame]:\n    \"\"\"\n    Compute the complement of genomic intervals \u2014 the gaps between them.\n\n    Returns intervals that represent the genomic regions **not** covered\n    by the input intervals. If ``view_df`` is provided, gaps are computed\n    within the boundaries of the view (e.g., chromosome sizes); otherwise\n    each contig spans ``[0, i64::MAX)``.\n\n    Bioframe inspired API.\n\n    The coordinate system (0-based or 1-based) is automatically detected from\n    DataFrame metadata set at I/O time.\n\n    Parameters:\n        df: Can be a path to a file, a polars DataFrame, or a pandas DataFrame. CSV with a header, BED and Parquet are supported.\n        view_df: Optional DataFrame defining contig boundaries (e.g., chromosome sizes). Each row should have contig, start, end columns.\n        cols: The names of columns containing the chromosome, start and end of the\n            genomic intervals.\n        view_cols: Column names for the view table. Defaults to ``cols`` when not specified.\n        output_type: Type of the output. default is \"polars.LazyFrame\", \"polars.DataFrame\", or \"pandas.DataFrame\" or \"datafusion.DataFrame\" are also supported.\n        projection_pushdown: Enable column projection pushdown.\n\n    Returns:\n        **polars.LazyFrame** or polars.DataFrame or pandas.DataFrame of complement\n        intervals (contig, start, end).\n\n    Raises:\n        MissingCoordinateSystemError: If input lacks coordinate system metadata\n            and ``datafusion.bio.coordinate_system_check`` is \"true\" (default).\n    \"\"\"\n    suffixes = (\"_1\", \"_2\")\n    _validate_overlap_input(cols, cols, None, suffixes, output_type)\n\n    filter_op = _get_filter_op_from_metadata_single(df)\n\n    cols = DEFAULT_INTERVAL_COLUMNS if cols is None else cols\n    view_cols = cols if view_cols is None else view_cols\n\n    # Register view table in DataFusion if provided\n    view_table_name = None\n    if view_df is not None:\n        view_table_name = _register_view_table(view_df, view_cols[0])\n    else:\n        logger.warning(\n            \"No view_df provided \u2014 complement will span [0, i64::MAX) per contig. \"\n            \"Pass a view_df with contig boundaries (e.g., chromosome sizes) \"\n            \"for meaningful results.\"\n        )\n\n    range_options = RangeOptions(\n        range_op=RangeOp.Complement,\n        filter_op=filter_op,\n        columns_1=cols,\n        columns_2=cols,\n        view_table=view_table_name,\n        view_columns=view_cols,\n    )\n\n    return range_operation(\n        df,\n        df,\n        range_options,\n        output_type,\n        ctx,\n        projection_pushdown=projection_pushdown,\n    )\n</code></pre>"},{"location":"api/#polars_bio.range_operations.count_overlaps","title":"<code>count_overlaps(df1, df2, suffixes=('', '_'), cols1=['chrom', 'start', 'end'], cols2=['chrom', 'start', 'end'], on_cols=None, output_type='polars.LazyFrame', naive_query=True, projection_pushdown=True)</code>  <code>staticmethod</code>","text":"<p>Count pairs of overlapping genomic intervals. Bioframe inspired API.</p> <p>The coordinate system (0-based or 1-based) is automatically detected from DataFrame metadata set at I/O time. Both inputs must have the same coordinate system.</p> <p>Parameters:</p> Name Type Description Default <code>df1</code> <code>Union[str, DataFrame, LazyFrame, 'pd.DataFrame']</code> <p>Can be a path to a file, a polars DataFrame, or a pandas DataFrame or a registered table (see register_vcf). CSV with a header, BED and Parquet are supported.</p> required <code>df2</code> <code>Union[str, DataFrame, LazyFrame, 'pd.DataFrame']</code> <p>Can be a path to a file, a polars DataFrame, or a pandas DataFrame or a registered table. CSV with a header, BED  and Parquet are supported.</p> required <code>suffixes</code> <code>tuple[str, str]</code> <p>Suffixes for the columns of the two overlapped sets.</p> <code>('', '_')</code> <code>cols1</code> <code>Union[list[str], None]</code> <p>The names of columns containing the chromosome, start and end of the genomic intervals, provided separately for each set.</p> <code>['chrom', 'start', 'end']</code> <code>cols2</code> <code>Union[list[str], None]</code> <p>The names of columns containing the chromosome, start and end of the genomic intervals, provided separately for each set.</p> <code>['chrom', 'start', 'end']</code> <code>on_cols</code> <code>Union[list[str], None]</code> <p>List of additional column names to join on. default is None.</p> <code>None</code> <code>output_type</code> <code>str</code> <p>Type of the output. default is \"polars.LazyFrame\", \"polars.DataFrame\", or \"pandas.DataFrame\" or \"datafusion.DataFrame\" are also supported.</p> <code>'polars.LazyFrame'</code> <code>naive_query</code> <code>bool</code> <p>If True, use naive query for counting overlaps based on overlaps.</p> <code>True</code> <code>projection_pushdown</code> <code>bool</code> <p>Enable column projection pushdown to optimize query performance by only reading the necessary columns at the DataFusion level.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[LazyFrame, DataFrame, 'pd.DataFrame', DataFrame]</code> <p>polars.LazyFrame or polars.DataFrame or pandas.DataFrame of the overlapping intervals.</p> <p>Raises:</p> Type Description <code>MissingCoordinateSystemError</code> <p>If either input lacks coordinate system metadata and <code>datafusion.bio.coordinate_system_check</code> is \"true\" (default).</p> <code>CoordinateSystemMismatchError</code> <p>If inputs have different coordinate systems.</p> Example <pre><code>import polars_bio as pb\nimport pandas as pd\n\ndf1 = pd.DataFrame([\n    ['chr1', 1, 5],\n    ['chr1', 3, 8],\n    ['chr1', 8, 10],\n    ['chr1', 12, 14]],\ncolumns=['chrom', 'start', 'end']\n)\ndf1.attrs[\"coordinate_system_zero_based\"] = False  # 1-based coordinates\n\ndf2 = pd.DataFrame(\n[['chr1', 4, 8],\n ['chr1', 10, 11]],\ncolumns=['chrom', 'start', 'end' ]\n)\ndf2.attrs[\"coordinate_system_zero_based\"] = False  # 1-based coordinates\n\ncounts = pb.count_overlaps(df1, df2, output_type=\"pandas.DataFrame\")\n\ncounts\n\nchrom  start  end  count\n0  chr1      1    5      1\n1  chr1      3    8      1\n2  chr1      8   10      0\n3  chr1     12   14      0\n</code></pre> Todo <p>Support return_input.</p> Source code in <code>polars_bio/range_op.py</code> <pre><code>@staticmethod\ndef count_overlaps(\n    df1: Union[str, pl.DataFrame, pl.LazyFrame, \"pd.DataFrame\"],\n    df2: Union[str, pl.DataFrame, pl.LazyFrame, \"pd.DataFrame\"],\n    suffixes: tuple[str, str] = (\"\", \"_\"),\n    cols1: Union[list[str], None] = [\"chrom\", \"start\", \"end\"],\n    cols2: Union[list[str], None] = [\"chrom\", \"start\", \"end\"],\n    on_cols: Union[list[str], None] = None,\n    output_type: str = \"polars.LazyFrame\",\n    naive_query: bool = True,\n    projection_pushdown: bool = True,\n) -&gt; Union[pl.LazyFrame, pl.DataFrame, \"pd.DataFrame\", datafusion.DataFrame]:\n    \"\"\"\n    Count pairs of overlapping genomic intervals.\n    Bioframe inspired API.\n\n    The coordinate system (0-based or 1-based) is automatically detected from\n    DataFrame metadata set at I/O time. Both inputs must have the same coordinate\n    system.\n\n    Parameters:\n        df1: Can be a path to a file, a polars DataFrame, or a pandas DataFrame or a registered table (see [register_vcf](api.md#polars_bio.register_vcf)). CSV with a header, BED and Parquet are supported.\n        df2: Can be a path to a file, a polars DataFrame, or a pandas DataFrame or a registered table. CSV with a header, BED  and Parquet are supported.\n        suffixes: Suffixes for the columns of the two overlapped sets.\n        cols1: The names of columns containing the chromosome, start and end of the\n            genomic intervals, provided separately for each set.\n        cols2:  The names of columns containing the chromosome, start and end of the\n            genomic intervals, provided separately for each set.\n        on_cols: List of additional column names to join on. default is None.\n        output_type: Type of the output. default is \"polars.LazyFrame\", \"polars.DataFrame\", or \"pandas.DataFrame\" or \"datafusion.DataFrame\" are also supported.\n        naive_query: If True, use naive query for counting overlaps based on overlaps.\n        projection_pushdown: Enable column projection pushdown to optimize query performance by only reading the necessary columns at the DataFusion level.\n\n    Returns:\n        **polars.LazyFrame** or polars.DataFrame or pandas.DataFrame of the overlapping intervals.\n\n    Raises:\n        MissingCoordinateSystemError: If either input lacks coordinate system metadata\n            and `datafusion.bio.coordinate_system_check` is \"true\" (default).\n        CoordinateSystemMismatchError: If inputs have different coordinate systems.\n\n    Example:\n        ```python\n        import polars_bio as pb\n        import pandas as pd\n\n        df1 = pd.DataFrame([\n            ['chr1', 1, 5],\n            ['chr1', 3, 8],\n            ['chr1', 8, 10],\n            ['chr1', 12, 14]],\n        columns=['chrom', 'start', 'end']\n        )\n        df1.attrs[\"coordinate_system_zero_based\"] = False  # 1-based coordinates\n\n        df2 = pd.DataFrame(\n        [['chr1', 4, 8],\n         ['chr1', 10, 11]],\n        columns=['chrom', 'start', 'end' ]\n        )\n        df2.attrs[\"coordinate_system_zero_based\"] = False  # 1-based coordinates\n\n        counts = pb.count_overlaps(df1, df2, output_type=\"pandas.DataFrame\")\n\n        counts\n\n        chrom  start  end  count\n        0  chr1      1    5      1\n        1  chr1      3    8      1\n        2  chr1      8   10      0\n        3  chr1     12   14      0\n        ```\n\n    Todo:\n         Support return_input.\n    \"\"\"\n    _validate_overlap_input(cols1, cols2, on_cols, suffixes, output_type)\n\n    # Get filter_op and zero_based from DataFrame metadata\n    zero_based = validate_coordinate_systems(df1, df2, ctx)\n    filter_op = FilterOp.Strict if zero_based else FilterOp.Weak\n\n    my_ctx = get_py_ctx()\n    on_cols = [] if on_cols is None else on_cols\n    cols1 = DEFAULT_INTERVAL_COLUMNS if cols1 is None else cols1\n    cols2 = DEFAULT_INTERVAL_COLUMNS if cols2 is None else cols2\n    if naive_query:\n        range_options = RangeOptions(\n            range_op=RangeOp.CountOverlapsNaive,\n            filter_op=filter_op,\n            suffixes=suffixes,\n            columns_1=cols1,\n            columns_2=cols2,\n        )\n        return range_operation(df2, df1, range_options, output_type, ctx)\n    df1 = read_df_to_datafusion(my_ctx, df1)\n    df2 = read_df_to_datafusion(my_ctx, df2)\n\n    curr_cols = set(df1.schema().names) | set(df2.schema().names)\n    s1start_s2end = prevent_column_collision(\"s1starts2end\", curr_cols)\n    s1end_s2start = prevent_column_collision(\"s1ends2start\", curr_cols)\n    contig = prevent_column_collision(\"contig\", curr_cols)\n    count = prevent_column_collision(\"count\", curr_cols)\n    starts = prevent_column_collision(\"starts\", curr_cols)\n    ends = prevent_column_collision(\"ends\", curr_cols)\n    is_s1 = prevent_column_collision(\"is_s1\", curr_cols)\n    suff, _ = suffixes\n    df1, df2 = df2, df1\n    df1 = df1.select(\n        *(\n            [\n                literal(1).alias(is_s1),\n                col(cols1[1]).alias(s1start_s2end),\n                col(cols1[2]).alias(s1end_s2start),\n                col(cols1[0]).alias(contig),\n            ]\n            + on_cols\n        )\n    )\n    df2 = df2.select(\n        *(\n            [\n                literal(0).alias(is_s1),\n                col(cols2[2]).alias(s1end_s2start),\n                col(cols2[1]).alias(s1start_s2end),\n                col(cols2[0]).alias(contig),\n            ]\n            + on_cols\n        )\n    )\n\n    df = df1.union(df2)\n\n    partitioning = [col(contig)] + [col(c) for c in on_cols]\n    df = df.select(\n        *(\n            [\n                s1start_s2end,\n                s1end_s2start,\n                contig,\n                is_s1,\n                datafusion.functions.sum(col(is_s1))\n                .over(\n                    datafusion.expr.Window(\n                        partition_by=partitioning,\n                        order_by=[\n                            col(s1start_s2end).sort(),\n                            col(is_s1).sort(ascending=zero_based),\n                        ],\n                    )\n                )\n                .alias(starts),\n                datafusion.functions.sum(col(is_s1))\n                .over(\n                    datafusion.expr.Window(\n                        partition_by=partitioning,\n                        order_by=[\n                            col(s1end_s2start).sort(),\n                            col(is_s1).sort(ascending=(not zero_based)),\n                        ],\n                    )\n                )\n                .alias(ends),\n            ]\n            + on_cols\n        )\n    )\n    df = df.filter(col(is_s1) == 0)\n    df = df.select(\n        *(\n            [\n                col(contig).alias(cols1[0] + suff),\n                col(s1end_s2start).alias(cols1[1] + suff),\n                col(s1start_s2end).alias(cols1[2] + suff),\n            ]\n            + on_cols\n            + [(col(starts) - col(ends)).alias(count)]\n        )\n    )\n\n    return convert_result(df, output_type)\n</code></pre>"},{"location":"api/#polars_bio.range_operations.coverage","title":"<code>coverage(df1, df2, suffixes=('_1', '_2'), on_cols=None, cols1=['chrom', 'start', 'end'], cols2=['chrom', 'start', 'end'], output_type='polars.LazyFrame', read_options=None, projection_pushdown=True)</code>  <code>staticmethod</code>","text":"<p>Calculate intervals coverage. Bioframe inspired API.</p> <p>The coordinate system (0-based or 1-based) is automatically detected from DataFrame metadata set at I/O time. Both inputs must have the same coordinate system.</p> <p>Parameters:</p> Name Type Description Default <code>df1</code> <code>Union[str, DataFrame, LazyFrame, 'pd.DataFrame']</code> <p>Can be a path to a file, a polars DataFrame, or a pandas DataFrame or a registered table (see register_vcf). CSV with a header, BED and Parquet are supported.</p> required <code>df2</code> <code>Union[str, DataFrame, LazyFrame, 'pd.DataFrame']</code> <p>Can be a path to a file, a polars DataFrame, or a pandas DataFrame or a registered table. CSV with a header, BED  and Parquet are supported.</p> required <code>cols1</code> <code>Union[list[str], None]</code> <p>The names of columns containing the chromosome, start and end of the genomic intervals, provided separately for each set.</p> <code>['chrom', 'start', 'end']</code> <code>cols2</code> <code>Union[list[str], None]</code> <p>The names of columns containing the chromosome, start and end of the genomic intervals, provided separately for each set.</p> <code>['chrom', 'start', 'end']</code> <code>suffixes</code> <code>tuple[str, str]</code> <p>Suffixes for the columns of the two overlapped sets.</p> <code>('_1', '_2')</code> <code>on_cols</code> <code>Union[list[str], None]</code> <p>List of additional column names to join on. default is None.</p> <code>None</code> <code>output_type</code> <code>str</code> <p>Type of the output. default is \"polars.LazyFrame\", \"polars.DataFrame\", or \"pandas.DataFrame\" or \"datafusion.DataFrame\" are also supported.</p> <code>'polars.LazyFrame'</code> <code>read_options</code> <code>Union[ReadOptions, None]</code> <p>Additional options for reading the input files.</p> <code>None</code> <code>projection_pushdown</code> <code>bool</code> <p>Enable column projection pushdown to optimize query performance by only reading the necessary columns at the DataFusion level.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[LazyFrame, DataFrame, 'pd.DataFrame', DataFrame]</code> <p>polars.LazyFrame or polars.DataFrame or pandas.DataFrame of the overlapping intervals.</p> <p>Raises:</p> Type Description <code>MissingCoordinateSystemError</code> <p>If either input lacks coordinate system metadata and <code>datafusion.bio.coordinate_system_check</code> is \"true\" (default).</p> <code>CoordinateSystemMismatchError</code> <p>If inputs have different coordinate systems.</p> Note <p>The default output format, i.e. LazyFrame, is recommended for large datasets as it supports output streaming and lazy evaluation. This enables efficient processing of large datasets without loading the entire output dataset into memory.</p> <p>Example:</p> Todo <p>Support for on_cols.</p> Source code in <code>polars_bio/range_op.py</code> <pre><code>@staticmethod\ndef coverage(\n    df1: Union[str, pl.DataFrame, pl.LazyFrame, \"pd.DataFrame\"],\n    df2: Union[str, pl.DataFrame, pl.LazyFrame, \"pd.DataFrame\"],\n    suffixes: tuple[str, str] = (\"_1\", \"_2\"),\n    on_cols: Union[list[str], None] = None,\n    cols1: Union[list[str], None] = [\"chrom\", \"start\", \"end\"],\n    cols2: Union[list[str], None] = [\"chrom\", \"start\", \"end\"],\n    output_type: str = \"polars.LazyFrame\",\n    read_options: Union[ReadOptions, None] = None,\n    projection_pushdown: bool = True,\n) -&gt; Union[pl.LazyFrame, pl.DataFrame, \"pd.DataFrame\", datafusion.DataFrame]:\n    \"\"\"\n    Calculate intervals coverage.\n    Bioframe inspired API.\n\n    The coordinate system (0-based or 1-based) is automatically detected from\n    DataFrame metadata set at I/O time. Both inputs must have the same coordinate\n    system.\n\n    Parameters:\n        df1: Can be a path to a file, a polars DataFrame, or a pandas DataFrame or a registered table (see [register_vcf](api.md#polars_bio.register_vcf)). CSV with a header, BED and Parquet are supported.\n        df2: Can be a path to a file, a polars DataFrame, or a pandas DataFrame or a registered table. CSV with a header, BED  and Parquet are supported.\n        cols1: The names of columns containing the chromosome, start and end of the\n            genomic intervals, provided separately for each set.\n        cols2:  The names of columns containing the chromosome, start and end of the\n            genomic intervals, provided separately for each set.\n        suffixes: Suffixes for the columns of the two overlapped sets.\n        on_cols: List of additional column names to join on. default is None.\n        output_type: Type of the output. default is \"polars.LazyFrame\", \"polars.DataFrame\", or \"pandas.DataFrame\" or \"datafusion.DataFrame\" are also supported.\n        read_options: Additional options for reading the input files.\n        projection_pushdown: Enable column projection pushdown to optimize query performance by only reading the necessary columns at the DataFusion level.\n\n    Returns:\n        **polars.LazyFrame** or polars.DataFrame or pandas.DataFrame of the overlapping intervals.\n\n    Raises:\n        MissingCoordinateSystemError: If either input lacks coordinate system metadata\n            and `datafusion.bio.coordinate_system_check` is \"true\" (default).\n        CoordinateSystemMismatchError: If inputs have different coordinate systems.\n\n    Note:\n        The default output format, i.e. [LazyFrame](https://docs.pola.rs/api/python/stable/reference/lazyframe/index.html), is recommended for large datasets as it supports output streaming and lazy evaluation.\n        This enables efficient processing of large datasets without loading the entire output dataset into memory.\n\n    Example:\n\n    Todo:\n        Support for on_cols.\n    \"\"\"\n\n    _validate_overlap_input(cols1, cols2, on_cols, suffixes, output_type)\n\n    # Get filter_op from DataFrame metadata\n    filter_op = _get_filter_op_from_metadata(df1, df2)\n\n    cols1 = DEFAULT_INTERVAL_COLUMNS if cols1 is None else cols1\n    cols2 = DEFAULT_INTERVAL_COLUMNS if cols2 is None else cols2\n    range_options = RangeOptions(\n        range_op=RangeOp.Coverage,\n        filter_op=filter_op,\n        suffixes=suffixes,\n        columns_1=cols1,\n        columns_2=cols2,\n    )\n    return range_operation(\n        df2,\n        df1,\n        range_options,\n        output_type,\n        ctx,\n        read_options,\n        projection_pushdown=projection_pushdown,\n    )\n</code></pre>"},{"location":"api/#polars_bio.range_operations.merge","title":"<code>merge(df, min_dist=0, cols=['chrom', 'start', 'end'], on_cols=None, output_type='polars.LazyFrame', projection_pushdown=True)</code>  <code>staticmethod</code>","text":"<p>Merge overlapping intervals. It is assumed that start &lt; end.</p> <p>The coordinate system (0-based or 1-based) is automatically detected from DataFrame metadata set at I/O time.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Union[str, DataFrame, LazyFrame, 'pd.DataFrame']</code> <p>Can be a path to a file, a polars DataFrame, or a pandas DataFrame. CSV with a header, BED  and Parquet are supported.</p> required <code>min_dist</code> <code>int</code> <p>Minimum distance (integer) between intervals to merge. Default is 0.</p> <code>0</code> <code>cols</code> <code>Union[list[str], None]</code> <p>The names of columns containing the chromosome, start and end of the genomic intervals, provided separately for each set.</p> <code>['chrom', 'start', 'end']</code> <code>on_cols</code> <code>Union[list[str], None]</code> <p>List of additional column names for clustering. default is None.</p> <code>None</code> <code>output_type</code> <code>str</code> <p>Type of the output. default is \"polars.LazyFrame\", \"polars.DataFrame\", or \"pandas.DataFrame\" or \"datafusion.DataFrame\" are also supported.</p> <code>'polars.LazyFrame'</code> <code>projection_pushdown</code> <code>bool</code> <p>Enable column projection pushdown to optimize query performance by only reading the necessary columns at the DataFusion level.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[LazyFrame, DataFrame, 'pd.DataFrame', DataFrame]</code> <p>polars.LazyFrame or polars.DataFrame or pandas.DataFrame of the overlapping intervals.</p> <p>Raises:</p> Type Description <code>MissingCoordinateSystemError</code> <p>If input lacks coordinate system metadata and <code>datafusion.bio.coordinate_system_check</code> is \"true\" (default).</p> <p>Example:</p> Todo <p>Support for on_cols.</p> Source code in <code>polars_bio/range_op.py</code> <pre><code>@staticmethod\ndef merge(\n    df: Union[str, pl.DataFrame, pl.LazyFrame, \"pd.DataFrame\"],\n    min_dist: int = 0,\n    cols: Union[list[str], None] = [\"chrom\", \"start\", \"end\"],\n    on_cols: Union[list[str], None] = None,\n    output_type: str = \"polars.LazyFrame\",\n    projection_pushdown: bool = True,\n) -&gt; Union[pl.LazyFrame, pl.DataFrame, \"pd.DataFrame\", datafusion.DataFrame]:\n    \"\"\"\n    Merge overlapping intervals. It is assumed that start &lt; end.\n\n    The coordinate system (0-based or 1-based) is automatically detected from\n    DataFrame metadata set at I/O time.\n\n    Parameters:\n        df: Can be a path to a file, a polars DataFrame, or a pandas DataFrame. CSV with a header, BED  and Parquet are supported.\n        min_dist: Minimum distance (integer) between intervals to merge. Default is 0.\n        cols: The names of columns containing the chromosome, start and end of the\n            genomic intervals, provided separately for each set.\n        on_cols: List of additional column names for clustering. default is None.\n        output_type: Type of the output. default is \"polars.LazyFrame\", \"polars.DataFrame\", or \"pandas.DataFrame\" or \"datafusion.DataFrame\" are also supported.\n        projection_pushdown: Enable column projection pushdown to optimize query performance by only reading the necessary columns at the DataFusion level.\n\n    Returns:\n        **polars.LazyFrame** or polars.DataFrame or pandas.DataFrame of the overlapping intervals.\n\n    Raises:\n        MissingCoordinateSystemError: If input lacks coordinate system metadata\n            and `datafusion.bio.coordinate_system_check` is \"true\" (default).\n\n    Example:\n\n    Todo:\n        Support for on_cols.\n    \"\"\"\n    suffixes = (\"_1\", \"_2\")\n    _validate_overlap_input(cols, cols, on_cols, suffixes, output_type)\n\n    # Get filter_op from DataFrame metadata\n    filter_op = _get_filter_op_from_metadata_single(df)\n\n    cols = DEFAULT_INTERVAL_COLUMNS if cols is None else cols\n    range_options = RangeOptions(\n        range_op=RangeOp.Merge,\n        filter_op=filter_op,\n        columns_1=cols,\n        columns_2=cols,\n        min_dist=min_dist,\n    )\n\n    return range_operation(\n        df,\n        df,\n        range_options,\n        output_type,\n        ctx,\n        projection_pushdown=projection_pushdown,\n    )\n</code></pre>"},{"location":"api/#polars_bio.range_operations.nearest","title":"<code>nearest(df1, df2, suffixes=('_1', '_2'), on_cols=None, cols1=['chrom', 'start', 'end'], cols2=['chrom', 'start', 'end'], k=1, overlap=True, distance=True, output_type='polars.LazyFrame', read_options=None, projection_pushdown=True)</code>  <code>staticmethod</code>","text":"<p>Find pairs of closest genomic intervals. Bioframe inspired API.</p> <p>The coordinate system (0-based or 1-based) is automatically detected from DataFrame metadata set at I/O time. Both inputs must have the same coordinate system.</p> <p>Parameters:</p> Name Type Description Default <code>df1</code> <code>Union[str, DataFrame, LazyFrame, 'pd.DataFrame']</code> <p>Can be a path to a file, a polars DataFrame, or a pandas DataFrame or a registered table (see register_vcf). CSV with a header, BED and Parquet are supported.</p> required <code>df2</code> <code>Union[str, DataFrame, LazyFrame, 'pd.DataFrame']</code> <p>Can be a path to a file, a polars DataFrame, or a pandas DataFrame or a registered table. CSV with a header, BED  and Parquet are supported.</p> required <code>cols1</code> <code>Union[list[str], None]</code> <p>The names of columns containing the chromosome, start and end of the genomic intervals, provided separately for each set.</p> <code>['chrom', 'start', 'end']</code> <code>cols2</code> <code>Union[list[str], None]</code> <p>The names of columns containing the chromosome, start and end of the genomic intervals, provided separately for each set.</p> <code>['chrom', 'start', 'end']</code> <code>suffixes</code> <code>tuple[str, str]</code> <p>Suffixes for the columns of the two overlapped sets.</p> <code>('_1', '_2')</code> <code>on_cols</code> <code>Union[list[str], None]</code> <p>List of additional column names to join on. default is None.</p> <code>None</code> <code>k</code> <code>int</code> <p>Number of nearest neighbors to return per query interval. Default is 1.</p> <code>1</code> <code>overlap</code> <code>bool</code> <p>If True (default), include overlapping intervals in results. If False, only return non-overlapping nearest neighbors.</p> <code>True</code> <code>distance</code> <code>bool</code> <p>If True (default), include a <code>distance</code> column in the output. If False, omit it.</p> <code>True</code> <code>output_type</code> <code>str</code> <p>Type of the output. default is \"polars.LazyFrame\", \"polars.DataFrame\", or \"pandas.DataFrame\" or \"datafusion.DataFrame\" are also supported.</p> <code>'polars.LazyFrame'</code> <code>read_options</code> <code>Union[ReadOptions, None]</code> <p>Additional options for reading the input files.</p> <code>None</code> <code>projection_pushdown</code> <code>bool</code> <p>Enable column projection pushdown to optimize query performance by only reading the necessary columns at the DataFusion level.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[LazyFrame, DataFrame, 'pd.DataFrame', DataFrame]</code> <p>polars.LazyFrame or polars.DataFrame or pandas.DataFrame of the overlapping intervals.</p> <p>Raises:</p> Type Description <code>MissingCoordinateSystemError</code> <p>If either input lacks coordinate system metadata and <code>datafusion.bio.coordinate_system_check</code> is \"true\" (default).</p> <code>CoordinateSystemMismatchError</code> <p>If inputs have different coordinate systems.</p> Note <p>The default output format, i.e. LazyFrame, is recommended for large datasets as it supports output streaming and lazy evaluation. This enables efficient processing of large datasets without loading the entire output dataset into memory.</p> <p>Example:</p> Todo <p>Support for on_cols.</p> Source code in <code>polars_bio/range_op.py</code> <pre><code>@staticmethod\ndef nearest(\n    df1: Union[str, pl.DataFrame, pl.LazyFrame, \"pd.DataFrame\"],\n    df2: Union[str, pl.DataFrame, pl.LazyFrame, \"pd.DataFrame\"],\n    suffixes: tuple[str, str] = (\"_1\", \"_2\"),\n    on_cols: Union[list[str], None] = None,\n    cols1: Union[list[str], None] = [\"chrom\", \"start\", \"end\"],\n    cols2: Union[list[str], None] = [\"chrom\", \"start\", \"end\"],\n    k: int = 1,\n    overlap: bool = True,\n    distance: bool = True,\n    output_type: str = \"polars.LazyFrame\",\n    read_options: Union[ReadOptions, None] = None,\n    projection_pushdown: bool = True,\n) -&gt; Union[pl.LazyFrame, pl.DataFrame, \"pd.DataFrame\", datafusion.DataFrame]:\n    \"\"\"\n    Find pairs of closest genomic intervals.\n    Bioframe inspired API.\n\n    The coordinate system (0-based or 1-based) is automatically detected from\n    DataFrame metadata set at I/O time. Both inputs must have the same coordinate\n    system.\n\n    Parameters:\n        df1: Can be a path to a file, a polars DataFrame, or a pandas DataFrame or a registered table (see [register_vcf](api.md#polars_bio.register_vcf)). CSV with a header, BED and Parquet are supported.\n        df2: Can be a path to a file, a polars DataFrame, or a pandas DataFrame or a registered table. CSV with a header, BED  and Parquet are supported.\n        cols1: The names of columns containing the chromosome, start and end of the\n            genomic intervals, provided separately for each set.\n        cols2:  The names of columns containing the chromosome, start and end of the\n            genomic intervals, provided separately for each set.\n        suffixes: Suffixes for the columns of the two overlapped sets.\n        on_cols: List of additional column names to join on. default is None.\n        k: Number of nearest neighbors to return per query interval. Default is 1.\n        overlap: If True (default), include overlapping intervals in results. If False, only return non-overlapping nearest neighbors.\n        distance: If True (default), include a `distance` column in the output. If False, omit it.\n        output_type: Type of the output. default is \"polars.LazyFrame\", \"polars.DataFrame\", or \"pandas.DataFrame\" or \"datafusion.DataFrame\" are also supported.\n        read_options: Additional options for reading the input files.\n        projection_pushdown: Enable column projection pushdown to optimize query performance by only reading the necessary columns at the DataFusion level.\n\n    Returns:\n        **polars.LazyFrame** or polars.DataFrame or pandas.DataFrame of the overlapping intervals.\n\n    Raises:\n        MissingCoordinateSystemError: If either input lacks coordinate system metadata\n            and `datafusion.bio.coordinate_system_check` is \"true\" (default).\n        CoordinateSystemMismatchError: If inputs have different coordinate systems.\n\n    Note:\n        The default output format, i.e. [LazyFrame](https://docs.pola.rs/api/python/stable/reference/lazyframe/index.html), is recommended for large datasets as it supports output streaming and lazy evaluation.\n        This enables efficient processing of large datasets without loading the entire output dataset into memory.\n\n    Example:\n\n    Todo:\n        Support for on_cols.\n    \"\"\"\n\n    _validate_overlap_input(cols1, cols2, on_cols, suffixes, output_type)\n\n    # Get filter_op from DataFrame metadata\n    filter_op = _get_filter_op_from_metadata(df1, df2)\n\n    cols1 = DEFAULT_INTERVAL_COLUMNS if cols1 is None else cols1\n    cols2 = DEFAULT_INTERVAL_COLUMNS if cols2 is None else cols2\n    range_options = RangeOptions(\n        range_op=RangeOp.Nearest,\n        filter_op=filter_op,\n        suffixes=suffixes,\n        columns_1=cols1,\n        columns_2=cols2,\n        nearest_k=k,\n        include_overlaps=overlap,\n        compute_distance=distance,\n    )\n    return range_operation(\n        df1,\n        df2,\n        range_options,\n        output_type,\n        ctx,\n        read_options,\n        projection_pushdown=projection_pushdown,\n    )\n</code></pre>"},{"location":"api/#polars_bio.range_operations.overlap","title":"<code>overlap(df1, df2, suffixes=('_1', '_2'), on_cols=None, cols1=['chrom', 'start', 'end'], cols2=['chrom', 'start', 'end'], algorithm='Coitrees', low_memory=False, output_type='polars.LazyFrame', read_options1=None, read_options2=None, projection_pushdown=True)</code>  <code>staticmethod</code>","text":"<p>Find pairs of overlapping genomic intervals. Bioframe inspired API.</p> <p>The coordinate system (0-based or 1-based) is automatically detected from DataFrame metadata set at I/O time. Both inputs must have the same coordinate system.</p> <p>Parameters:</p> Name Type Description Default <code>df1</code> <code>Union[str, DataFrame, LazyFrame, 'pd.DataFrame']</code> <p>Can be a path to a file, a polars DataFrame, or a pandas DataFrame or a registered table (see register_vcf). CSV with a header, BED and Parquet are supported.</p> required <code>df2</code> <code>Union[str, DataFrame, LazyFrame, 'pd.DataFrame']</code> <p>Can be a path to a file, a polars DataFrame, or a pandas DataFrame or a registered table. CSV with a header, BED  and Parquet are supported.</p> required <code>cols1</code> <code>Union[list[str], None]</code> <p>The names of columns containing the chromosome, start and end of the genomic intervals, provided separately for each set.</p> <code>['chrom', 'start', 'end']</code> <code>cols2</code> <code>Union[list[str], None]</code> <p>The names of columns containing the chromosome, start and end of the genomic intervals, provided separately for each set.</p> <code>['chrom', 'start', 'end']</code> <code>suffixes</code> <code>tuple[str, str]</code> <p>Suffixes for the columns of the two overlapped sets.</p> <code>('_1', '_2')</code> <code>on_cols</code> <code>Union[list[str], None]</code> <p>List of additional column names to join on. default is None.</p> <code>None</code> <code>algorithm</code> <code>str</code> <p>The algorithm to use for the overlap operation. Available options: Coitrees, IntervalTree, ArrayIntervalTree, Lapper, SuperIntervals</p> <code>'Coitrees'</code> <code>low_memory</code> <code>bool</code> <p>If True, use low memory method for output generation. This caps the output batch size, trading some performance for significantly lower peak memory consumption. Recommended for operations that produce very large result sets.</p> <code>False</code> <code>output_type</code> <code>str</code> <p>Type of the output. default is \"polars.LazyFrame\", \"polars.DataFrame\", or \"pandas.DataFrame\" or \"datafusion.DataFrame\" are also supported.</p> <code>'polars.LazyFrame'</code> <code>read_options1</code> <code>Union[ReadOptions, None]</code> <p>Additional options for reading the input files.</p> <code>None</code> <code>read_options2</code> <code>Union[ReadOptions, None]</code> <p>Additional options for reading the input files.</p> <code>None</code> <code>projection_pushdown</code> <code>bool</code> <p>Enable column projection pushdown to optimize query performance by only reading the necessary columns at the DataFusion level.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[LazyFrame, DataFrame, 'pd.DataFrame', DataFrame]</code> <p>polars.LazyFrame or polars.DataFrame or pandas.DataFrame of the overlapping intervals.</p> <p>Raises:</p> Type Description <code>MissingCoordinateSystemError</code> <p>If either input lacks coordinate system metadata and <code>datafusion.bio.coordinate_system_check</code> is \"true\" (default). Use polars-bio I/O functions (scan_, read_) which automatically set metadata, or set it manually on Polars DataFrames via <code>df.config_meta.set(coordinate_system_zero_based=True/False)</code> or on Pandas DataFrames via <code>df.attrs[\"coordinate_system_zero_based\"] = True/False</code>. Set <code>pb.set_option(\"datafusion.bio.coordinate_system_check\", False)</code> to disable strict checking and fall back to global coordinate system setting.</p> <code>CoordinateSystemMismatchError</code> <p>If inputs have different coordinate systems.</p> Note <ol> <li>The default output format, i.e.  LazyFrame, is recommended for large datasets as it supports output streaming and lazy evaluation. This enables efficient processing of large datasets without loading the entire output dataset into memory.</li> <li>Streaming is only supported for polars.LazyFrame output.</li> </ol> Example <pre><code>import polars_bio as pb\nimport pandas as pd\n\ndf1 = pd.DataFrame([\n    ['chr1', 1, 5],\n    ['chr1', 3, 8],\n    ['chr1', 8, 10],\n    ['chr1', 12, 14]],\ncolumns=['chrom', 'start', 'end']\n)\ndf1.attrs[\"coordinate_system_zero_based\"] = False  # 1-based coordinates\n\ndf2 = pd.DataFrame(\n[['chr1', 4, 8],\n ['chr1', 10, 11]],\ncolumns=['chrom', 'start', 'end' ]\n)\ndf2.attrs[\"coordinate_system_zero_based\"] = False  # 1-based coordinates\n\noverlapping_intervals = pb.overlap(df1, df2, output_type=\"pandas.DataFrame\")\n\noverlapping_intervals\n    chrom_1         start_1     end_1 chrom_2       start_2  end_2\n0     chr1            1          5     chr1            4          8\n1     chr1            3          8     chr1            4          8\n</code></pre> Todo <p>Support for on_cols.</p> Source code in <code>polars_bio/range_op.py</code> <pre><code>@staticmethod\ndef overlap(\n    df1: Union[str, pl.DataFrame, pl.LazyFrame, \"pd.DataFrame\"],\n    df2: Union[str, pl.DataFrame, pl.LazyFrame, \"pd.DataFrame\"],\n    suffixes: tuple[str, str] = (\"_1\", \"_2\"),\n    on_cols: Union[list[str], None] = None,\n    cols1: Union[list[str], None] = [\"chrom\", \"start\", \"end\"],\n    cols2: Union[list[str], None] = [\"chrom\", \"start\", \"end\"],\n    algorithm: str = \"Coitrees\",\n    low_memory: bool = False,\n    output_type: str = \"polars.LazyFrame\",\n    read_options1: Union[ReadOptions, None] = None,\n    read_options2: Union[ReadOptions, None] = None,\n    projection_pushdown: bool = True,\n) -&gt; Union[pl.LazyFrame, pl.DataFrame, \"pd.DataFrame\", datafusion.DataFrame]:\n    \"\"\"\n    Find pairs of overlapping genomic intervals.\n    Bioframe inspired API.\n\n    The coordinate system (0-based or 1-based) is automatically detected from\n    DataFrame metadata set at I/O time. Both inputs must have the same coordinate\n    system.\n\n    Parameters:\n        df1: Can be a path to a file, a polars DataFrame, or a pandas DataFrame or a registered table (see [register_vcf](api.md#polars_bio.register_vcf)). CSV with a header, BED and Parquet are supported.\n        df2: Can be a path to a file, a polars DataFrame, or a pandas DataFrame or a registered table. CSV with a header, BED  and Parquet are supported.\n        cols1: The names of columns containing the chromosome, start and end of the\n            genomic intervals, provided separately for each set.\n        cols2:  The names of columns containing the chromosome, start and end of the\n            genomic intervals, provided separately for each set.\n        suffixes: Suffixes for the columns of the two overlapped sets.\n        on_cols: List of additional column names to join on. default is None.\n        algorithm: The algorithm to use for the overlap operation. Available options: Coitrees, IntervalTree, ArrayIntervalTree, Lapper, SuperIntervals\n        low_memory: If True, use low memory method for output generation. This caps the output batch size, trading some performance for significantly lower peak memory consumption. Recommended for operations that produce very large result sets.\n        output_type: Type of the output. default is \"polars.LazyFrame\", \"polars.DataFrame\", or \"pandas.DataFrame\" or \"datafusion.DataFrame\" are also supported.\n        read_options1: Additional options for reading the input files.\n        read_options2: Additional options for reading the input files.\n        projection_pushdown: Enable column projection pushdown to optimize query performance by only reading the necessary columns at the DataFusion level.\n\n    Returns:\n        **polars.LazyFrame** or polars.DataFrame or pandas.DataFrame of the overlapping intervals.\n\n    Raises:\n        MissingCoordinateSystemError: If either input lacks coordinate system metadata\n            and `datafusion.bio.coordinate_system_check` is \"true\" (default). Use polars-bio\n            I/O functions (scan_*, read_*) which automatically set metadata, or set it manually\n            on Polars DataFrames via `df.config_meta.set(coordinate_system_zero_based=True/False)`\n            or on Pandas DataFrames via `df.attrs[\"coordinate_system_zero_based\"] = True/False`.\n            Set `pb.set_option(\"datafusion.bio.coordinate_system_check\", False)` to disable\n            strict checking and fall back to global coordinate system setting.\n        CoordinateSystemMismatchError: If inputs have different coordinate systems.\n\n    Note:\n        1. The default output format, i.e.  [LazyFrame](https://docs.pola.rs/api/python/stable/reference/lazyframe/index.html), is recommended for large datasets as it supports output streaming and lazy evaluation.\n        This enables efficient processing of large datasets without loading the entire output dataset into memory.\n        2. Streaming is only supported for polars.LazyFrame output.\n\n    Example:\n        ```python\n        import polars_bio as pb\n        import pandas as pd\n\n        df1 = pd.DataFrame([\n            ['chr1', 1, 5],\n            ['chr1', 3, 8],\n            ['chr1', 8, 10],\n            ['chr1', 12, 14]],\n        columns=['chrom', 'start', 'end']\n        )\n        df1.attrs[\"coordinate_system_zero_based\"] = False  # 1-based coordinates\n\n        df2 = pd.DataFrame(\n        [['chr1', 4, 8],\n         ['chr1', 10, 11]],\n        columns=['chrom', 'start', 'end' ]\n        )\n        df2.attrs[\"coordinate_system_zero_based\"] = False  # 1-based coordinates\n\n        overlapping_intervals = pb.overlap(df1, df2, output_type=\"pandas.DataFrame\")\n\n        overlapping_intervals\n            chrom_1         start_1     end_1 chrom_2       start_2  end_2\n        0     chr1            1          5     chr1            4          8\n        1     chr1            3          8     chr1            4          8\n\n        ```\n\n    Todo:\n         Support for on_cols.\n    \"\"\"\n\n    _validate_overlap_input(cols1, cols2, on_cols, suffixes, output_type)\n\n    # Get filter_op from DataFrame metadata\n    filter_op = _get_filter_op_from_metadata(df1, df2)\n\n    cols1 = DEFAULT_INTERVAL_COLUMNS if cols1 is None else cols1\n    cols2 = DEFAULT_INTERVAL_COLUMNS if cols2 is None else cols2\n    range_options = RangeOptions(\n        range_op=RangeOp.Overlap,\n        filter_op=filter_op,\n        suffixes=suffixes,\n        columns_1=cols1,\n        columns_2=cols2,\n        overlap_alg=algorithm,\n        overlap_low_memory=low_memory,\n    )\n\n    return range_operation(\n        df1,\n        df2,\n        range_options,\n        output_type,\n        ctx,\n        read_options1,\n        read_options2,\n        projection_pushdown,\n    )\n</code></pre>"},{"location":"api/#polars_bio.range_operations.subtract","title":"<code>subtract(df1, df2, cols1=['chrom', 'start', 'end'], cols2=['chrom', 'start', 'end'], output_type='polars.LazyFrame', projection_pushdown=True)</code>  <code>staticmethod</code>","text":"<p>Subtract the second set of intervals from the first.</p> <p>For each interval in <code>df1</code>, removes any portion that overlaps with intervals in <code>df2</code>. The result contains the remaining fragments.</p> <p>Bioframe inspired API.</p> <p>The coordinate system (0-based or 1-based) is automatically detected from DataFrame metadata set at I/O time. Both inputs must have the same coordinate system.</p> <p>Parameters:</p> Name Type Description Default <code>df1</code> <code>Union[str, DataFrame, LazyFrame, 'pd.DataFrame']</code> <p>Can be a path to a file, a polars DataFrame, or a pandas DataFrame. CSV with a header, BED and Parquet are supported.</p> required <code>df2</code> <code>Union[str, DataFrame, LazyFrame, 'pd.DataFrame']</code> <p>Can be a path to a file, a polars DataFrame, or a pandas DataFrame. CSV with a header, BED and Parquet are supported.</p> required <code>cols1</code> <code>Union[list[str], None]</code> <p>The names of columns containing the chromosome, start and end of the genomic intervals for the first set.</p> <code>['chrom', 'start', 'end']</code> <code>cols2</code> <code>Union[list[str], None]</code> <p>The names of columns containing the chromosome, start and end of the genomic intervals for the second set.</p> <code>['chrom', 'start', 'end']</code> <code>output_type</code> <code>str</code> <p>Type of the output. default is \"polars.LazyFrame\", \"polars.DataFrame\", or \"pandas.DataFrame\" or \"datafusion.DataFrame\" are also supported.</p> <code>'polars.LazyFrame'</code> <code>projection_pushdown</code> <code>bool</code> <p>Enable column projection pushdown.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[LazyFrame, DataFrame, 'pd.DataFrame', DataFrame]</code> <p>polars.LazyFrame or polars.DataFrame or pandas.DataFrame of the</p> <code>Union[LazyFrame, DataFrame, 'pd.DataFrame', DataFrame]</code> <p>remaining interval fragments (contig, start, end).</p> <p>Raises:</p> Type Description <code>MissingCoordinateSystemError</code> <p>If either input lacks coordinate system metadata and <code>datafusion.bio.coordinate_system_check</code> is \"true\" (default).</p> <code>CoordinateSystemMismatchError</code> <p>If inputs have different coordinate systems.</p> Source code in <code>polars_bio/range_op.py</code> <pre><code>@staticmethod\ndef subtract(\n    df1: Union[str, pl.DataFrame, pl.LazyFrame, \"pd.DataFrame\"],\n    df2: Union[str, pl.DataFrame, pl.LazyFrame, \"pd.DataFrame\"],\n    cols1: Union[list[str], None] = [\"chrom\", \"start\", \"end\"],\n    cols2: Union[list[str], None] = [\"chrom\", \"start\", \"end\"],\n    output_type: str = \"polars.LazyFrame\",\n    projection_pushdown: bool = True,\n) -&gt; Union[pl.LazyFrame, pl.DataFrame, \"pd.DataFrame\", datafusion.DataFrame]:\n    \"\"\"\n    Subtract the second set of intervals from the first.\n\n    For each interval in ``df1``, removes any portion that overlaps with\n    intervals in ``df2``. The result contains the remaining fragments.\n\n    Bioframe inspired API.\n\n    The coordinate system (0-based or 1-based) is automatically detected from\n    DataFrame metadata set at I/O time. Both inputs must have the same coordinate\n    system.\n\n    Parameters:\n        df1: Can be a path to a file, a polars DataFrame, or a pandas DataFrame. CSV with a header, BED and Parquet are supported.\n        df2: Can be a path to a file, a polars DataFrame, or a pandas DataFrame. CSV with a header, BED and Parquet are supported.\n        cols1: The names of columns containing the chromosome, start and end of the\n            genomic intervals for the first set.\n        cols2: The names of columns containing the chromosome, start and end of the\n            genomic intervals for the second set.\n        output_type: Type of the output. default is \"polars.LazyFrame\", \"polars.DataFrame\", or \"pandas.DataFrame\" or \"datafusion.DataFrame\" are also supported.\n        projection_pushdown: Enable column projection pushdown.\n\n    Returns:\n        **polars.LazyFrame** or polars.DataFrame or pandas.DataFrame of the\n        remaining interval fragments (contig, start, end).\n\n    Raises:\n        MissingCoordinateSystemError: If either input lacks coordinate system metadata\n            and ``datafusion.bio.coordinate_system_check`` is \"true\" (default).\n        CoordinateSystemMismatchError: If inputs have different coordinate systems.\n    \"\"\"\n    suffixes = (\"_1\", \"_2\")\n    _validate_overlap_input(cols1, cols2, None, suffixes, output_type)\n\n    filter_op = _get_filter_op_from_metadata(df1, df2)\n\n    cols1 = DEFAULT_INTERVAL_COLUMNS if cols1 is None else cols1\n    cols2 = DEFAULT_INTERVAL_COLUMNS if cols2 is None else cols2\n    range_options = RangeOptions(\n        range_op=RangeOp.Subtract,\n        filter_op=filter_op,\n        columns_1=cols1,\n        columns_2=cols2,\n    )\n\n    return range_operation(\n        df1,\n        df2,\n        range_options,\n        output_type,\n        ctx,\n        projection_pushdown=projection_pushdown,\n    )\n</code></pre>"},{"location":"api/#polars_bio.get_metadata","title":"<code>get_metadata(df)</code>","text":"<p>Get all metadata attached to a DataFrame or LazyFrame.</p> <p>Returns all metadata including: - Source file information (format, path) - Format-specific metadata (VCF INFO/FORMAT fields, FASTQ quality encoding, etc.) - Comprehensive Arrow schema metadata (if available)</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <p>Polars DataFrame or LazyFrame (or Pandas DataFrame)</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dict with keys:</p> <code>dict</code> <ul> <li>\"format\": File format identifier (e.g., \"vcf\", \"fastq\", \"bam\")</li> </ul> <code>dict</code> <ul> <li>\"path\": Original file path</li> </ul> <code>dict</code> <ul> <li>\"coordinate_system_zero_based\": Boolean indicating coordinate system (True=0-based, False=1-based, None=not set)</li> </ul> <code>dict</code> <ul> <li>\"header\": Format-specific header data as dict, may include:</li> <li>For VCF: \"info_fields\", \"format_fields\", \"sample_names\", \"version\", \"contigs\", \"filters\", etc.</li> <li>For FASTQ: quality encoding information</li> <li>For other formats: format-specific metadata</li> <li>\"_datafusion_table_name\": Internal DataFusion table name (for debugging)</li> </ul> <p>Examples:</p> <p>Get all metadata from a VCF file: </p><pre><code>import polars_bio as pb\nlf = pb.scan_vcf(\"file.vcf\")\nmeta = pb.get_metadata(lf)\n</code></pre><p></p> <p>Access basic metadata: </p><pre><code>meta[\"format\"]                        # Returns: 'vcf'\nmeta[\"path\"]                          # Returns: 'file.vcf'\nmeta[\"coordinate_system_zero_based\"]  # Returns: False (1-based for VCF)\n</code></pre><p></p> <p>Access VCF-specific metadata: </p><pre><code>info_fields = meta[\"header\"][\"info_fields\"]\nformat_fields = meta[\"header\"][\"format_fields\"]\nsample_names = meta[\"header\"][\"sample_names\"]\nversion = meta[\"header\"][\"version\"]\ncontigs = meta[\"header\"][\"contigs\"]\n</code></pre><p></p> Source code in <code>polars_bio/_metadata.py</code> <pre><code>def get_metadata(df) -&gt; dict:\n    \"\"\"Get all metadata attached to a DataFrame or LazyFrame.\n\n    Returns all metadata including:\n    - Source file information (format, path)\n    - Format-specific metadata (VCF INFO/FORMAT fields, FASTQ quality encoding, etc.)\n    - Comprehensive Arrow schema metadata (if available)\n\n    Args:\n        df: Polars DataFrame or LazyFrame (or Pandas DataFrame)\n\n    Returns:\n        Dict with keys:\n        - \"format\": File format identifier (e.g., \"vcf\", \"fastq\", \"bam\")\n        - \"path\": Original file path\n        - \"coordinate_system_zero_based\": Boolean indicating coordinate system (True=0-based, False=1-based, None=not set)\n        - \"header\": Format-specific header data as dict, may include:\n            - For VCF: \"info_fields\", \"format_fields\", \"sample_names\", \"version\", \"contigs\", \"filters\", etc.\n            - For FASTQ: quality encoding information\n            - For other formats: format-specific metadata\n            - \"_datafusion_table_name\": Internal DataFusion table name (for debugging)\n\n    Examples:\n        Get all metadata from a VCF file:\n        ```python\n        import polars_bio as pb\n        lf = pb.scan_vcf(\"file.vcf\")\n        meta = pb.get_metadata(lf)\n        ```\n\n        Access basic metadata:\n        ```python\n        meta[\"format\"]                        # Returns: 'vcf'\n        meta[\"path\"]                          # Returns: 'file.vcf'\n        meta[\"coordinate_system_zero_based\"]  # Returns: False (1-based for VCF)\n        ```\n\n        Access VCF-specific metadata:\n        ```python\n        info_fields = meta[\"header\"][\"info_fields\"]\n        format_fields = meta[\"header\"][\"format_fields\"]\n        sample_names = meta[\"header\"][\"sample_names\"]\n        version = meta[\"header\"][\"version\"]\n        contigs = meta[\"header\"][\"contigs\"]\n        ```\n    \"\"\"\n    result = {\n        \"format\": None,\n        \"path\": None,\n        \"coordinate_system_zero_based\": None,\n        \"header\": None,\n    }\n\n    if _has_config_meta(df):\n        # Polars DataFrame/LazyFrame\n        try:\n            metadata = df.config_meta.get_metadata()\n        except (KeyError, AttributeError, TypeError):\n            return result\n\n        result[\"format\"] = metadata.get(SOURCE_FORMAT_KEY)\n        result[\"path\"] = metadata.get(SOURCE_PATH_KEY)\n        result[\"coordinate_system_zero_based\"] = metadata.get(COORDINATE_SYSTEM_KEY)\n\n        header_json = metadata.get(SOURCE_HEADER_KEY)\n        if header_json:\n            try:\n                result[\"header\"] = json.loads(header_json)\n            except (json.JSONDecodeError, TypeError):\n                pass\n\n    elif _is_pandas_dataframe(df):\n        # Pandas DataFrame\n        if hasattr(df, \"attrs\"):\n            result[\"format\"] = df.attrs.get(SOURCE_FORMAT_KEY)\n            result[\"path\"] = df.attrs.get(SOURCE_PATH_KEY)\n            result[\"coordinate_system_zero_based\"] = df.attrs.get(COORDINATE_SYSTEM_KEY)\n\n            header_json = df.attrs.get(SOURCE_HEADER_KEY)\n            if header_json:\n                try:\n                    result[\"header\"] = json.loads(header_json)\n                except (json.JSONDecodeError, TypeError):\n                    pass\n\n    return result\n</code></pre>"},{"location":"api/#polars_bio.get_option","title":"<code>get_option(key)</code>","text":"<p>Get the value of a configuration option.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <p>The configuration key.</p> required <p>Returns:</p> Type Description <p>The current value of the option as a string, or None if not set.</p> Example <pre><code>import polars_bio as pb\npb.get_option(\"datafusion.bio.coordinate_system_zero_based\")\n'true'\n</code></pre> Source code in <code>polars_bio/context.py</code> <pre><code>def get_option(key):\n    \"\"\"Get the value of a configuration option.\n\n    Args:\n        key: The configuration key.\n\n    Returns:\n        The current value of the option as a string, or None if not set.\n\n    Example:\n        ```python\n        import polars_bio as pb\n        pb.get_option(\"datafusion.bio.coordinate_system_zero_based\")\n        'true'\n        ```\n    \"\"\"\n    return Context().get_option(key)\n</code></pre>"},{"location":"api/#polars_bio.print_metadata_json","title":"<code>print_metadata_json(df, indent=2)</code>","text":"<p>Print metadata as pretty-formatted JSON.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Union[DataFrame, LazyFrame]</code> <p>Polars DataFrame or LazyFrame</p> required <code>indent</code> <code>int</code> <p>Number of spaces for indentation (default: 2)</p> <code>2</code> Example <pre><code>import polars_bio as pb\nlf = pb.scan_vcf(\"file.vcf\")\npb.print_metadata_json(lf)\n</code></pre> Source code in <code>polars_bio/_metadata.py</code> <pre><code>def print_metadata_json(df: Union[pl.DataFrame, pl.LazyFrame], indent: int = 2) -&gt; None:\n    \"\"\"Print metadata as pretty-formatted JSON.\n\n    Args:\n        df: Polars DataFrame or LazyFrame\n        indent: Number of spaces for indentation (default: 2)\n\n    Example:\n        ```python\n        import polars_bio as pb\n        lf = pb.scan_vcf(\"file.vcf\")\n        pb.print_metadata_json(lf)\n        ```\n    \"\"\"\n    meta = get_metadata(df)\n    print(json.dumps(meta, indent=indent, default=str))\n</code></pre>"},{"location":"api/#polars_bio.print_metadata_summary","title":"<code>print_metadata_summary(df)</code>","text":"<p>Print a human-readable summary of all metadata.</p> <p>Displays a formatted summary of all metadata attached to a DataFrame or LazyFrame, including format, path, coordinate system, and format-specific information.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Union[DataFrame, LazyFrame]</code> <p>Polars DataFrame or LazyFrame</p> required Example <pre><code>import polars_bio as pb\nlf = pb.scan_vcf(\"file.vcf\")\npb.print_metadata_summary(lf)\n</code></pre> Source code in <code>polars_bio/_metadata.py</code> <pre><code>def print_metadata_summary(df: Union[pl.DataFrame, pl.LazyFrame]) -&gt; None:\n    \"\"\"Print a human-readable summary of all metadata.\n\n    Displays a formatted summary of all metadata attached to a DataFrame or LazyFrame,\n    including format, path, coordinate system, and format-specific information.\n\n    Args:\n        df: Polars DataFrame or LazyFrame\n\n    Example:\n        ```python\n        import polars_bio as pb\n        lf = pb.scan_vcf(\"file.vcf\")\n        pb.print_metadata_summary(lf)\n        ```\n    \"\"\"\n    meta = get_metadata(df)\n    if not meta or not any([meta.get(\"format\"), meta.get(\"path\"), meta.get(\"header\")]):\n        print(\"No metadata available\")\n        return\n\n    print(\"=\" * 70)\n    print(\"Metadata Summary\")\n    print(\"=\" * 70)\n    print()\n\n    # Basic metadata\n    if meta.get(\"format\"):\n        print(f\"Format: {meta['format']}\")\n    if meta.get(\"path\"):\n        print(f\"Path: {meta['path']}\")\n    if meta.get(\"coordinate_system_zero_based\") is not None:\n        coord_sys = \"0-based\" if meta[\"coordinate_system_zero_based\"] else \"1-based\"\n        print(f\"Coordinate System: {coord_sys}\")\n\n    # Format-specific metadata\n    if meta.get(\"header\"):\n        header = meta[\"header\"]\n        print()\n        print(\"Format-specific metadata:\")\n        print(\"-\" * 70)\n\n        # VCF-specific\n        if meta.get(\"format\") == \"vcf\":\n            if \"version\" in header:\n                print(f\"  VCF Version: {header['version']}\")\n            if \"sample_names\" in header:\n                samples = header[\"sample_names\"]\n                print(f\"  Samples ({len(samples)}): {', '.join(samples[:5])}\")\n                if len(samples) &gt; 5:\n                    print(f\"    ... and {len(samples) - 5} more\")\n            if \"info_fields\" in header:\n                print(f\"  INFO fields: {len(header['info_fields'])}\")\n                for field_id in list(header[\"info_fields\"].keys())[:3]:\n                    field = header[\"info_fields\"][field_id]\n                    print(\n                        f\"    - {field_id}: {field.get('type')} ({field.get('description', 'No description')})\"\n                    )\n                if len(header[\"info_fields\"]) &gt; 3:\n                    print(f\"    ... and {len(header['info_fields']) - 3} more\")\n            if \"format_fields\" in header:\n                print(f\"  FORMAT fields: {len(header['format_fields'])}\")\n                for field_id in list(header[\"format_fields\"].keys())[:3]:\n                    field = header[\"format_fields\"][field_id]\n                    print(\n                        f\"    - {field_id}: {field.get('type')} ({field.get('description', 'No description')})\"\n                    )\n                if len(header[\"format_fields\"]) &gt; 3:\n                    print(f\"    ... and {len(header['format_fields']) - 3} more\")\n            if \"contigs\" in header and header[\"contigs\"]:\n                print(f\"  Contigs: {len(header['contigs'])}\")\n            if \"filters\" in header and header[\"filters\"]:\n                print(f\"  Filters: {len(header['filters'])}\")\n\n        # Other formats can be added here as needed\n\n    print()\n    print(\"=\" * 70)\n</code></pre>"},{"location":"api/#polars_bio.set_loglevel","title":"<code>set_loglevel(level)</code>","text":"<p>Set the log level for the logger and root logger.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>str</code> <p>The log level to set. Can be \"debug\", \"info\", \"warn\", or \"warning\".</p> required <p>Note</p> <p>The log level should be set as a first step after importing the library. Once set it can be only decreased, not increased. In order to increase the log level, you need to restart the Python session.</p> <p>Example: </p><pre><code>import polars_bio as pb\npb.set_loglevel(\"info\")\n</code></pre><p></p> Source code in <code>polars_bio/logging.py</code> <pre><code>def set_loglevel(level: str):\n    \"\"\"Set the log level for the logger and root logger.\n\n    Args:\n        level: The log level to set. Can be \"debug\", \"info\", \"warn\", or \"warning\".\n\n    !!! note\n        The log level should be set as a **first** step after importing the library.\n        Once set it can be only **decreased**, not increased. In order to increase\n        the log level, you need to restart the Python session.\n\n        Example:\n        ```python\n        import polars_bio as pb\n        pb.set_loglevel(\"info\")\n        ```\n    \"\"\"\n    level = level.lower()\n    if level == \"debug\":\n        logger.setLevel(logging.DEBUG)\n        root_logger.setLevel(logging.DEBUG)\n        logging.basicConfig(level=logging.DEBUG)\n    elif level == \"info\":\n        logger.setLevel(logging.INFO)\n        root_logger.setLevel(logging.INFO)\n        logging.basicConfig(level=logging.INFO)\n    elif level == \"warn\" or level == \"warning\":\n        logger.setLevel(logging.WARN)\n        root_logger.setLevel(logging.WARN)\n        logging.basicConfig(level=logging.WARN)\n    else:\n        raise ValueError(f\"{level} is not a valid log level\")\n</code></pre>"},{"location":"api/#polars_bio.set_option","title":"<code>set_option(key, value)</code>","text":"<p>Set a configuration option.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <p>The configuration key.</p> required <code>value</code> <p>The value to set (bool values are converted to \"true\"/\"false\").</p> required Example <pre><code>import polars_bio as pb\npb.set_option(\"datafusion.bio.coordinate_system_zero_based\", False)\n</code></pre> Source code in <code>polars_bio/context.py</code> <pre><code>def set_option(key, value):\n    \"\"\"Set a configuration option.\n\n    Args:\n        key: The configuration key.\n        value: The value to set (bool values are converted to \"true\"/\"false\").\n\n    Example:\n        ```python\n        import polars_bio as pb\n        pb.set_option(\"datafusion.bio.coordinate_system_zero_based\", False)\n        ```\n    \"\"\"\n    Context().set_option(key, value)\n</code></pre>"},{"location":"api/#polars_bio.set_source_metadata","title":"<code>set_source_metadata(df, format, path='', header=None)</code>","text":"<p>Set standardized source file metadata.</p> <p>Stores metadata about the source file format, path, and format-specific header information. This standardized approach works across all file formats (VCF, FASTQ, BAM, GFF, BED, FASTA, CRAM).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <p>Polars DataFrame or LazyFrame (or Pandas DataFrame)</p> required <code>format</code> <code>str</code> <p>File format identifier (e.g., \"vcf\", \"fastq\", \"bam\")</p> required <code>path</code> <code>str</code> <p>Original file path (default: \"\")</p> <code>''</code> <code>header</code> <code>dict</code> <p>Format-specific header data as dict (default: None)     For VCF: {\"info_fields\": {...}, \"format_fields\": {...}, \"sample_names\": [...], ...}     For other formats: format-specific metadata</p> <code>None</code> Example <pre><code>import polars_bio as pb\nlf = pb.scan_vcf(\"sample.vcf\")\nheader = {\"info_fields\": {...}, \"sample_names\": [\"sample1\"]}\npb.set_source_metadata(lf, format=\"vcf\", path=\"sample.vcf\", header=header)\n</code></pre> Source code in <code>polars_bio/_metadata.py</code> <pre><code>def set_source_metadata(df, format: str, path: str = \"\", header: dict = None):\n    \"\"\"Set standardized source file metadata.\n\n    Stores metadata about the source file format, path, and format-specific\n    header information. This standardized approach works across all file\n    formats (VCF, FASTQ, BAM, GFF, BED, FASTA, CRAM).\n\n    Args:\n        df: Polars DataFrame or LazyFrame (or Pandas DataFrame)\n        format: File format identifier (e.g., \"vcf\", \"fastq\", \"bam\")\n        path: Original file path (default: \"\")\n        header: Format-specific header data as dict (default: None)\n                For VCF: {\"info_fields\": {...}, \"format_fields\": {...}, \"sample_names\": [...], ...}\n                For other formats: format-specific metadata\n\n    Example:\n        ```python\n        import polars_bio as pb\n        lf = pb.scan_vcf(\"sample.vcf\")\n        header = {\"info_fields\": {...}, \"sample_names\": [\"sample1\"]}\n        pb.set_source_metadata(lf, format=\"vcf\", path=\"sample.vcf\", header=header)\n        ```\n    \"\"\"\n    if _has_config_meta(df):\n        # Polars DataFrame/LazyFrame\n        metadata_updates = {\n            SOURCE_FORMAT_KEY: format,\n            SOURCE_PATH_KEY: path,\n            SOURCE_HEADER_KEY: json.dumps(header) if header else \"\",\n        }\n        df.config_meta.set(**metadata_updates)\n    elif _is_pandas_dataframe(df):\n        # Pandas DataFrame\n        if not hasattr(df, \"attrs\"):\n            df.attrs = {}\n        df.attrs[SOURCE_FORMAT_KEY] = format\n        df.attrs[SOURCE_PATH_KEY] = path\n        df.attrs[SOURCE_HEADER_KEY] = json.dumps(header) if header else \"\"\n</code></pre>"},{"location":"contact/","title":"\ud83d\udce1 Contact","text":""},{"location":"contact/#contact-us","title":"Contact Us","text":"<p>You can drop us an email and join Discord to introduce yourself and ask questions.</p>"},{"location":"developers/","title":"\ud83d\udee0\ufe0f Developers guide","text":""},{"location":"developers/#architecture-overview","title":"Architecture Overview","text":"<p>polars-bio is a Python library for high-performance genomic data processing. It combines a Python API layer with a Rust-based execution engine powered by Apache DataFusion, extended with bioinformatics-specific capabilities through upstream <code>datafusion-bio-*</code> crates.</p> <pre><code>flowchart TB\n    subgraph Python[\"Python API Layer\"]\n        direction LR\n        io[\"I/O Operations&lt;br/&gt;(scan/read/write)\"]\n        range[\"Range Operations&lt;br/&gt;(overlap, nearest, ...)\"]\n        pileup[\"Pileup Operations&lt;br/&gt;(depth)\"]\n        sql[\"SQL API\"]\n    end\n\n    subgraph PyO3[\"PyO3 Rust Bindings\"]\n        direction LR\n        py_register[\"py_register_table()\"]\n        py_read[\"py_read_table() / py_read_sql()\"]\n        py_pileup[\"py_pileup_depth()\"]\n    end\n\n    subgraph DataFusion[\"Apache DataFusion Engine\"]\n        direction LR\n        ctx[\"SessionContext\"]\n        optimizer[\"Physical Optimizer\"]\n        exec[\"Execution Engine\"]\n    end\n\n    subgraph Upstream[\"Upstream Crates\"]\n        direction LR\n        formats[\"datafusion-bio-formats&lt;br/&gt;(9 sub-crates)\"]\n        functions[\"datafusion-bio-functions&lt;br/&gt;(ranges + pileup)\"]\n    end\n\n    subgraph Storage[\"Data Sources\"]\n        direction LR\n        local[\"Local Files\"]\n        cloud[\"Cloud Storage&lt;br/&gt;(S3, GCS, Azure)\"]\n    end\n\n    Python --&gt; PyO3\n    PyO3 --&gt; DataFusion\n    DataFusion --&gt; Upstream\n    Upstream --&gt; Storage</code></pre> <p>The key design principles are:</p> <ul> <li>Zero-copy data exchange between Python (Polars/Pandas) and Rust via the Arrow C Data Interface</li> <li>Predicate and projection pushdown through DataFusion's optimizer to minimize I/O</li> <li>Streaming execution for out-of-core processing of datasets larger than memory</li> <li>Parallel execution via DataFusion's partitioned execution model</li> </ul>"},{"location":"developers/#upstream-repositories","title":"Upstream Repositories","text":"<p>polars-bio relies on two upstream repository families that provide the DataFusion extensions for bioinformatics workloads. These are pinned as git dependencies in <code>Cargo.toml</code>.</p>"},{"location":"developers/#datafusion-bio-formats","title":"datafusion-bio-formats","text":"<p>Repository: github.com/biodatageeks/datafusion-bio-formats</p> <p>Provides DataFusion <code>TableProvider</code> implementations for bioinformatics file formats, using noodles for low-level format I/O.</p> Crate Format Features <code>datafusion-bio-format-bam</code> BAM Indexed reads (BAI/CSI), predicate &amp; projection pushdown, parallel partitioning <code>datafusion-bio-format-cram</code> CRAM Indexed reads (CRAI), predicate &amp; projection pushdown, reference-based compression <code>datafusion-bio-format-vcf</code> VCF Indexed reads (TBI/CSI), predicate &amp; projection pushdown, INFO/FORMAT field extraction <code>datafusion-bio-format-gff</code> GFF3 Indexed reads (TBI/CSI), predicate &amp; projection pushdown, attribute field extraction <code>datafusion-bio-format-bed</code> BED Single-threaded reads, limit pushdown <code>datafusion-bio-format-fastq</code> FASTQ GZI-indexed parallel BGZF decoding, limit pushdown <code>datafusion-bio-format-fasta</code> FASTA Single-threaded reads, limit pushdown <code>datafusion-bio-format-pairs</code> Pairs Indexed reads (TBI/CSI), predicate &amp; projection pushdown <code>datafusion-bio-format-core</code> Core Shared utilities, coordinate system metadata, OpenDAL cloud storage integration"},{"location":"developers/#datafusion-bio-functions","title":"datafusion-bio-functions","text":"<p>Repository: github.com/biodatageeks/datafusion-bio-functions</p> <p>Provides DataFusion extensions for genomic range operations and pileup computation.</p> Crate Purpose Mechanism <code>datafusion-bio-function-ranges</code> Overlap, nearest, coverage, count-overlaps <code>PhysicalOptimizerRule</code> for interval join rewriting + UDTF for coverage/count-overlaps <code>datafusion-bio-function-pileup</code> Per-base read depth (pileup) UDTF registered as <code>depth()</code> in SQL"},{"location":"developers/#dependency-pinning","title":"Dependency pinning","text":"<p>Both repositories are pinned to specific git revisions in <code>Cargo.toml</code>:</p> <pre><code># Format crates (all pinned to the same revision)\ndatafusion-bio-format-vcf = { git = \"https://github.com/biodatageeks/datafusion-bio-formats.git\", rev = \"...\" }\ndatafusion-bio-format-bam = { git = \"https://github.com/biodatageeks/datafusion-bio-formats.git\", rev = \"...\" }\n# ... (9 sub-crates total)\n\n# Function crates\ndatafusion-bio-function-ranges = { git = \"https://github.com/biodatageeks/datafusion-bio-functions.git\", rev = \"...\" }\ndatafusion-bio-function-pileup = { git = \"https://github.com/biodatageeks/datafusion-bio-functions.git\", rev = \"...\", default-features = false }\n</code></pre> <p>Note</p> <p><code>datafusion-bio-function-pileup</code> uses <code>default-features = false</code> to avoid a duplicate BAM dependency conflict with <code>datafusion-bio-format-bam</code>.</p>"},{"location":"developers/#io-pipeline-polars-io-plugin","title":"I/O Pipeline &amp; Polars IO Plugin","text":"<p>polars-bio uses the Polars IO plugin mechanism (<code>register_io_source</code>) to bridge between DataFusion's streaming execution and Polars' lazy evaluation. This section documents the end-to-end I/O pipeline and the pushdown optimizations applied at each stage.</p>"},{"location":"developers/#overview","title":"Overview","text":"<p>Every <code>scan_*()</code> / <code>read_*()</code> call flows through the same pipeline:</p> <pre><code>flowchart TB\n    subgraph User[\"User API\"]\n        scan[\"scan_bam() / scan_vcf() / scan_gff() / ...\"]\n    end\n\n    subgraph ReadFile[\"_read_file()\"]\n        register[\"1. py_register_table()&lt;br/&gt;Register DataFusion TableProvider\"]\n        schema[\"2. py_get_table_schema()&lt;br/&gt;Extract Arrow schema + metadata\"]\n        extract[\"3. extract_all_schema_metadata()&lt;br/&gt;Parse bio.* metadata\"]\n        lazy[\"4. _lazy_scan()&lt;br/&gt;Create Polars LazyFrame via IO plugin\"]\n        meta[\"5. set_coordinate_system() +&lt;br/&gt;set_source_metadata()\"]\n    end\n\n    subgraph IOPlugin[\"Polars IO Plugin (_overlap_source callback)\"]\n        direction TB\n        predicate[\"Predicate pushdown\"]\n        projection[\"Projection pushdown\"]\n        limit[\"Limit pushdown\"]\n        stream[\"execute_stream()&lt;br/&gt;Yield Arrow batches\"]\n        safety[\"Safety net:&lt;br/&gt;client-side filter/select\"]\n    end\n\n    subgraph DataFusion[\"DataFusion Engine\"]\n        tp[\"TableProvider&lt;br/&gt;(BamTableProvider, VcfTableProvider, ...)\"]\n        idx[\"Index-based partition pruning&lt;br/&gt;(BAI/CSI/TBI/CRAI)\"]\n        record[\"Record-level filtering\"]\n        proj[\"Column-level skip\"]\n    end\n\n    scan --&gt; ReadFile\n    ReadFile --&gt; IOPlugin\n    IOPlugin --&gt; DataFusion</code></pre>"},{"location":"developers/#step-by-step","title":"Step-by-step","text":"<p>1. Registration \u2014 <code>py_register_table()</code> (Rust/PyO3) creates a DataFusion <code>TableProvider</code> for the given format (e.g., <code>BamTableProvider</code>, <code>VcfTableProvider</code>). The provider handles file opening, index discovery, schema inference, and partition planning.</p> <p>2. Schema extraction \u2014 <code>py_get_table_schema()</code> returns the Arrow schema with all <code>bio.*</code> metadata without reading any data. This is used to derive the Polars schema for the IO plugin.</p> <p>3. Metadata parsing \u2014 <code>extract_all_schema_metadata()</code> decodes Arrow metadata into format-specific structures (VCF headers, sample names, etc.). See Metadata Flow for details.</p> <p>4. IO plugin registration \u2014 <code>_lazy_scan()</code> calls Polars' <code>register_io_source()</code>, providing:</p> <ul> <li>A schema (dict of column name \u2192 Polars dtype, derived from the Arrow schema)</li> <li>A callback function (<code>_overlap_source</code>) that receives pushdown hints from Polars' query optimizer</li> </ul> <p>5. Metadata attachment \u2014 Coordinate system and source metadata are set on the returned LazyFrame via <code>config_meta</code>.</p>"},{"location":"developers/#the-io-plugin-callback-_overlap_source","title":"The IO plugin callback (<code>_overlap_source</code>)","text":"<p>When Polars collects the LazyFrame (<code>.collect()</code> or streaming), it calls the registered <code>_overlap_source</code> callback with four parameters provided by the Polars engine:</p> <pre><code>def _overlap_source(\n    with_columns: pl.Expr | None,   # Requested columns (projection)\n    predicate: pl.Expr | None,       # Filter expression\n    n_rows: int | None,              # Row limit\n    _batch_size: int | None,         # Batch size hint\n) -&gt; Iterator[pl.DataFrame]:\n</code></pre> <p>The callback implements three pushdown optimizations, falling back to client-side evaluation when pushdown fails:</p>"},{"location":"developers/#predicate-pushdown","title":"Predicate pushdown","text":"<p>Predicate pushdown translates Polars filter expressions into DataFusion predicates so that filtering happens inside the DataFusion execution engine \u2014 before data reaches Python.</p> <p>The translation has two levels:</p> <p>Level 1: Index-based region pruning \u2014 When a genomic region filter is present (e.g., <code>pl.col(\"chrom\") == \"chr1\"</code>) and an index file exists, the upstream <code>TableProvider</code> uses the index to skip irrelevant file regions entirely. This is handled automatically by DataFusion's plan optimization \u2014 the predicate in the DataFusion <code>DataFrame.filter()</code> is propagated to the <code>TableProvider::scan()</code> method which performs partition pruning.</p> <p>Level 2: Record-level filtering \u2014 For non-genomic predicates (e.g., <code>pl.col(\"mapping_quality\") &gt;= 30</code>), the filter is evaluated per-record during parsing, before Arrow RecordBatch construction.</p> <p>The translation pipeline is:</p> <pre><code>Polars Expr                          (pl.col(\"chrom\") == \"chr1\")\n    \u2193 translate_predicate()\nDataFusion Expr                      (col(\"chrom\") == lit(\"chr1\"))\n    \u2193 datafusion_expr_to_sql()\nSQL string                           (\"chrom = 'chr1'\")\n    \u2193 query_df.parse_sql_expr()\nBinding-compatible DataFusion Expr   (same PyO3 compilation unit)\n    \u2193 query_df.filter()\nDataFusion DataFrame with predicate\n</code></pre> <p>Why the SQL roundtrip?</p> <p>The <code>polars_bio</code> Rust crate and the <code>pip install datafusion</code> Python package are separate PyO3 compilation units. Their <code>Expr</code> types are not interchangeable at the Python level. The bridge goes through <code>parse_sql_expr()</code>, which creates an <code>Expr</code> from the same compilation unit as the <code>DataFrame</code>, avoiding type mismatches.</p> <p>Format-aware validation \u2014 The predicate translator knows the column types for each format and validates operator compatibility:</p> Column type Supported operators String (<code>chrom</code>, <code>name</code>, ...) <code>==</code>, <code>!=</code>, <code>IN</code>, <code>NOT IN</code> Numeric (<code>start</code>, <code>end</code>, <code>mapping_quality</code>, ...) <code>==</code>, <code>!=</code>, <code>&lt;</code>, <code>&lt;=</code>, <code>&gt;</code>, <code>&gt;=</code>, <code>BETWEEN</code>, <code>IN</code>, <code>NOT IN</code> Unknown (BAM tags, VCF INFO fields, GFF attributes) All \u2014 DataFusion type-checks at execution Compound <code>AND</code> combinations of the above <p>Safety net \u2014 If translation fails (unsupported expression, type error), the callback logs a warning and applies the filter client-side on each batch after streaming:</p> <pre><code>if predicate is not None and not datafusion_predicate_applied:\n    out = out.filter(predicate)  # client-side fallback\n</code></pre>"},{"location":"developers/#projection-pushdown","title":"Projection pushdown","text":"<p>When Polars' optimizer determines only a subset of columns is needed (e.g., <code>.select([\"name\", \"chrom\"])</code>), the <code>with_columns</code> parameter carries the requested column names. The callback pushes this down to DataFusion:</p> <pre><code>select_exprs = [query_df.parse_sql_expr(f'\"{c}\"') for c in requested_cols]\nquery_df = query_df.select(*select_exprs)\n</code></pre> <p>This propagates through the DataFusion plan to the <code>TableProvider</code>, where parsing-level projection skips fields during record decoding:</p> <ul> <li>BAM: Unprojected fields (sequence, quality scores, CIGAR, tags) are not decoded</li> <li>VCF: Unprojected INFO/FORMAT fields are not parsed</li> <li>GFF: Attribute fields are only parsed if requested (GFF re-registers the table with <code>attr_fields</code> when specific attribute columns are projected)</li> </ul> <p>Safety net \u2014 If DataFusion projection fails, client-side select is applied:</p> <pre><code>if with_columns is not None and not datafusion_projection_applied:\n    out = out.select(with_columns)  # client-side fallback\n</code></pre>"},{"location":"developers/#limit-pushdown","title":"Limit pushdown","text":"<p>When a <code>.head(n)</code> or <code>.limit(n)</code> is applied, Polars passes <code>n_rows</code> to the callback. This is pushed to DataFusion:</p> <pre><code>if n_rows and n_rows &gt; 0:\n    query_df = query_df.limit(int(n_rows))\n</code></pre> <p>DataFusion propagates the limit to the <code>TableProvider</code>, which stops reading after enough records are produced. An additional client-side limit guard tracks remaining rows across batches.</p>"},{"location":"developers/#streaming-execution","title":"Streaming execution","text":"<p>After all pushdowns are applied, the callback executes the query as a stream of Arrow RecordBatches:</p> <pre><code>df_stream = query_df.execute_stream()\nfor r in df_stream:\n    out = pl.DataFrame(r.to_pyarrow())\n    # Apply client-side fallbacks if needed\n    yield out\n</code></pre> <p>Each yielded <code>pl.DataFrame</code> batch becomes part of the Polars LazyFrame execution graph. Polars can then apply further operations (joins, aggregations, sinks) on the stream.</p>"},{"location":"developers/#gff-special-handling","title":"GFF special handling","text":"<p>GFF is the only format where projection affects table registration. GFF files have a semi-structured <code>attributes</code> column containing <code>key=value</code> pairs. When specific attribute columns are projected (e.g., <code>gene_id</code>, <code>transcript_id</code>), the callback re-registers the table with those <code>attr_fields</code> so the TableProvider parses them into individual columns:</p> <pre><code>if input_format == InputFormat.Gff and attr_fields:\n    gff_opts = GffReadOptions(attr_fields=attr_fields, ...)\n    table_obj = py_register_table(_ctx, file_path, None, InputFormat.Gff, ropts)\n    table_to_query = table_obj.name\n</code></pre>"},{"location":"developers/#polars-integration","title":"Polars Integration","text":"<p>polars-bio leverages deep integration with Polars through the Arrow C Data Interface, enabling high-performance zero-copy data exchange between Polars LazyFrames and the Rust-based genomic range operations engine.</p>"},{"location":"developers/#architecture","title":"Architecture","text":"<pre><code>flowchart LR\n    subgraph Python[\"Python Layer\"]\n        LF[\"Polars LazyFrame\"]\n        DF[\"Polars DataFrame\"]\n    end\n\n    subgraph FFI[\"Arrow C Data Interface\"]\n        stream[\"Arrow C Stream&lt;br/&gt;(__arrow_c_stream__)\"]\n    end\n\n    subgraph Rust[\"Rust Layer (polars-bio)\"]\n        reader[\"ArrowArrayStreamReader\"]\n        datafusion[\"DataFusion Engine\"]\n        range_ops[\"Range Operations&lt;br/&gt;(overlap, nearest, etc.)\"]\n    end\n\n    LF --&gt; |\"ArrowStreamExportable\"| stream\n    DF --&gt; |\"to_arrow()\"| stream\n    stream --&gt; |\"Zero-copy FFI\"| reader\n    reader --&gt; datafusion\n    datafusion --&gt; range_ops</code></pre>"},{"location":"developers/#how-it-works","title":"How It Works","text":"<p>When you pass a Polars LazyFrame to range operations like <code>overlap()</code> or <code>nearest()</code>:</p> <ol> <li>Stream Export: The LazyFrame exports itself as an Arrow C Stream via <code>collect_batches(lazy=True)._inner.__arrow_c_stream__()</code> (Polars &gt;= 1.37.0)</li> <li>Zero-Copy Transfer: The stream pointer is passed directly to Rust - no data copying or Python object conversion</li> <li>GIL-Free Execution: Once the stream is exported, all data processing happens in Rust without holding Python's GIL</li> <li>Streaming Execution: Data flows through DataFusion's streaming engine, processing batches on-demand</li> </ol>"},{"location":"developers/#performance-benefits","title":"Performance Benefits","text":"Aspect Previous Approach Arrow C Stream GIL acquisition Per batch Once at export Data conversion Polars -&gt; PyArrow -&gt; Arrow Direct FFI Memory overhead Python iterator objects None Batch processing Python <code>__next__()</code> calls Native Rust iteration"},{"location":"developers/#requirements","title":"Requirements","text":"<ul> <li>Polars &gt;= 1.37.0 (required for <code>ArrowStreamExportable</code>)</li> </ul>"},{"location":"developers/#batch-size-configuration","title":"Batch Size Configuration","text":"<p>polars-bio automatically synchronizes the batch size between Polars streaming and DataFusion execution. When you set <code>datafusion.execution.batch_size</code>, Polars' <code>collect_batches()</code> will use the same chunk size:</p> <pre><code>import polars_bio as pb\n\n# Set batch size for both Polars and DataFusion\npb.set_option(\"datafusion.execution.batch_size\", \"8192\")\n\n# Now LazyFrame streaming uses 8192-row batches\n# This ensures consistent memory usage and processing patterns\n</code></pre> Setting Effect <code>datafusion.execution.batch_size</code> Controls batch size for both Polars streaming export and DataFusion processing Default 8192 rows (synchronized between Polars and DataFusion) <code>\"65536\"</code> Larger batches for high-throughput scenarios <p>Tip</p> <p>Matching batch sizes between Polars and DataFusion improves cache locality and reduces memory fragmentation when processing large datasets.</p>"},{"location":"developers/#example","title":"Example","text":"<pre><code>import polars as pl\nimport polars_bio as pb\n\n# Create a LazyFrame from a large file\nlf1 = pl.scan_parquet(\"variants.parquet\")\nlf2 = pl.scan_parquet(\"regions.parquet\")\n\n# Set coordinate system metadata\nlf1 = lf1.config_meta.set(coordinate_system_zero_based=True)\nlf2 = lf2.config_meta.set(coordinate_system_zero_based=True)\n\n# Range operation uses Arrow C Stream for efficient data transfer\nresult = pb.overlap(\n    lf1, lf2,\n    cols1=[\"chrom\", \"start\", \"end\"],\n    cols2=[\"chrom\", \"start\", \"end\"],\n    output_type=\"polars.LazyFrame\"\n)\n\n# Execute with Polars streaming engine\nresult.collect(engine=\"streaming\")\n</code></pre>"},{"location":"developers/#metadata-flow-polars-arrow","title":"Metadata Flow: Polars \u2194 Arrow","text":"<p>polars-bio maintains rich metadata (coordinate system, source format, file headers) across the boundary between Polars DataFrames and the Arrow/DataFusion layer. This section describes how metadata is stored, transferred, and preserved.</p>"},{"location":"developers/#overview_1","title":"Overview","text":"<pre><code>flowchart TB\n    subgraph Read[\"Read Path (file \u2192 Polars)\"]\n        direction TB\n        file[\"Genomic File&lt;br/&gt;(BAM, VCF, GFF, ...)\"]\n        tp[\"TableProvider&lt;br/&gt;(Rust, datafusion-bio-formats)\"]\n        arrow_schema[\"Arrow Schema&lt;br/&gt;with bio.* metadata\"]\n        extract[\"extract_all_schema_metadata()&lt;br/&gt;(Python)\"]\n        polars_meta[\"Polars config_meta&lt;br/&gt;(coordinate_system, source_format,&lt;br/&gt;source_path, source_header)\"]\n\n        file --&gt; tp\n        tp --&gt; |\"py_get_table_schema()\"| arrow_schema\n        arrow_schema --&gt; extract\n        extract --&gt; |\"set_coordinate_system()&lt;br/&gt;set_source_metadata()\"| polars_meta\n    end\n\n    subgraph Write[\"Write Path (Polars \u2192 file)\"]\n        direction TB\n        polars_meta_w[\"Polars config_meta&lt;br/&gt;(source_header JSON)\"]\n        arrow_schema_w[\"Arrow Schema&lt;br/&gt;with VCF field metadata\"]\n        tp_w[\"TableProvider::insert_into()&lt;br/&gt;(Rust)\"]\n        file_w[\"Output File\"]\n\n        polars_meta_w --&gt; |\"apply_vcf_metadata_to_schema()\"| arrow_schema_w\n        arrow_schema_w --&gt; tp_w\n        tp_w --&gt; file_w\n    end</code></pre>"},{"location":"developers/#arrow-side-metadata-datafusionrust","title":"Arrow-side metadata (DataFusion/Rust)","text":"<p>When a <code>TableProvider</code> registers a file, the upstream <code>datafusion-bio-formats</code> crates embed format-specific metadata into the Arrow schema using <code>bio.*</code>-prefixed keys:</p> Key prefix Format Examples <code>bio.coordinate_system_zero_based</code> All <code>\"true\"</code> or <code>\"false\"</code> <code>bio.vcf.file_format</code> VCF <code>\"VCFv4.2\"</code> <code>bio.vcf.contigs</code> VCF JSON array of contig definitions <code>bio.vcf.filters</code> VCF JSON array of filter definitions <code>bio.vcf.field.*</code> VCF Per-field metadata (<code>field_type</code>, <code>number</code>, <code>type</code>, <code>description</code>) <code>bio.bam.*</code> BAM/SAM/CRAM Format-specific metadata <code>bio.gff.*</code> GFF Format-specific metadata <code>bio.fastq.*</code> FASTQ Format-specific metadata <p>The Arrow schema is extracted without materializing any data via <code>py_get_table_schema()</code>, which calls <code>table.schema().as_arrow()</code> in Rust.</p>"},{"location":"developers/#polars-side-metadata-python","title":"Polars-side metadata (Python)","text":"<p>On the Python side, metadata is stored using the polars-config-meta library, which attaches key-value pairs to Polars DataFrames and LazyFrames via <code>config_meta</code>:</p> Key Type Description <code>coordinate_system_zero_based</code> <code>bool</code> <code>True</code> = 0-based half-open, <code>False</code> = 1-based closed <code>source_format</code> <code>str</code> Format identifier (<code>\"vcf\"</code>, <code>\"bam\"</code>, <code>\"gff\"</code>, ...) <code>source_path</code> <code>str</code> Original file path <code>source_header</code> <code>str</code> JSON-encoded format-specific header data <p>For Pandas DataFrames, the same keys are stored in <code>df.attrs</code>.</p>"},{"location":"developers/#read-path-arrow-polars","title":"Read path: Arrow \u2192 Polars","text":"<p>When <code>scan_*()</code> or <code>read_*()</code> is called, the I/O pipeline transfers metadata in three steps:</p> <ol> <li>Register table \u2014 <code>py_register_table()</code> creates a DataFusion <code>TableProvider</code> that embeds <code>bio.*</code> metadata in the Arrow schema</li> <li>Extract schema \u2014 <code>py_get_table_schema()</code> returns the PyArrow schema; <code>extract_all_schema_metadata()</code> parses all <code>bio.*</code> keys into a structured dict with format-specific parsers (VCF fields, sample names, contigs, etc.)</li> <li>Set Polars metadata \u2014 <code>set_coordinate_system()</code> and <code>set_source_metadata()</code> attach the parsed metadata to the LazyFrame via <code>config_meta</code></li> </ol> <pre><code># Internally, _read_file() does:\ntable = py_register_table(ctx, path, None, input_format, read_options)\nschema = py_get_table_schema(ctx, table.name)          # Arrow schema with bio.* metadata\nfull_metadata = extract_all_schema_metadata(schema)     # Parse into structured dict\nlf = _lazy_scan(schema, ...)                            # Create LazyFrame\nset_coordinate_system(lf, zero_based)                   # Attach to config_meta\nset_source_metadata(lf, format=\"vcf\", path=path, header=header_metadata)\n</code></pre>"},{"location":"developers/#write-path-polars-arrow","title":"Write path: Polars \u2192 Arrow","text":"<p>When writing files (e.g., <code>write_vcf()</code>, <code>sink_bam()</code>), metadata flows back from the Polars DataFrame to the Arrow schema:</p> <ol> <li>Extract Polars metadata \u2014 <code>get_metadata()</code> reads <code>source_header</code> JSON from <code>config_meta</code></li> <li>Apply to Arrow schema \u2014 <code>apply_vcf_metadata_to_schema()</code> (Rust) converts the JSON metadata back into Arrow field-level metadata (<code>bio.vcf.field.*</code> keys)</li> <li>Write with headers \u2014 The <code>TableProvider::insert_into()</code> implementation uses the schema metadata to write proper file headers (VCF INFO/FORMAT definitions, BAM <code>@SQ</code>/<code>@RG</code>/<code>@PG</code> records, etc.)</li> </ol>"},{"location":"developers/#metadata-preservation","title":"Metadata preservation","text":"<p>Polars metadata set via <code>config_meta</code> is preserved through most Polars operations:</p> <pre><code>lf = pb.scan_vcf(\"variants.vcf\")\nfiltered = lf.filter(pl.col(\"qual\") &gt; 30)    # metadata preserved\nselected = lf.select([\"chrom\", \"start\"])       # metadata preserved\nlimited = lf.head(100)                         # metadata preserved\n\n# All share the same metadata\nassert pb.get_metadata(lf)[\"format\"] == pb.get_metadata(filtered)[\"format\"]  # \"vcf\"\n</code></pre>"},{"location":"developers/#registered-table-metadata","title":"Registered table metadata","text":"<p>For registered DataFusion tables, coordinate system metadata can also be read directly from the Arrow schema:</p> <pre><code>from polars_bio.context import ctx\n\npb.register_vcf(\"variants.vcf\", name=\"my_variants\")\ndf = ctx.table(\"my_variants\")\nschema = df.schema()\nprint(schema.metadata)  # {b'bio.coordinate_system_zero_based': b'false', ...}\n</code></pre>"},{"location":"developers/#coordinate-system-internals","title":"Coordinate System Internals","text":"<p>polars-bio supports both 0-based half-open <code>[start, end)</code> and 1-based closed <code>[start, end]</code> coordinate systems. The user-facing API is documented in Features \u2014 Coordinate systems support. This section describes how the coordinate system propagates through the internal pipeline and affects query semantics.</p>"},{"location":"developers/#end-to-end-flow","title":"End-to-end flow","text":"<pre><code>flowchart LR\n    subgraph IO[\"I/O Layer\"]\n        scan[\"scan_*(use_zero_based=...)\"]\n    end\n\n    subgraph Meta[\"Metadata Layer\"]\n        config[\"config_meta&lt;br/&gt;coordinate_system_zero_based\"]\n        validate[\"validate_coordinate_systems()\"]\n    end\n\n    subgraph Python[\"Range Operations (Python)\"]\n        filter_op[\"FilterOp.Strict&lt;br/&gt;(0-based)&lt;br/&gt;FilterOp.Weak&lt;br/&gt;(1-based)\"]\n    end\n\n    subgraph Rust[\"Range Operations (Rust)\"]\n        upstream[\"RangesFilterOp\"]\n        sql_sign[\"SQL join condition:&lt;br/&gt;Strict \u2192 '&gt;' (exclusive end)&lt;br/&gt;Weak \u2192 '&gt;=' (inclusive end)\"]\n    end\n\n    scan --&gt; config\n    config --&gt; validate\n    validate --&gt; filter_op\n    filter_op --&gt; |\"PyO3\"| upstream\n    upstream --&gt; sql_sign</code></pre>"},{"location":"developers/#how-it-works_1","title":"How it works","text":"<ol> <li> <p>I/O sets metadata \u2014 When <code>scan_*()</code> or <code>read_*()</code> is called, <code>set_coordinate_system(lf, zero_based)</code> stores the coordinate system in <code>config_meta</code>. The <code>use_zero_based</code> parameter defaults to the global session setting <code>datafusion.bio.coordinate_system_zero_based</code>.</p> </li> <li> <p>Validation at operation time \u2014 Before every range operation (<code>overlap</code>, <code>nearest</code>, <code>count_overlaps</code>, <code>coverage</code>, <code>merge</code>), <code>validate_coordinate_systems()</code> reads <code>config_meta</code> from both input DataFrames and checks they match. In strict mode (<code>coordinate_system_check=true</code>), missing metadata raises <code>MissingCoordinateSystemError</code>. In lenient mode (default), it falls back to the global setting with a warning.</p> </li> <li> <p>Coordinate system \u2192 FilterOp \u2014 The validated boolean is mapped to a <code>FilterOp</code> enum:</p> <code>zero_based</code> <code>FilterOp</code> Interval semantics <code>True</code> <code>Strict</code> <code>[start, end)</code> \u2014 end is exclusive <code>False</code> <code>Weak</code> <code>[start, end]</code> \u2014 end is inclusive </li> <li> <p>FilterOp \u2192 SQL join condition \u2014 In Rust, <code>FilterOp</code> controls the comparison operator in the interval overlap predicate:</p> <ul> <li>Strict (0-based): <code>a.start &lt; b.end AND a.end &gt; b.start</code> \u2014 strict inequalities because end positions are exclusive</li> <li>Weak (1-based): <code>a.start &lt;= b.end AND a.end &gt;= b.start</code> \u2014 weak inequalities because end positions are inclusive</li> </ul> </li> </ol>"},{"location":"developers/#storage-locations","title":"Storage locations","text":"<p>The coordinate system is stored in different places depending on the context:</p> Context Storage Key Polars DataFrame/LazyFrame <code>config_meta</code> (polars-config-meta) <code>coordinate_system_zero_based</code> Pandas DataFrame <code>df.attrs</code> <code>coordinate_system_zero_based</code> DataFusion registered table Arrow schema metadata <code>bio.coordinate_system_zero_based</code> Session default DataFusion config <code>datafusion.bio.coordinate_system_zero_based</code>"},{"location":"developers/#session-parameters","title":"Session parameters","text":"Parameter Default Description <code>datafusion.bio.coordinate_system_zero_based</code> <code>\"false\"</code> (1-based) Global default applied when <code>use_zero_based</code> is not specified <code>datafusion.bio.coordinate_system_check</code> <code>\"false\"</code> (lenient) <code>\"true\"</code> = raise error on missing metadata; <code>\"false\"</code> = fall back to global default with warning"},{"location":"developers/#format-defaults","title":"Format defaults","text":"<p>Different file formats have conventional coordinate systems. polars-bio uses the global default (1-based) for all formats, but users can override at I/O time with <code>use_zero_based=True</code>:</p> Format Convention polars-bio default VCF 1-based 1-based GFF 1-based 1-based SAM/BAM/CRAM 1-based 1-based BED 0-based 1-based (override with <code>use_zero_based=True</code>) Pairs 0-based or 1-based 1-based <p>Warning</p> <p>BED format conventionally uses 0-based coordinates. If your pipeline mixes BED files with VCF/BAM data, be sure to read BED files with <code>use_zero_based=True</code> or set the global default accordingly \u2014 otherwise, range operations will produce incorrect results.</p>"},{"location":"developers/#range-operation-algorithm","title":"Range Operation Algorithm","text":"<p><code>polars-bio</code> implements a set of interval operations on genomic ranges, including binary operations (overlap, nearest, count-overlaps, coverage, subtract) and unary operations (merge, cluster, complement). The binary operations share a very similar algorithmic structure, which is presented in the diagram below. The unary operations (merge, cluster, complement) take a single set of intervals and produce transformed output \u2014 merged intervals, cluster assignments, or gap intervals respectively.</p> <pre><code>flowchart TB\n    %% Define header node\n    H[\"Interval operation\"]\n\n    %% Define DataFrame nodes\n    I0[\"left DataFrame\"]\n    I1[\"right DataFrame\"]\n\n    style I0 stroke-dasharray: 5 5, stroke-width: 1\n\n    %% Draw edges with labels\n    H --&gt;|probe /streaming/ side| I0\n    H --&gt;|build /search structure/ side| I1\n\n    %% Record batches under left DataFrame within a dotted box\n    I0 --&gt; LeftGroup\n    subgraph LeftGroup[\"Record Batches\"]\n        direction TB\n        LB0[\"Batch 1\"]\n        LB1[\"Batch 2\"]\n        LB2[\"Batch 3\"]\n    end\n    style LeftGroup stroke-dasharray: 5 5, stroke-width: 1\n\n    %% Record batches under right DataFrame within a dotted box\n    I1 --&gt; RightGroup\n    subgraph RightGroup[\"Record Batches\"]\n        direction TB\n        RB0[\"Batch 1\"]\n        RB1[\"Batch 2\"]\n        RB2[\"Batch 3\"]\n    end\n</code></pre> <p>The basic concept is that each operation consists of two sides: the probe side and the build side. The probe side is the one that is streamed, while the build side is the one that is implemented as a search data structure (for generic overlap operation the search structure can be changed using algorithm parameter, for other operations is always Cache Oblivious Interval Trees as according to the benchmark COITrees outperforms other data structures). In the case of nearest operation there is an additional sorted list of intervals used for searching for closest intervals in the case of non-existing overlaps.</p> <p>Note</p> <p>Available search structure implementations for overlap operation:</p> <ul> <li>COITrees</li> <li>IITree</li> <li>AVL-tree</li> <li>rust-lapper</li> <li>superintervals - available since <code>polars-bio</code> version <code>0.12.0</code></li> </ul> <p>Once the build side data structure is ready, then records from the probe side are processed against the search structure organized as record batches. Each record batch can be processed independently. Search structure nodes contains identifiers of the rows from the build side that are then used to construct a new record that is returned as a result of the operation.</p>"},{"location":"developers/#out-of-core-streaming-processing","title":"Out-of-core (streaming) processing","text":"<p>This algorithm allows you to process your results without requiring all your data to be in memory at the same time. In particular, the probe side can be streamed from a file stored locally or on a cloud object storage, while the build side needs to be fully materialized in memory. In real applications, the probe side is usually a large file with genomic intervals, while the build side is a smaller file with annotations or other genomic features. This allows you to process large genomic datasets without running out of memory.</p> <p>Note</p> <ol> <li>In this sense, the order of the sides is important, as the probe side is streamed and processed in batches, while the build side is fully materialized in memory.</li> <li>The smaller the build side and larger the number of overlaps are, the higher is the gain of memory efficiency. For instance, when we compare the real <code>8-7</code> (<code>10^7 vs. 1.2*10^6</code>) and synthetic (<code>10^7 vs. 10^7</code>) datasets, we can see that we benefit more from using streaming mode in the former benchmark.</li> </ol>"},{"location":"developers/#parallelization","title":"Parallelization","text":"<p>In the current implementation, the probe side can be processed in parallel using multiple threads on partitioned (implicitly or explicitly partitioned inputs \u2014 see parallel engine). The build side is predominantly single-threaded (with the notable exception of BGZF compressed or partitioned Parquet/CSV input data files reading, which can be parallelized).</p>"},{"location":"developers/#comparison-with-existing-tools","title":"Comparison with Existing Tools","text":"<p>The table below compares <code>polars-bio</code> with other popular Python libraries for genomic ranges operations.</p> Feature/Library polars-bio Bioframe PyRanges0 PyRanges1 pybedtools GenomicRanges out-of-core processing \u2705 \u274c \u274c \u274c \u274c \u274c parallel processing \u2705 \u274c \u2705<sup>1</sup> \u274c \u274c \u274c vectorized execution engine \u2705 \u274c \u274c \u274c \u274c \u274c cloud object storage support \u2705 \u2705/\u274c<sup>2</sup> \u274c \u274c \u274c \u2705 Pandas/Polars DataFrame support \u2705/\u2705 \u2705/\u274c \u2705/\u274c<sup>3</sup> \u2705/\u274c<sup>4</sup> \u274c/\u274c \u2705/\u2705 <p>Note</p> <p><sup>1</sup> PyRanges0 supports parallel processing with Ray, but it does not bring any performance benefits over single-threaded execution and it is not recommended.</p> <p><sup>2</sup> Some input functions, such as <code>read_table</code> support cloud object storage</p> <p><sup>3</sup> Only export/import with data copying is supported</p> <p><sup>4</sup> RangeFrame class extends Pandas DataFrame</p>"},{"location":"developers/#datafusion-extension-points","title":"DataFusion Extension Points","text":"<p>polars-bio uses the following Apache DataFusion extension points:</p>"},{"location":"developers/#physicaloptimizerrule-for-interval-join-rewriting","title":"PhysicalOptimizerRule for interval join rewriting","text":"<p>DefaultPhysicalPlanner and PhysicalOptimizerRule are used for detecting and rewriting generic interval join operations (i.e. overlap and nearest) with optimized execution strategies. This is implemented in datafusion-bio-function-ranges which exposes optimized interval join operations for Apache DataFusion with both SQL and DataFrame APIs.</p>"},{"location":"developers/#tableprovider-udtf-for-specialized-operations","title":"TableProvider + UDTF for specialized operations","text":"<p>TableProvider and User-Defined Table Function mechanisms are used for implementing specialized operations:</p> <ul> <li>coverage and count-overlaps: Implemented as UDTFs in <code>datafusion-bio-function-ranges</code></li> <li>depth (pileup): Implemented as a UDTF in <code>datafusion-bio-function-pileup</code>, registered as <code>depth()</code> in SQL</li> </ul> <pre><code>import polars_bio as pb\n\n# Coverage via UDTF\nresult = pb.coverage(df1, df2)\n\n# Depth via SQL UDTF\nresult = pb.sql(\"SELECT * FROM depth('alignments.bam')\").collect()\n</code></pre>"},{"location":"developers/#predicate-projection-pushdown-examples","title":"Predicate &amp; Projection Pushdown Examples","text":"<p>This section provides concrete usage examples and execution plan inspection for the pushdown optimizations described in the I/O Pipeline section above.</p>"},{"location":"developers/#record-level-filter-pushdown","title":"Record-level filter pushdown","text":"<p>Beyond index-based region queries, all formats support record-level predicate evaluation. Filters on columns like <code>mapping_quality</code>, <code>flag</code>, <code>score</code>, or <code>strand</code> are evaluated as each record is read, filtering early before Arrow RecordBatch construction.</p> <p>This works with or without an index file:</p> <pre><code>import polars as pl\nimport polars_bio as pb\n\n# No index needed \u2014 filters applied per-record during scan\ndf = (\n    pb.scan_bam(\"alignments.bam\")\n    .filter((pl.col(\"mapping_quality\") &gt;= 30) &amp; (pl.col(\"flag\") &amp; 4 == 0))\n    .collect()\n)\n\n# Combine genomic region (uses index) with record filter (applied per-record)\ndf = (\n    pb.scan_bam(\"alignments.bam\")\n    .filter(\n        (pl.col(\"chrom\") == \"chr1\")\n        &amp; (pl.col(\"start\") &gt;= 1000000)\n        &amp; (pl.col(\"mapping_quality\") &gt;= 30)\n    )\n    .collect()\n)\n</code></pre>"},{"location":"developers/#projection-pushdown_1","title":"Projection pushdown","text":"<p>BAM, CRAM, VCF, and Pairs formats support parsing-level projection pushdown. When you select a subset of columns, unprojected fields are skipped entirely during record parsing \u2014 no string formatting, sequence decoding, map lookups, or memory allocation for those fields. This can significantly reduce I/O and CPU time, especially for wide schemas like BAM (11+ columns) where you only need a few fields.</p> <p>Projection pushdown is enabled by default (<code>projection_pushdown=True</code>) on all <code>scan_*</code>/<code>read_*</code> calls and range operations. To disable it, pass <code>projection_pushdown=False</code>.</p> <pre><code>import polars_bio as pb\n\n# Only name and chrom are parsed from each BAM record (projection pushdown is on by default)\ndf = (\n    pb.scan_bam(\"alignments.bam\")\n    .select([\"name\", \"chrom\"])\n    .collect()\n)\n\n# Works the same for CRAM and VCF\ndf = pb.scan_cram(\"alignments.cram\").select([\"name\", \"chrom\"]).collect()\n\n# Works with SQL too \u2014 only referenced columns are parsed\npb.register_vcf(\"variants.vcf.gz\", \"variants\")\nresult = pb.sql(\"SELECT chrom, start FROM variants\").collect()\n</code></pre>"},{"location":"developers/#inspecting-the-execution-plan","title":"Inspecting the execution plan","text":"<p>You can verify pushdown is active by inspecting the physical execution plan:</p> <pre><code>from polars_bio.context import ctx\nfrom polars_bio.polars_bio import (\n    InputFormat, ReadOptions, BamReadOptions,\n    py_register_table, py_read_table,\n)\n\nread_options = ReadOptions(bam_read_options=BamReadOptions())\ntable = py_register_table(ctx, \"alignments.bam\", None, InputFormat.Bam, read_options)\ndf = py_read_table(ctx, table.name).select_columns(\"name\", \"chrom\")\nprint(df.execution_plan())\n# CooperativeExec\n#   BamExec: projection=[name, chrom]    &lt;-- only 2 of 11 columns parsed\n</code></pre> <p>Tip</p> <p><code>COUNT(*)</code> queries also benefit \u2014 when no columns are needed, the empty projection path avoids parsing any fields while still counting records correctly.</p>"},{"location":"developers/#building-development","title":"Building &amp; Development","text":""},{"location":"developers/#building-from-source","title":"Building from source","text":"<p>polars-bio can be built from source using maturin and poetry:</p> <pre><code>git clone https://github.com/biodatageeks/polars-bio.git\ncd polars-bio\npoetry env use 3.12\npoetry update\nRUSTFLAGS=\"-Ctarget-cpu=native\" maturin build --release -m Cargo.toml\npip install target/wheels/polars_bio-*.whl\n</code></pre> <p>For development (installs directly into the current virtual environment):</p> <pre><code>RUSTFLAGS=\"-Ctarget-cpu=native\" maturin develop --release -m Cargo.toml\n</code></pre> <p>Required dependencies</p> <ul> <li>Python &gt;= 3.10, &lt; 3.15 (3.12 or 3.13 recommended, 3.14 is experimental)</li> <li>poetry</li> <li>cmake</li> <li>Rust compiler + Cargo (rustup recommended)</li> </ul>"},{"location":"developers/#running-tests","title":"Running tests","text":"<p>Unit and integration tests use pytest:</p> <pre><code># Run all tests\npython -m pytest tests/ -v\n\n# Run specific test modules\npython -m pytest tests/test_io_bam.py -v\npython -m pytest tests/test_io_vcf.py -v\n</code></pre> <p>For cloud storage integration tests (requires <code>azure-cli</code> and <code>docker</code>):</p> <pre><code>cd it\nsource bin/start.sh\nJUPYTER_PLATFORM_DIRS=1 pytest it_object_storage_io.py -o log_cli=true --log-cli-level=INFO\nsource bin/stop.sh\n</code></pre> <p>Check the <code>README</code> in the <code>it</code> directory for more information.</p>"},{"location":"developers/#building-documentation","title":"Building documentation","text":"<p>The documentation is built with MkDocs and the Material theme:</p> <pre><code># Full build with all dynamic content\nMKDOCS_EXPORTER_PDF=false JUPYTER_PLATFORM_DIRS=1 mkdocs serve -w polars_bio\n</code></pre> <p>To speed up development builds by disabling dynamic content rendering:</p> <pre><code>MKDOCS_EXPORTER_PDF=false ENABLE_MD_EXEC=false ENABLE_MKDOCSTRINGS=false ENABLE_JUPYTER=false JUPYTER_PLATFORM_DIRS=1 mkdocs serve\n</code></pre>"},{"location":"developers/#rust-compilation-check","title":"Rust compilation check","text":"<p>For a quick compilation check without building the full wheel:</p> <pre><code>cargo check\n</code></pre>"},{"location":"faq/","title":"\u2753 FAQ","text":"<ol> <li> <p>What versions of Polars are supported?</p> <p>Short answer: Polars &gt;= 1.37.0 is required.</p> <p>Long answer: polars-bio requires Polars 1.37.1 or later because it uses the <code>ArrowStreamExportable</code> feature (PR #25994) for efficient zero-copy data exchange between Polars LazyFrames and the Rust-based genomic operations engine. This feature provides:</p> <ul> <li>Arrow C Stream FFI: LazyFrames export data via <code>__arrow_c_stream__()</code>, enabling direct Arrow FFI transfer to Rust without Python object conversions</li> <li>GIL-free streaming: The GIL is only acquired once when exporting the stream; all subsequent batch processing happens in pure Rust</li> <li>Reduced memory overhead: No Python iterator objects or intermediate conversions</li> </ul> <p>We recommend handling most of the heavy lifting on the DataFusion side (e.g., using SQL and views) and relying on Polars' streaming capabilities primarily for projection, filtering, and sinking results. See the Polars Integration section for more details on the architecture.</p> </li> <li> <p>What to do if I get  <code>Illegal instruction (core dumped)</code> when using polars-bio? This error is likely due to the fact that the ABI of the polars-bio wheel package does not match the ABI of the Python interpreter. To fix this, you can build the wheel package from source. See Quickstart for more information. </p><pre><code>#/var/log/syslog\n\npolars-bio-intel kernel: [ 1611.175045] traps: python[8844] trap invalid opcode ip:709d3ec253cc sp:7ffcc28754e8 error:0 in polars_bio.abi3.so[709d36533000+9aab000]\n</code></pre><p></p> </li> <li> <p>How to build the documentation?    To build the documentation, you need to install the <code>polars-bio</code> package and then run the following command in the root directory of the repository: </p><pre><code>MKDOCS_EXPORTER_PDF=false JUPYTER_PLATFORM_DIRS=1 mkdocs serve  -w polars_bio\n</code></pre> Some pages of the documentation take a while to build\u2014to speed up the process, you can disable dynamic content rendering: <pre><code>MKDOCS_EXPORTER_PDF=false ENABLE_MD_EXEC=false ENABLE_MKDOCSTRINGS=false ENABLE_JUPYTER=false JUPYTER_PLATFORM_DIRS=1 mkdocs serve\n</code></pre><p></p> </li> <li> <p>How to build the source code and install in the current virtual environment? </p><pre><code>RUSTFLAGS=\"-Ctarget-cpu=native\" maturin develop --release  -m Cargo.toml\n</code></pre><p></p> </li> <li> <p>How to run the integration tests?    To run the integration tests, you need to have the <code>azure-cli</code>, <code>docker</code>, and <code>pytest</code> installed. Then, you can run the following commands: </p><pre><code>cd it\nsource bin/start.sh\nJUPYTER_PLATFORM_DIRS=1 pytest it_object_storage_io.py -o log_cli=true --log-cli-level=INFO\nsource bin/stop.sh\n</code></pre> Check the <code>README</code> in <code>it</code> directory for more information.<p></p> </li> </ol>"},{"location":"features/","title":"\ud83d\udd28Features","text":""},{"location":"features/#genomic-ranges-operations","title":"Genomic ranges operations","text":"operation Bioframe polars-bio PyRanges0 PyRanges1 Pybedtools GenomicRanges overlap overlap overlap join<sup>1</sup> join_overlaps intersect<sup>2</sup> find_overlaps<sup>3</sup> nearest closest nearest nearest nearest_ranges closest<sup>4</sup> nearest<sup>5</sup> count_overlaps count_overlaps count_overlaps count_overlaps count_overlaps intersect<sup>6</sup> count_overlaps cluster cluster cluster cluster cluster_overlaps cluster merge merge merge merge merge_overlaps merge reduce<sup>7</sup> complement complement complement complement_ranges complement gaps<sup>8</sup> subtract subtract subtract subtract subtract_overlaps subtract subtract coverage coverage coverage coverage coverage coverage expand expand expand extend extend_ranges slop resize sort sort_bedframe sort sort sort_ranges sort sort read_table read_table read_table read_bed read_bed BedTool read_bed <p>Note</p> <ol> <li>There is an overlap method in PyRanges, but its output is only limited to indices of intervals from the other Dataframe that overlap. In Bioframe's benchmark also join method instead of overlap was used.</li> <li>wa and wb options used to obtain a comparable output.</li> <li>Output contains only a list with the same length as query, containing hits to overlapping indices. Data transformation is required to obtain the same output as in other libraries.   Since the performance was far worse than in more efficient libraries anyway, additional data transformation was not included in the benchmark.</li> <li>s=first was used to obtain a comparable output.</li> <li>select=\"arbitrary\" was used to obtain a comparable output.</li> <li>-c flag used with <code>intersect</code> to count overlaps per feature.</li> <li>GenomicRanges exposes merge as reduce().</li> <li>GenomicRanges exposes complement as gaps().</li> </ol> <p>Limitations</p> <p>For now polars-bio uses <code>int32</code> positions encoding for interval operations (issue) meaning that it does not support operation on chromosomes longer than 2Gb. <code>int64</code> support is planned for future releases (issue).</p>"},{"location":"features/#pileup-operations","title":"Pileup operations","text":"<p>Per-base read depth computation from alignment files using CIGAR operations. Produces mosdepth-compatible coverage blocks.</p> Feature mosdepth samtools depth polars-bio depth <pre><code>import polars_bio as pb\n\n# Compute per-base depth from a BAM file\ndf = pb.depth(\"alignments.bam\").collect()\n\n# With MAPQ filter (equivalent to samtools depth -q 20)\ndf = pb.depth(\"alignments.bam\", min_mapping_quality=20).collect()\n\n# Via SQL\ndf = pb.sql(\"SELECT * FROM depth('alignments.bam')\").collect()\n</code></pre>"},{"location":"features/#coordinate-systems-support","title":"Coordinate systems support","text":"<p>polars-bio supports both 0-based half-open and 1-based closed coordinate systems for genomic ranges operations. By default, it uses 1-based closed coordinates, which is the native format for VCF, GFF, and SAM/BAM files.</p>"},{"location":"features/#how-it-works","title":"How it works","text":"<p>The coordinate system is managed through DataFrame metadata that is set at I/O time and read by range operations. This ensures consistency throughout your analysis pipeline.</p> <pre><code>flowchart TB\n    subgraph IO[\"I/O Layer\"]\n        scan[\"scan_vcf/gff/bam/cram/bed()\"]\n        read[\"read_vcf/gff/bam/cram/bed()\"]\n    end\n\n    subgraph Config[\"Session Configuration\"]\n        zero_based[\"datafusion.bio.coordinate_system_zero_based&lt;br/&gt;(default: false = 1-based)\"]\n        check[\"datafusion.bio.coordinate_system_check&lt;br/&gt;(default: false = lenient)\"]\n    end\n\n    subgraph DF[\"DataFrame with Metadata\"]\n        polars_meta[\"Polars DataFrame/LazyFrame&lt;br/&gt;coordinate_system_zero_based\"]\n        pandas_meta[\"Pandas DataFrame&lt;br/&gt;df.attrs\"]\n    end\n\n    subgraph RangeOps[\"Range Operations\"]\n        overlap[\"overlap()\"]\n        nearest[\"nearest()\"]\n        count[\"count_overlaps()\"]\n        coverage[\"coverage()\"]\n        merge[\"merge()\"]\n        cluster[\"cluster()\"]\n        complement[\"complement()\"]\n        subtract[\"subtract()\"]\n    end\n\n    subgraph Validation[\"Metadata Validation\"]\n        validate[\"validate_coordinate_systems()\"]\n        error1[\"MissingCoordinateSystemError\"]\n        error2[\"CoordinateSystemMismatchError\"]\n        fallback[\"Fallback to global config&lt;br/&gt;+ emit warning\"]\n    end\n\n    scan --&gt; |\"sets metadata\"| polars_meta\n    read --&gt; |\"sets metadata\"| polars_meta\n    zero_based --&gt; |\"use_zero_based param&lt;br/&gt;or default\"| scan\n    zero_based --&gt; |\"use_zero_based param&lt;br/&gt;or default\"| read\n\n    polars_meta --&gt; overlap\n    polars_meta --&gt; nearest\n    polars_meta --&gt; count\n    polars_meta --&gt; coverage\n    polars_meta --&gt; merge\n    polars_meta --&gt; cluster\n    polars_meta --&gt; complement\n    polars_meta --&gt; subtract\n    pandas_meta --&gt; overlap\n\n    overlap --&gt; validate\n    nearest --&gt; validate\n    count --&gt; validate\n    coverage --&gt; validate\n    merge --&gt; validate\n    cluster --&gt; validate\n    complement --&gt; validate\n    subtract --&gt; validate\n\n    validate --&gt; |\"metadata missing\"| check\n    validate --&gt; |\"metadata mismatch\"| error2\n    check --&gt; |\"true (strict)\"| error1\n    check --&gt; |\"false (lenient)\"| fallback\n    fallback --&gt; zero_based</code></pre>"},{"location":"features/#session-parameters","title":"Session parameters","text":"<p>polars-bio provides two session parameters to control coordinate system behavior:</p> Parameter Default Description <code>datafusion.bio.coordinate_system_zero_based</code> <code>\"false\"</code> (1-based) Default coordinate system for I/O operations when <code>use_zero_based</code> is not specified <code>datafusion.bio.coordinate_system_check</code> <code>\"false\"</code> (lenient) Whether to raise an error when DataFrame metadata is missing <pre><code>import polars_bio as pb\n\n# Check current settings\nprint(pb.get_option(pb.POLARS_BIO_COORDINATE_SYSTEM_ZERO_BASED))  # \"false\"\nprint(pb.get_option(pb.POLARS_BIO_COORDINATE_SYSTEM_CHECK))       # \"false\"\n\n# Change to 0-based coordinates globally\npb.set_option(pb.POLARS_BIO_COORDINATE_SYSTEM_ZERO_BASED, True)\n</code></pre>"},{"location":"features/#reading-files-with-coordinate-system-metadata","title":"Reading files with coordinate system metadata","text":"<p>When you read genomic files using polars-bio I/O functions, the coordinate system metadata is automatically set on the returned DataFrame:</p> <pre><code>import polars_bio as pb\n\n# Default: 1-based coordinates (use_zero_based=False)\ndf = pb.scan_vcf(\"variants.vcf\")\n# Metadata is automatically set: coordinate_system_zero_based=False\n\n# Explicit 0-based coordinates\ndf_zero = pb.scan_bed(\"regions.bed\", use_zero_based=True)\n# Metadata is automatically set: coordinate_system_zero_based=True\n\n# Range operations read coordinate system from metadata\nresult = pb.overlap(df, df_zero, ...)  # Raises CoordinateSystemMismatchError!\n</code></pre>"},{"location":"features/#setting-metadata-on-dataframes","title":"Setting metadata on DataFrames","text":"<p>For DataFrames not created via polars-bio I/O functions, you must set the coordinate system metadata manually:</p> Polars DataFrame/LazyFramePandas DataFrame <pre><code>import polars as pl\n\n# Create a DataFrame\ndf = pl.DataFrame({\n    \"chrom\": [\"chr1\", \"chr1\"],\n    \"start\": [100, 200],\n    \"end\": [150, 250]\n}).lazy()\n\n# Set coordinate system metadata (requires polars-config-meta)\ndf = df.config_meta.set(coordinate_system_zero_based=False)  # 1-based\n\n# Now it can be used with range operations\nresult = pb.overlap(df, other_df, ...)\n</code></pre> <pre><code>import pandas as pd\n\n# Create a DataFrame\npdf = pd.DataFrame({\n    \"chrom\": [\"chr1\", \"chr1\"],\n    \"start\": [100, 200],\n    \"end\": [150, 250]\n})\n\n# Set coordinate system metadata via df.attrs\npdf.attrs[\"coordinate_system_zero_based\"] = False  # 1-based\n\n# Now it can be used with range operations\nresult = pb.overlap(pdf, other_df, output_type=\"pandas.DataFrame\", ...)\n</code></pre>"},{"location":"features/#error-handling","title":"Error handling","text":"<p>polars-bio raises specific errors to prevent coordinate system mismatches:</p>"},{"location":"features/#missingcoordinatesystemerror","title":"MissingCoordinateSystemError","text":"<p>Raised when a DataFrame lacks coordinate system metadata:</p> <pre><code>import polars as pl\nimport polars_bio as pb\n\n# DataFrame without metadata\ndf = pl.DataFrame({\"chrom\": [\"chr1\"], \"start\": [100], \"end\": [200]}).lazy()\n\n# This raises MissingCoordinateSystemError\npb.overlap(df, other_df, ...)\n</code></pre> <p>How to fix: Set metadata on your DataFrame before passing it to range operations (see examples above).</p>"},{"location":"features/#coordinatesystemmismatcherror","title":"CoordinateSystemMismatchError","text":"<p>Raised when two DataFrames have different coordinate systems:</p> <pre><code>import polars_bio as pb\n\n# One DataFrame is 1-based, another is 0-based\ndf1 = pb.scan_vcf(\"file.vcf\")                    # 1-based (default)\ndf2 = pb.scan_bed(\"file.bed\", use_zero_based=True)  # 0-based\n\n# This raises CoordinateSystemMismatchError\npb.overlap(df1, df2, ...)\n</code></pre> <p>How to fix: Ensure both DataFrames use the same coordinate system.</p>"},{"location":"features/#default-behavior-lenient-validation","title":"Default behavior (lenient validation)","text":"<p>By default, polars-bio uses lenient validation (<code>coordinate_system_check=false</code>). When a DataFrame lacks coordinate system metadata, it falls back to the global configuration and emits a warning:</p> <pre><code>import polars as pl\nimport polars_bio as pb\n\n# DataFrames without metadata will use the global config with a warning\ndf = pl.DataFrame({\"chrom\": [\"chr1\"], \"start\": [100], \"end\": [200]}).lazy()\nresult = pb.overlap(df, other_df, ...)  # Uses global coordinate system setting\n# Warning: Coordinate system metadata is missing. Using global config...\n</code></pre>"},{"location":"features/#strict-mode","title":"Strict mode","text":"<p>For production pipelines where coordinate system consistency is critical, you can enable strict validation:</p> <pre><code>import polars_bio as pb\n\n# Enable strict coordinate system check\npb.set_option(pb.POLARS_BIO_COORDINATE_SYSTEM_CHECK, True)\n\n# Now DataFrames without metadata will raise MissingCoordinateSystemError\n</code></pre> <p>Tip</p> <p>Enable strict mode in production pipelines to catch coordinate system mismatches early and prevent incorrect results.</p>"},{"location":"features/#migration-from-previous-versions","title":"Migration from previous versions","text":"<p>If you're upgrading from a previous version of polars-bio:</p> <ol> <li>Range operations no longer accept <code>use_zero_based</code> parameter - coordinate system is read from DataFrame metadata</li> <li>I/O functions use <code>use_zero_based</code> parameter (renamed from <code>one_based</code> with inverted logic)</li> <li>Pandas DataFrames require explicit metadata - set <code>df.attrs[\"coordinate_system_zero_based\"]</code> before range operations</li> </ol> <pre><code># Before (old API)\nresult = pb.overlap(df1, df2, use_zero_based=True, ...)\n\n# After (new API) - set metadata at I/O time or on DataFrames\ndf1 = pb.scan_vcf(\"file.vcf\", use_zero_based=True)\ndf2 = pb.scan_bed(\"file.bed\", use_zero_based=True)\nresult = pb.overlap(df1, df2, ...)  # Reads from metadata\n</code></pre>"},{"location":"features/#file-metadata","title":"File Metadata","text":"<p>polars-bio automatically attaches comprehensive metadata to DataFrames when reading genomic files. This metadata includes format information, coordinate systems, and format-specific details like VCF header fields.</p>"},{"location":"features/#vcf-changes-since-0240","title":"VCF changes since 0.24.0","text":"<p>Compared to <code>v0.24.0</code>, VCF handling has the following behavior changes:</p> <ol> <li> <p>Multisample FORMAT schema changed (breaking)    Multisample VCF FORMAT data is now exposed as a nested <code>genotypes</code> column (<code>list&lt;struct&lt;sample_id, values&gt;&gt;</code>) instead of flattened columns like <code>NA12878_GT</code>.</p> </li> <li> <p>Single-sample FORMAT schema unchanged    Single-sample VCFs still expose FORMAT fields as top-level columns (<code>GT</code>, <code>DP</code>, <code>GQ</code>, ...).</p> </li> <li> <p><code>info_fields=None</code> now includes all INFO fields by default    When <code>info_fields</code> is not provided, all header INFO fields are available in the schema.    Use <code>info_fields=[]</code> to exclude INFO columns.</p> </li> <li> <p>FORMAT metadata fidelity improved <code>meta[\"header\"][\"format_fields\"]</code> now preserves FORMAT <code>number</code>/<code>type</code>/<code>description</code> via schema-level metadata.</p> </li> </ol>"},{"location":"features/#migration-examples","title":"Migration examples","text":"<pre><code>import polars_bio as pb\n\n# Before (v0.24.0 style multisample access):\n# df = pb.read_vcf(\"multisample.vcf\", format_fields=[\"GT\", \"DP\"])\n# df.select([\"chrom\", \"start\", \"NA12878_GT\", \"NA12878_DP\"])\n\n# Now (current):\ndf = pb.read_vcf(\"multisample.vcf\", format_fields=[\"GT\", \"DP\"])\ndf.select([\"chrom\", \"start\", \"genotypes\"])\n\n# INFO behavior:\ndf_all_info = pb.read_vcf(\"variants.vcf\")              # all INFO fields\ndf_no_info = pb.read_vcf(\"variants.vcf\", info_fields=[])  # no INFO fields\n</code></pre>"},{"location":"features/#metadata-structure","title":"Metadata Structure","text":"<p>The metadata is stored in a clean, user-friendly structure:</p> <pre><code>import polars_bio as pb\n\nlf = pb.scan_vcf(\"variants.vcf\")\nmeta = pb.get_metadata(lf)\n\n# Returns:\n{\n  \"format\": \"vcf\",                           # File format\n  \"path\": \"variants.vcf\",                    # Source file path\n  \"coordinate_system_zero_based\": False,     # Coordinate system (VCF is 1-based)\n  \"header\": {\n    \"version\": \"VCFv4.2\",                    # VCF version\n    \"sample_names\": [\"Sample1\", \"Sample2\"],  # Sample names\n    \"info_fields\": {                         # INFO field definitions\n      \"AF\": {\n        \"number\": \"A\",\n        \"type\": \"Float\",\n        \"description\": \"Allele Frequency\",\n        \"id\": \"AF\"\n      }\n    },\n    \"format_fields\": {                       # FORMAT field definitions\n      \"GT\": {\n        \"number\": \"1\",\n        \"type\": \"String\",\n        \"description\": \"Genotype\"\n      }\n    },\n    \"contigs\": [...],                        # Contig definitions\n    \"filters\": [...],                        # Filter definitions\n    \"_datafusion_table_name\": \"variants\"     # Internal table name (for debugging)\n  }\n}\n</code></pre>"},{"location":"features/#accessing-metadata","title":"Accessing Metadata","text":"<p>polars-bio provides three main functions for working with metadata:</p>"},{"location":"features/#1-get-all-metadata-as-a-dictionary","title":"1. Get all metadata as a dictionary","text":"<pre><code>import polars_bio as pb\n\nlf = pb.scan_vcf(\"file.vcf\")\nmeta = pb.get_metadata(lf)\n\n# Access different parts\nprint(meta[\"format\"])                       # \"vcf\"\nprint(meta[\"path\"])                         # \"file.vcf\"\nprint(meta[\"coordinate_system_zero_based\"]) # False (1-based)\n\n# Access VCF-specific fields\nprint(meta[\"header\"][\"version\"])            # \"VCFv4.2\"\nprint(meta[\"header\"][\"sample_names\"])       # [\"Sample1\", \"Sample2\"]\n\n# Access INFO field definitions\naf_field = meta[\"header\"][\"info_fields\"][\"AF\"]\nprint(af_field[\"type\"])                     # \"Float\"\nprint(af_field[\"description\"])              # \"Allele Frequency\"\n\n# Access FORMAT field definitions\ngt_field = meta[\"header\"][\"format_fields\"][\"GT\"]\nprint(gt_field[\"type\"])                     # \"String\"\n</code></pre>"},{"location":"features/#2-print-metadata-as-formatted-json","title":"2. Print metadata as formatted JSON","text":"<pre><code>import polars_bio as pb\n\nlf = pb.scan_vcf(\"file.vcf\")\n\n# Print as pretty JSON\npb.print_metadata_json(lf)\n\n# Customize indentation\npb.print_metadata_json(lf, indent=4)\n</code></pre>"},{"location":"features/#3-print-human-readable-summary","title":"3. Print human-readable summary","text":"<pre><code>import polars_bio as pb\n\nlf = pb.scan_vcf(\"file.vcf\")\npb.print_metadata_summary(lf)\n</code></pre> <p>Output: </p><pre><code>======================================================================\nMetadata Summary\n======================================================================\n\nFormat: vcf\nPath: file.vcf\nCoordinate System: 1-based\n\nFormat-specific metadata:\n----------------------------------------------------------------------\n  VCF Version: VCFv4.2\n  Samples (3): Sample1, Sample2, Sample3\n  INFO fields: 5\n    - AF: Float (Allele Frequency)\n    - DP: Integer (Total Depth)\n    - AC: Integer (Allele Count)\n  FORMAT fields: 3\n    - GT: String (Genotype)\n    - DP: Integer (Read Depth)\n    - GQ: Integer (Genotype Quality)\n\n======================================================================\n</code></pre><p></p>"},{"location":"features/#format-specific-metadata","title":"Format-Specific Metadata","text":"<p>Different file formats include different metadata:</p> VCFFASTQBED/BAM/GFF <pre><code>lf = pb.scan_vcf(\"variants.vcf\")\nmeta = pb.get_metadata(lf)\n\n# VCF header metadata\nmeta[\"header\"][\"version\"]          # VCF version\nmeta[\"header\"][\"sample_names\"]     # Sample names\nmeta[\"header\"][\"info_fields\"]      # INFO field definitions\nmeta[\"header\"][\"format_fields\"]    # FORMAT field definitions\nmeta[\"header\"][\"contigs\"]          # Contig definitions\nmeta[\"header\"][\"filters\"]          # Filter definitions\n</code></pre> <pre><code>lf = pb.scan_fastq(\"reads.fastq.gz\")\nmeta = pb.get_metadata(lf)\n\n# FASTQ-specific metadata\nmeta[\"format\"]                     # \"fastq\"\nmeta[\"path\"]                       # \"reads.fastq.gz\"\nmeta[\"coordinate_system_zero_based\"] # None (N/A for FASTQ)\n</code></pre> <pre><code>lf = pb.scan_bed(\"regions.bed\")\nmeta = pb.get_metadata(lf)\n\n# Basic metadata\nmeta[\"format\"]                     # \"bed\"\nmeta[\"coordinate_system_zero_based\"] # True (0-based)\n</code></pre>"},{"location":"features/#setting-custom-metadata","title":"Setting Custom Metadata","text":"<p>You can set metadata on DataFrames created from other sources:</p> <pre><code>import polars as pl\nimport polars_bio as pb\n\n# Create a DataFrame\ndf = pl.DataFrame({\n    \"chrom\": [\"chr1\", \"chr1\"],\n    \"start\": [100, 200],\n    \"end\": [150, 250]\n}).lazy()\n\n# Set metadata\npb.set_source_metadata(\n    df,\n    format=\"bed\",\n    path=\"custom.bed\",\n    header={\"description\": \"Custom intervals\"}\n)\n\n# Now metadata is available\nmeta = pb.get_metadata(df)\nprint(meta[\"format\"])  # \"bed\"\nprint(meta[\"header\"][\"description\"])  # \"Custom intervals\"\n</code></pre>"},{"location":"features/#metadata-preservation","title":"Metadata Preservation","text":"<p>Metadata is preserved through Polars operations:</p> <pre><code>lf = pb.scan_vcf(\"variants.vcf\")\n\n# Metadata persists after operations\nfiltered = lf.filter(pl.col(\"qual\") &gt; 30)\nselected = lf.select([\"chrom\", \"start\", \"end\"])\nlimited = lf.head(100)\n\n# All have the same metadata\nmeta1 = pb.get_metadata(lf)\nmeta2 = pb.get_metadata(filtered)\nmeta3 = pb.get_metadata(selected)\n\nassert meta1[\"format\"] == meta2[\"format\"] == meta3[\"format\"]  # All \"vcf\"\n</code></pre>"},{"location":"features/#using-metadata-for-debugging","title":"Using Metadata for Debugging","text":"<p>The <code>_datafusion_table_name</code> field is useful for debugging DataFusion SQL queries:</p> <pre><code>lf = pb.scan_vcf(\"variants.vcf\")\nmeta = pb.get_metadata(lf)\n\n# Get internal table name\ntable_name = meta[\"header\"][\"_datafusion_table_name\"]\nprint(f\"Table name: {table_name}\")  # \"variants\"\n\n# Use it in SQL queries for debugging\nresult = pb.sql(f\"SELECT COUNT(*) FROM {table_name}\")\n</code></pre>"},{"location":"features/#api-reference","title":"API Reference","text":"Function Description <code>get_metadata(df)</code> Get all metadata as a dictionary <code>print_metadata_json(df, indent=2)</code> Print metadata as formatted JSON <code>print_metadata_summary(df)</code> Print human-readable metadata summary <code>set_source_metadata(df, format, path, header)</code> Set metadata on a DataFrame"},{"location":"features/#file-formats-support","title":"File formats support","text":"<p>For bioinformatic format there are always three methods available: <code>read_*</code> (eager), <code>scan_*</code> (lazy) and <code>register_*</code> that can be used to either read file into Polars DataFrame/LazyFrame or register it as a DataFusion table for further processing using SQL or builtin interval methods. In either case, local and or cloud storage files can be used as an input. Please refer to cloud storage section for more details.</p> Format Single-threaded Parallel (indexed) Limit pushdown Predicate pushdown Projection pushdown BED \u274c \u274c \u274c VCF  (TBI/CSI) BAM  (BAI/CSI) CRAM  (CRAI) FASTQ  (GZI) \u274c \u274c FASTA \u274c \u274c \u274c GFF3  (TBI/CSI) Pairs  (TBI/CSI)"},{"location":"features/#indexed-reads-predicate-pushdown","title":"Indexed reads &amp; predicate pushdown","text":"<p>When an index file is present alongside the data file (BAI/CSI for BAM, CRAI for CRAM, TBI/CSI for VCF, GFF, and Pairs), polars-bio can push genomic region filters down to the DataFusion execution layer. This enables index-based random access \u2014 only the relevant genomic regions are read from disk, dramatically improving performance for selective queries on large files.</p> <p>Index files are auto-discovered by convention. Predicate pushdown is enabled by default for BAM, CRAM, VCF, GFF, and Pairs formats \u2014 no extra configuration is needed.</p>"},{"location":"features/#supported-index-formats","title":"Supported index formats","text":"Data Format Index Formats Naming Convention BAM BAI, CSI <code>sample.bam.bai</code> or <code>sample.bai</code>, <code>sample.bam.csi</code> CRAM CRAI <code>sample.cram.crai</code> VCF (bgzf) TBI, CSI <code>sample.vcf.gz.tbi</code>, <code>sample.vcf.gz.csi</code> GFF (bgzf) TBI, CSI <code>sample.gff.gz.tbi</code>, <code>sample.gff.gz.csi</code> Pairs (bgzf) TBI, CSI <code>contacts.pairs.gz.tbi</code>, <code>contacts.pairs.gz.csi</code> FASTQ (bgzf) GZI <code>sample.fastq.bgz.gzi</code>"},{"location":"features/#usage-with-the-scanread-api","title":"Usage with the scan/read API","text":"<p>Simply use <code>.filter()</code> \u2014 predicate pushdown is enabled by default for BAM, CRAM, VCF, GFF, and Pairs:</p> <pre><code>import polars as pl\nimport polars_bio as pb\n\n# Single chromosome filter \u2014 only chr1 data is read from disk\ndf = (\n    pb.scan_bam(\"alignments.bam\")\n    .filter(pl.col(\"chrom\") == \"chr1\")\n    .collect()\n)\n\n# Multi-chromosome filter\ndf = (\n    pb.scan_vcf(\"variants.vcf.gz\")\n    .filter(pl.col(\"chrom\").is_in([\"chr21\", \"chr22\"]))\n    .collect()\n)\n\n# Region query \u2014 combines chromosome and coordinate filters\ndf = (\n    pb.scan_bam(\"alignments.bam\")\n    .filter(\n        (pl.col(\"chrom\") == \"chr1\")\n        &amp; (pl.col(\"start\") &gt;= 10000)\n        &amp; (pl.col(\"end\") &lt;= 50000)\n    )\n    .collect()\n)\n\n# CRAM with predicate pushdown\ndf = (\n    pb.scan_cram(\"alignments.cram\")\n    .filter(pl.col(\"chrom\") == \"chr1\")\n    .collect()\n)\n</code></pre> <p>Tip</p> <p>Predicate pushdown supports: equality (<code>==</code>), comparisons (<code>&gt;=</code>, <code>&lt;=</code>, <code>&gt;</code>, <code>&lt;</code>), <code>is_in()</code>, <code>is_null()</code>, <code>is_not_null()</code>, and combinations with <code>&amp;</code> (AND). Complex predicates like <code>.str.contains()</code> or OR logic are automatically filtered client-side. To disable pushdown, pass <code>predicate_pushdown=False</code>.</p>"},{"location":"features/#usage-with-the-sql-api","title":"Usage with the SQL API","text":"<p>The SQL path works automatically \u2014 DataFusion parses the WHERE clause and uses the index without any extra flags:</p> <pre><code>import polars_bio as pb\n\npb.register_bam(\"alignments.bam\", \"reads\")\n\n# Single chromosome\nresult = pb.sql(\"SELECT * FROM reads WHERE chrom = 'chr1'\").collect()\n\n# Region query\nresult = pb.sql(\n    \"SELECT * FROM reads WHERE chrom = 'chr1' AND start &gt;= 10000 AND \\\"end\\\" &lt;= 50000\"\n).collect()\n\n# Combined genomic and record filters\nresult = pb.sql(\n    \"SELECT * FROM reads WHERE chrom = 'chr1' AND mapping_quality &gt;= 30\"\n).collect()\n</code></pre>"},{"location":"features/#automatic-parallel-partitioning","title":"Automatic parallel partitioning","text":"<p>When an index file is present, DataFusion distributes genomic regions across balanced partitions using index-derived size estimates, enabling parallel execution. Formats with known contig lengths (BAM, CRAM) can split large regions into sub-regions for full parallelism even on single-chromosome queries. For FASTQ files, a GZI index alongside a BGZF-compressed file enables parallel decoding of compressed blocks. This is controlled by the global <code>target_partitions</code> setting:</p> <pre><code>import polars_bio as pb\n\npb.set_option(\"datafusion.execution.target_partitions\", \"8\")\ndf = pb.read_bam(\"large_file.bam\")  # 8 partitions will be used for parallel execution\ndf = pb.read_fastq(\"reads.fastq.bgz\")  # parallel BGZF decoding when .gzi index is present\n</code></pre> <p>Partitioning behavior (BAM, CRAM, VCF, GFF):</p> Index Available? SQL Filters Partitions Yes <code>chrom = 'chr1' AND start &gt;= 1000</code> up to target_partitions (region split into sub-regions) Yes <code>chrom IN ('chr1', 'chr2')</code> up to target_partitions (both regions split to fill bins) Yes <code>mapping_quality &gt;= 30</code> (no genomic filter) up to target_partitions (all chroms balanced + split) Yes None (full scan) up to target_partitions (all chroms balanced + split) No Any 1 (sequential full scan) <p>Partitioning behavior (FASTQ):</p> File type GZI Index? Partitions BGZF (<code>.fastq.bgz</code>) Yes (<code>.fastq.bgz.gzi</code>) up to target_partitions (parallel block decoding) BGZF (<code>.fastq.bgz</code>) No 1 (sequential read) GZIP (<code>.fastq.gz</code>) N/A 1 (sequential \u2014 GZIP cannot be parallelized) Uncompressed (<code>.fastq</code>) N/A up to target_partitions (byte-range parallel)"},{"location":"features/#record-level-filter-pushdown","title":"Record-level filter pushdown","text":"<p>All formats support record-level predicate evaluation \u2014 filters on columns like <code>mapping_quality</code>, <code>flag</code>, or <code>strand</code> are evaluated per-record during scan, with or without an index file. See the Developers Guide for the translation pipeline internals and examples.</p>"},{"location":"features/#projection-pushdown","title":"Projection pushdown","text":"<p>BAM, CRAM, VCF, and Pairs formats support parsing-level projection pushdown \u2014 unprojected fields are skipped entirely during record parsing. Enabled by default (<code>projection_pushdown=True</code>). See the Developers Guide for internals and execution plan inspection.</p>"},{"location":"features/#index-file-generation","title":"Index file generation","text":"<p>Creating index files</p> <p>Create index files using standard bioinformatics tools:</p> <pre><code># BAM: sort and index\nsamtools sort input.bam -o sorted.bam\nsamtools index sorted.bam                # creates sorted.bam.bai\n\n# CRAM: sort and index\nsamtools sort input.cram -o sorted.cram --reference ref.fa\nsamtools index sorted.cram               # creates sorted.cram.crai\n\n# VCF: sort, compress, and index\nbcftools sort input.vcf -Oz -o sorted.vcf.gz\nbcftools index -t sorted.vcf.gz          # creates sorted.vcf.gz.tbi\n\n# GFF: sort, compress, and index\n(grep \"^#\" input.gff; grep -v \"^#\" input.gff | sort -k1,1 -k4,4n) | bgzip &gt; sorted.gff.gz\ntabix -p gff sorted.gff.gz               # creates sorted.gff.gz.tbi\n\n# Pairs: sort, compress, and index (col 2=chr1, col 3=pos1)\nsort -k2,2 -k3,3n contacts.pairs | bgzip &gt; contacts.pairs.gz\ntabix -s 2 -b 3 -e 3 contacts.pairs.gz   # creates contacts.pairs.gz.tbi\n\n# FASTQ: BGZF compress and create GZI index for parallel reads\nbgzip reads.fastq                         # creates reads.fastq.bgz\nbgzip -r reads.fastq.bgz                 # creates reads.fastq.bgz.gzi\n</code></pre>"},{"location":"features/#file-output","title":"File Output","text":"<p>polars-bio supports writing DataFrames back to bioinformatic file formats. Two methods are available for each supported format:</p> <ul> <li><code>write_*</code> - Eager write that collects the DataFrame and writes it to disk, returns row count</li> <li><code>sink_*</code> - Streaming write for LazyFrames that processes data in batches without full materialization</li> </ul>"},{"location":"features/#output-format-support","title":"Output Format Support","text":"Format write_* sink_* Compression Notes VCF <code>.vcf.gz</code>, <code>.vcf.bgz</code> Auto-detected from extension BAM BGZF (built-in) Binary alignment format SAM None Plain text alignment format CRAM Built-in Requires reference FASTA FASTQ <code>.fastq.gz</code>, <code>.fastq.bgz</code> Auto-detected from extension"},{"location":"features/#basic-usage","title":"Basic Usage","text":"<pre><code>import polars_bio as pb\n\n# Read, transform, and write back\ndf = pb.read_bam(\"input.bam\", tag_fields=[\"NM\", \"AS\"])\nfiltered = df.filter(pl.col(\"mapping_quality\") &gt; 20)\npb.write_bam(filtered, \"output.bam\")\n\n# Streaming write with LazyFrame\nlf = pb.scan_vcf(\"variants.vcf\")\npb.sink_vcf(lf.filter(pl.col(\"qual\") &gt; 30), \"filtered.vcf.bgz\")\n</code></pre>"},{"location":"features/#sorted-output-with-sort_on_write","title":"Sorted Output with <code>sort_on_write</code>","text":"<p>BAM, SAM, and CRAM write functions support the <code>sort_on_write</code> parameter to produce coordinate-sorted output:</p> <pre><code>import polars_bio as pb\n\n# Write coordinate-sorted BAM\ndf = pb.read_bam(\"unsorted.bam\")\npb.write_bam(df, \"sorted.bam\", sort_on_write=True)\n\n# Streaming sorted write\nlf = pb.scan_sam(\"input.sam\")\npb.sink_bam(lf, \"sorted.bam\", sort_on_write=True)\n</code></pre> <p>When <code>sort_on_write=True</code>:</p> <ul> <li>Records are sorted by <code>(chrom ASC, start ASC)</code> during write</li> <li>Output header contains <code>@HD ... SO:coordinate</code></li> </ul> <p>When <code>sort_on_write=False</code> (default):</p> <ul> <li>Records are written in input order</li> <li>Output header contains <code>@HD ... SO:unsorted</code></li> </ul>"},{"location":"features/#cram-output","title":"CRAM Output","text":"<p>CRAM format requires a reference FASTA file for writing:</p> <pre><code>import polars_bio as pb\n\n# CRAM write requires reference_path\ndf = pb.read_cram(\"input.cram\", reference_path=\"reference.fa\")\npb.write_cram(df, \"output.cram\", reference_path=\"reference.fa\")\n\n# Streaming CRAM write\nlf = pb.scan_cram(\"input.cram\", reference_path=\"reference.fa\")\npb.sink_cram(lf, \"output.cram\", reference_path=\"reference.fa\", sort_on_write=True)\n</code></pre> <p>Warning</p> <p>The <code>reference_path</code> parameter is required for <code>write_cram()</code> and <code>sink_cram()</code>. Attempting to write CRAM without a reference will raise an error.</p>"},{"location":"features/#output-compression","title":"Output Compression","text":"<p>Output compression is auto-detected from the file extension for VCF and FASTQ formats:</p> Extension Compression <code>.vcf</code> / <code>.fastq</code> None (plain text) <code>.vcf.gz</code> / <code>.fastq.gz</code> GZIP <code>.vcf.bgz</code> / <code>.fastq.bgz</code> BGZF (block gzip) <pre><code>import polars_bio as pb\n\n# VCF\ndf = pb.read_vcf(\"variants.vcf\")\npb.write_vcf(df, \"output.vcf\")        # plain text\npb.write_vcf(df, \"output.vcf.gz\")     # GZIP\npb.write_vcf(df, \"output.vcf.bgz\")    # BGZF (recommended for indexing)\n\n# FASTQ\ndf = pb.read_fastq(\"reads.fastq\")\npb.write_fastq(df, \"output.fastq\")       # plain text\npb.write_fastq(df, \"output.fastq.gz\")    # GZIP\npb.write_fastq(df, \"output.fastq.bgz\")   # BGZF (recommended for parallel reads with GZI index)\n\n# Streaming write\nlf = pb.scan_fastq(\"large_reads.fastq.gz\")\npb.sink_fastq(lf.limit(1000), \"sample.fastq\")\n</code></pre>"},{"location":"features/#header-preservation","title":"Header Preservation","text":"<p>When reading and writing alignment files (BAM/SAM/CRAM), polars-bio preserves header metadata including:</p> <ul> <li><code>@SQ</code> (sequence dictionary)</li> <li><code>@RG</code> (read groups)</li> <li><code>@PG</code> (program records)</li> </ul> <p>This enables lossless roundtrip workflows:</p> <pre><code>import polars_bio as pb\n\n# Read with full header preservation\ndf = pb.read_bam(\"input.bam\")\n\n# Filter records\nfiltered = df.filter(pl.col(\"mapping_quality\") &gt; 20)\n\n# Write back - header metadata is preserved\npb.write_bam(filtered, \"filtered.bam\")\n</code></pre>"},{"location":"features/#polars-extension-methods","title":"Polars Extension Methods","text":"<p>Write functions are also available as Polars namespace extensions:</p> <pre><code>import polars_bio as pb\n\n# DataFrame extensions\ndf = pb.read_bam(\"input.bam\")\ndf.pb.write_bam(\"output.bam\", sort_on_write=True)\ndf.pb.write_sam(\"output.sam\")\ndf.pb.write_cram(\"output.cram\", reference_path=\"ref.fa\")\ndf.pb.write_vcf(\"output.vcf.bgz\")\ndf.pb.write_fastq(\"output.fastq.gz\")\n\n# LazyFrame extensions\nlf = pb.scan_bam(\"input.bam\")\nlf.pb.sink_bam(\"output.bam\", sort_on_write=True)\nlf.pb.sink_sam(\"output.sam\")\nlf.pb.sink_cram(\"output.cram\", reference_path=\"ref.fa\")\nlf.pb.sink_vcf(\"output.vcf.bgz\")\nlf.pb.sink_fastq(\"output.fastq.bgz\")\n</code></pre>"},{"location":"features/#sql-powered-data-processing","title":"SQL-powered data processing","text":"<p>polars-bio provides a SQL-like API for bioinformatic data querying or manipulation. Check SQL reference for more details.</p> <pre><code>import polars_bio as pb\npb.register_vcf(\"gs://gcp-public-data--gnomad/release/4.1/genome_sv/gnomad.v4.1.sv.sites.vcf.gz\", \"gnomad_sv\", info_fields=[\"SVTYPE\", \"SVLEN\"])\npb.sql(\"SELECT * FROM gnomad_sv WHERE SVTYPE = 'DEL' AND SVLEN &gt; 1000\").limit(3).collect()\n</code></pre>"},{"location":"features/#accessing-registered-tables","title":"Accessing registered tables","text":"<p>You can access registered tables programmatically using the <code>ctx.table()</code> method, which returns a DataFusion DataFrame:</p> <pre><code>import polars_bio as pb\nfrom polars_bio.context import ctx\n\n# Register a file as a table\npb.register_vcf(\"variants.vcf\", name=\"my_variants\")\n\n# Get the table as a DataFusion DataFrame\ndf = ctx.table(\"my_variants\")\n\n# Access the Arrow schema (includes coordinate system metadata)\nschema = df.schema()\nprint(schema.metadata)  # {b'bio.coordinate_system_zero_based': b'false'}\n\n# Execute queries on the DataFrame\nresult = df.filter(df[\"chrom\"] == \"chr1\").collect()\n</code></pre> <p>Tip</p> <p>The <code>ctx.table()</code> method is useful for:</p> <ol> <li>Accessing Arrow schema metadata (including coordinate system information)</li> <li>Using the DataFusion DataFrame API directly</li> <li>Integrating with other DataFusion-based tools</li> </ol>"},{"location":"features/#schema-inspection","title":"Schema Inspection","text":"<p>Quickly inspect BAM/CRAM file schemas without reading the entire file:</p> <pre><code>import polars_bio as pb\n\n# Get schema information for BAM file\nschema = pb.describe_bam(\"file.bam\")\nprint(schema)\n# shape: (11, 2)\n# \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n# \u2502 column          \u2506 datatype \u2502\n# \u2502 ---             \u2506 ---      \u2502\n# \u2502 str             \u2506 str      \u2502\n# \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n# \u2502 name            \u2506 String   \u2502\n# \u2502 chrom           \u2506 String   \u2502\n# \u2502 start           \u2506 UInt32   \u2502\n# ...\n\n# Include tag columns in schema\nschema = pb.describe_bam(\"file.bam\", tag_fields=[\"NM\", \"AS\", \"MD\"])\nprint(schema)  # Shows 14 columns including tags\n\n# CRAM schema\nschema = pb.describe_cram(\"file.cram\")\n</code></pre>"},{"location":"features/#bam-optional-tags","title":"BAM Optional Tags","text":"<p>polars-bio supports reading BAM optional alignment tags as individual columns. Tags are only parsed when explicitly requested, ensuring zero overhead for standard reads.</p> <p>Note: CRAM tag support is planned for a future release. The <code>tag_fields</code> parameter is accepted for CRAM functions but currently ignored with a warning.</p>"},{"location":"features/#usage","title":"Usage","text":"<pre><code>import polars_bio as pb\n\n# Read BAM with specific tags\ndf = pb.read_bam(\n    \"alignments.bam\",\n    tag_fields=[\"NM\", \"AS\", \"MD\"]  # Edit distance, alignment score, mismatch string\n)\n\n# Tags appear as regular columns\nprint(df.select([\"name\", \"chrom\", \"NM\", \"AS\"]))\n\n# Lazy scan with tag filtering\nlf = pb.scan_bam(\"alignments.bam\", tag_fields=[\"NM\", \"AS\"])\nhigh_quality = lf.filter((pl.col(\"NM\") &lt;= 2) &amp; (pl.col(\"AS\") &gt;= 100)).collect()\n\n# SQL queries (tags must be quoted)\npb.register_bam(\"alignments.bam\", \"reads\", tag_fields=[\"NM\", \"RG\"])\nresult = pb.sql('SELECT name, \"NM\" FROM reads WHERE \"NM\" &lt;= 2').collect()\n</code></pre>"},{"location":"features/#common-tags","title":"Common Tags","text":"<ul> <li>NM (Int32): Edit distance to reference</li> <li>MD (Utf8): Mismatch positions string</li> <li>AS (Int32): Alignment score</li> <li>XS (Int32): Secondary alignment score</li> <li>RG (Utf8): Read group identifier</li> <li>CB (Utf8): Cell barcode (single-cell)</li> <li>UB (Utf8): UMI barcode (single-cell)</li> </ul> <p>Full registry includes ~40 common SAM tags.</p>"},{"location":"features/#performance","title":"Performance","text":"<ul> <li>Zero overhead when <code>tag_fields=None</code> (default)</li> <li>Projection pushdown: only selected tags are parsed</li> <li>Tags parsed once per batch, not per record</li> </ul> <pre><code>shape: (3, 10)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 chrom \u2506 start \u2506 end   \u2506 id                             \u2506 \u2026 \u2506 qual  \u2506 filter     \u2506 svtype \u2506 svlen \u2502\n\u2502 ---   \u2506 ---   \u2506 ---   \u2506 ---                            \u2506   \u2506 ---   \u2506 ---        \u2506 ---    \u2506 ---   \u2502\n\u2502 str   \u2506 u32   \u2506 u32   \u2506 str                            \u2506   \u2506 f64   \u2506 str        \u2506 str    \u2506 i32   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 chr1  \u2506 22000 \u2506 30000 \u2506 gnomAD-SV_v3_DEL_chr1_fa103016 \u2506 \u2026 \u2506 999.0 \u2506 HIGH_NCR   \u2506 DEL    \u2506 8000  \u2502\n\u2502 chr1  \u2506 40000 \u2506 47000 \u2506 gnomAD-SV_v3_DEL_chr1_b26f63f7 \u2506 \u2026 \u2506 145.0 \u2506 PASS       \u2506 DEL    \u2506 7000  \u2502\n\u2502 chr1  \u2506 79086 \u2506 88118 \u2506 gnomAD-SV_v3_DEL_chr1_733c4ef0 \u2506 \u2026 \u2506 344.0 \u2506 UNRESOLVED \u2506 DEL    \u2506 9032  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>You can use view mechanism to create a virtual table from a DataFrame that contain preprocessing steps and reuse it in multiple steps. To avoid materializing the intermediate results in memory, you can run your processing in  streaming mode.</p>"},{"location":"features/#parallel-engine","title":"Parallel engine \ud83c\udfce\ufe0f","text":"<p>It is straightforward to parallelize operations in polars-bio. The library is built on top of Apache DataFusion  you can set the degree of parallelism using the <code>datafusion.execution.target_partitions</code> option, e.g.: </p><pre><code>import polars_bio as pb\npb.set_option(\"datafusion.execution.target_partitions\", \"8\")\n</code></pre><p></p> <p>Tip</p> <ol> <li>The default value is 1 (parallel execution disabled).</li> <li>The <code>datafusion.execution.target_partitions</code> option is a global setting and affects all operations in the current session.</li> <li>Check available strategies for optimal performance.</li> <li>See  the other configuration settings in the Apache DataFusion documentation.</li> </ol>"},{"location":"features/#cloud-storage","title":"Cloud storage \u2601\ufe0f","text":"<p>polars-bio supports direct streamed reading from cloud storages (e.g. S3, GCS) enabling processing large-scale genomics data without materializing in memory. It is built upon the OpenDAL project, a unified data access layer for cloud storage, which allows to read  bioinformatic file formats from various cloud storage providers. For Apache DataFusion native file formats, such as Parquet or CSV please refer to DataFusion user guide.</p>"},{"location":"features/#example","title":"Example","text":"<pre><code>import polars_bio as pb\n## Register VCF files from Google Cloud Storage that will be streamed - no need to download them to the local disk, size ~0.8TB\npb.register_vcf(\"gs://gcp-public-data--gnomad/release/2.1.1/liftover_grch38/vcf/genomes/gnomad.genomes.r2.1.1.sites.liftover_grch38.vcf.bgz\", \"gnomad_big\", allow_anonymous=True)\npb.register_vcf(\"gs://gcp-public-data--gnomad/release/4.1/genome_sv/gnomad.v4.1.sv.sites.vcf.gz\", \"gnomad_sv\", allow_anonymous=True)\npb.overlap(\"gnomad_sv\", \"gnomad_big\", streaming=True).sink_parquet(\"/tmp/overlap.parquet\")\n</code></pre> It is  especially useful when combined with SQL support for preprocessing and streaming processing capabilities. <p>Tip</p> <p>If you access cloud storage with authentication provided, please make sure the <code>allow_anonymous</code> parameter is set to <code>False</code> in the read/describe/register_table functions.</p>"},{"location":"features/#supported-features","title":"Supported features","text":"Feature AWS S3 Google Cloud Storage Azure Blob Storage Anonymous access Authenticated access Requester Pays Concurrent requests<sup>1</sup> Streaming reads <p>Note</p> <p><sup>1</sup>For more information on concurrent requests and block size tuning please refer to issue.</p>"},{"location":"features/#aws-s3-configuration","title":"AWS S3 configuration","text":"<p>Supported environment variables:</p> Variable Description AWS_ACCESS_KEY_ID AWS access key ID for authenticated access to S3. AWS_SECRET_ACCESS_KEY AWS secret access key for authenticated access to S3. AWS_ENDPOINT_URL Custom S3 endpoint URL for accessing S3-compatible storage. AWS_REGION  or AWS_DEFAULT_REGION AWS region for accessing S3."},{"location":"features/#google-cloud-storage-configuration","title":"Google Cloud Storage configuration","text":"<p>Supported environment variables:</p> Variable Description GOOGLE_APPLICATION_CREDENTIALS Path to the Google Cloud service account key file for authenticated access to GCS."},{"location":"features/#azure-blob-storage-configuration","title":"Azure Blob Storage configuration","text":"<p>Supported environment variables:</p> Variable Description AZURE_STORAGE_ACCOUNT Azure Storage account name for authenticated access to Azure Blob Storage. AZURE_STORAGE_KEY Azure Storage account key for authenticated access to Azure Blob Storage. AZURE_ENDPOINT_URL Azure Blob Storage endpoint URL for accessing Azure Blob Storage."},{"location":"features/#streaming","title":"Streaming \ud83d\ude82","text":"<p>polars-bio supports out-of-core processing with Apache DataFusion async streams and Polars LazyFrame streaming option. It can bring  significant speedup as well reduction in memory usage allowing to process large datasets that do not fit in memory. See our benchmark results. There are 2 ways of using streaming mode:</p> <ol> <li> <p>By setting the <code>output_type</code> to <code>datafusion.DataFrame</code> and using the Python DataFrame API, including methods such as count, write_parquet or write_csv or write_json. In this option you completely bypass the polars streaming engine.</p> <p><code>python import polars_bio as pb import polars as pl pb.overlap(\"/tmp/gnomad.v4.1.sv.sites.parquet\", \"/tmp/gnomad.exomes.v4.1.sites.chr1.parquet\", output_type=\"datafusion.DataFrame\").write_parquet(\"/tmp/overlap.parquet\") pl.scan_parquet(\"/tmp/overlap.parquet\").collect().count()</code> </p><pre><code> shape: (1, 6)\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 chrom_1    \u2506 start_1    \u2506 end_1      \u2506 chrom_2    \u2506 start_2    \u2506 end_2      \u2502\n \u2502 ---        \u2506 ---        \u2506 ---        \u2506 ---        \u2506 ---        \u2506 ---        \u2502\n \u2502 u32        \u2506 u32        \u2506 u32        \u2506 u32        \u2506 u32        \u2506 u32        \u2502\n \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n \u2502 2629727337 \u2506 2629727337 \u2506 2629727337 \u2506 2629727337 \u2506 2629727337 \u2506 2629727337 \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre><p></p> <p>Tip</p> <p>If you only need to write the results as fast as possible into one of the above file formats or quickly get the row count, then it is in the most cases the best option.</p> </li> <li> <p>Using polars new streaming engine:</p> <pre><code>import os\nimport polars_bio as pb\nos.environ['BENCH_DATA_ROOT'] = \"/Users/mwiewior/research/data/databio\"\nos.environ['POLARS_VERBOSE'] = \"1\"\n\ncols=[\"contig\", \"pos_start\", \"pos_end\"]\nBENCH_DATA_ROOT = os.getenv('BENCH_DATA_ROOT', '/data/bench_data/databio')\ndf_1 = f\"{BENCH_DATA_ROOT}/exons/*.parquet\"\ndf_2 =  f\"{BENCH_DATA_ROOT}/exons/*.parquet\"\npb.overlap(df_1, df_2, cols1=cols, cols2=cols).collect(engine=\"streaming\").limit()\n</code></pre> <pre><code>1652814rows [00:00, 20208793.67rows/s]\n[MultiScanState]: Readers disconnected\npolars-stream: done running graph phase\npolars-stream: updating graph state\nshape: (5, 6)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 contig_1 \u2506 pos_start_1 \u2506 pos_end_1 \u2506 contig_2 \u2506 pos_start_2 \u2506 pos_end_2 \u2502\n\u2502 ---      \u2506 ---         \u2506 ---       \u2506 ---      \u2506 ---         \u2506 ---       \u2502\n\u2502 str      \u2506 i32         \u2506 i32       \u2506 str      \u2506 i32         \u2506 i32       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 chr1     \u2506 11873       \u2506 12227     \u2506 chr1     \u2506 11873       \u2506 12227     \u2502\n\u2502 chr1     \u2506 12612       \u2506 12721     \u2506 chr1     \u2506 12612       \u2506 12721     \u2502\n\u2502 chr1     \u2506 13220       \u2506 14409     \u2506 chr1     \u2506 13220       \u2506 14409     \u2502\n\u2502 chr1     \u2506 13220       \u2506 14409     \u2506 chr1     \u2506 14361       \u2506 14829     \u2502\n\u2502 chr1     \u2506 14361       \u2506 14829     \u2506 chr1     \u2506 13220       \u2506 14409     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> </li> </ol> <p>Parallellism can be controlled using the <code>datafusion.execution.target_partitions</code>option as described in the parallel engine section (compare the row/s metric in the following examples).</p> <p></p><pre><code> pb.set_option(\"datafusion.execution.target_partitions\", \"1\")\n pb.overlap(df_1, df_2, cols1=cols, cols2=cols).collect(engine=\"streaming\").count()\n</code></pre> <pre><code> 1652814rows [00:00, 19664163.99rows/s]\n shape: (1, 6)\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 contig_1 \u2506 pos_start_1 \u2506 pos_end_1 \u2506 contig_2 \u2506 pos_start_2 \u2506 pos_end_2 \u2502\n \u2502 ---      \u2506 ---         \u2506 ---       \u2506 ---      \u2506 ---         \u2506 ---       \u2502\n \u2502 u32      \u2506 u32         \u2506 u32       \u2506 u32      \u2506 u32         \u2506 u32       \u2502\n \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n \u2502 1652814  \u2506 1652814     \u2506 1652814   \u2506 1652814  \u2506 1652814     \u2506 1652814   \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <pre><code> pb.set_option(\"datafusion.execution.target_partitions\", \"2\")\n pb.overlap(df_1, df_2, cols1=cols, cols2=cols).collect(engine=\"streaming\").count()\n</code></pre><p></p> <pre><code> 1652814rows [00:00, 27841987.75rows/s]\n shape: (1, 6)\n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n \u2502 contig_1 \u2506 pos_start_1 \u2506 pos_end_1 \u2506 contig_2 \u2506 pos_start_2 \u2506 pos_end_2 \u2502\n \u2502 ---      \u2506 ---         \u2506 ---       \u2506 ---      \u2506 ---         \u2506 ---       \u2502\n \u2502 u32      \u2506 u32         \u2506 u32       \u2506 u32      \u2506 u32         \u2506 u32       \u2502\n \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n \u2502 1652814  \u2506 1652814     \u2506 1652814   \u2506 1652814  \u2506 1652814     \u2506 1652814   \u2502\n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"features/#compression","title":"Compression","text":"<p>polars-bio supports GZIP (default file extension <code>*.gz</code>) and Block GZIP (BGZIP, default file extension <code>*.bgz</code>) when reading files from local and cloud storages. For BGZIP-compressed FASTQ files, parallel decoding of compressed blocks is automatic \u2014 see Automatic parallel partitioning and Index file generation for details. Please take a look at the following GitHub discussion.</p>"},{"location":"features/#dataframes-support","title":"DataFrames support","text":"I/O Bioframe polars-bio PyRanges Pybedtools GenomicRanges Pandas DataFrame Polars DataFrame Polars LazyFrame Native readers"},{"location":"features/#polars-integration","title":"Polars Integration","text":"<p>polars-bio leverages deep integration with Polars through the Arrow C Data Interface, enabling high-performance zero-copy data exchange between Polars LazyFrames and the Rust-based genomic range operations engine. Requires Polars &gt;= 1.37.0.</p> <p>See the Developers Guide for architecture diagrams, performance details, and batch size configuration.</p>"},{"location":"performance/","title":"Results summary \ud83d\udcc8","text":""},{"location":"performance/#results-summary","title":"Results summary \ud83d\udcc8","text":""},{"location":"performance/#current-performance","title":"Current Performance \ud83d\udd25","text":"<p>Latest Benchmark Comparison: View Interactive Performance Report</p> <p>The benchmark comparison shows performance across three key operations (overlap, nearest, count_overlaps) comparing polars-bio against alternative tools (pyranges1, genomicranges, bioframe). Updated automatically on each release.</p>"},{"location":"performance/#single-thread-performance","title":"Single-thread performance \ud83c\udfc3\u200d","text":""},{"location":"performance/#parallel-performance","title":"Parallel performance \ud83c\udfc3\u200d\ud83c\udfc3\u200d","text":""},{"location":"performance/#benchmarks","title":"Benchmarks \ud83e\uddea","text":""},{"location":"performance/#detailed-results-shortcuts","title":"Detailed results shortcuts \ud83d\udc68\u200d\ud83d\udd2c","text":"<ul> <li>Binary operations</li> <li>Parallel execution and scalability</li> <li>Memory characteristics</li> <li>DataFrame formats performance</li> </ul>"},{"location":"performance/#test-datasets","title":"Test datasets \ud83d\uddc3\ufe0f","text":"<p>AIList dataset was used for benchmarking.</p> Dataset# Name Size(x1000) Non-flatness 0 chainRn4 2,351 6 1 fBrain 199 1 2 exons 439 2 3 chainOrnAna1 1,957 6 4 chainVicPac2 7,684 8 5 chainXenTro3Link 50,981 7 6 chainMonDom5Link 128,187 7 7 ex-anno 1,194 2 8 ex-rna 9,945 7 <p>Note</p> <p>Test dataset in Parquet format can be downloaded from:</p> <ul> <li>for single-thread tests</li> <li>for parallel tests (8 partitions per dataset)</li> </ul>"},{"location":"performance/#test-libraries","title":"Test libraries \ud83d\udcda","text":"<ul> <li>Bioframe-0.7.2</li> <li>PyRanges0-0.0.132</li> <li>PyRanges1-e634a11</li> <li>pybedtools-0.10.0</li> <li>PyGenomics-0.1.1</li> <li>GenomicRanges-0.5.0</li> </ul> <p>Note</p> <p>Some tests were not conducted for all libraries in case of poor performance of specific tools, e.g. <code>pybedtools</code>, <code>PyGenomics</code> and <code>GenomicRanges</code> for the largest outputs.</p>"},{"location":"performance/#output-compatibility","title":"Output compatibility \ud83d\udda5\ufe0f","text":"<p>See API comparison for more details on parameters used in the benchmark.</p>"},{"location":"performance/#binary-operations","title":"Binary operations","text":""},{"location":"performance/#overlap-operation","title":"Overlap operation","text":"<p>Test cases were categorized based on the size \ud83d\udc55 of the input datasets and the expected output size into the following groups:</p> <ul> <li>S-size: output &lt; 1,000,000</li> <li>M-size: 1,000,000 &lt; output &lt; 100,000,000</li> <li>L-size: 100,000,000 &lt; output &lt; 1,000,000,000</li> <li>XL-size: output &gt; 1,000,000,000</li> </ul> <p>Tip</p> <ol> <li>Naming convention for the test cases is as follows <code>test-case-size (dataset-1-id, dataset-2-id)</code>, e.g.: <code>S-size (1-2)</code>, where <code>1</code> and <code>2</code> are the indices of the datasets used in the test case.</li> <li>In the case of all but polars-bio native reader the reported timings exclude the time to read the data from disk and do the required preprocessing (e.g. Python object creation) and column mappings.</li> </ol>"},{"location":"performance/#apple-silicon-macos","title":"Apple Silicon (macOS) \ud83c\udf4e","text":"<p>Here is the configuration of the Apple Silicon machine used for benchmarking:</p> <ul> <li>cpu architecture: <code>arm64</code></li> <li>cpu name: <code>Apple M3 Max</code></li> <li>cpu cores: <code>16</code></li> <li>memory: <code>64 GB</code></li> <li>kernel: <code>Darwin Kernel Version 24.2.0: Fri Dec  6 19:02:12 PST 2024; root:xnu-11215.61.5~2/RELEASE_ARM64_T6031</code></li> <li>system: <code>Darwin</code></li> <li>os-release: <code>macOS-15.2-arm64-arm-64bit</code></li> <li>python: <code>3.12.4</code></li> <li>polars-bio: <code>0.3.0</code></li> </ul>"},{"location":"performance/#s-size","title":"S-size","text":""},{"location":"performance/#s-size-1-2","title":"S-size (1-2)","text":"<p>Output size: 54,246</p> Library Min (s) Max (s) Mean (s) Speedup bioframe 0.100738 0.101541 0.101119 0.25x polars_bio 0.032156 0.035501 0.033394 0.77x pyranges0 0.024100 0.028271 0.025589 1.00x pyranges1 0.053770 0.054647 0.054121 0.47x pybedtools0 0.281969 0.283385 0.282857 0.09x pygenomics 1.424975 1.436369 1.430531 0.02x genomicranges 0.972717 0.979013 0.975761 0.03x"},{"location":"performance/#s-size-2-7","title":"S-size (2-7)","text":"<p>Output size: 273,500</p> Library Min (s) Max (s) Mean (s) Speedup bioframe 0.298039 0.309271 0.302905 0.30x polars_bio 0.089324 0.092200 0.090332 1.00x pyranges0 0.096478 0.103456 0.101023 0.89x pyranges1 0.195621 0.198025 0.197146 0.46x pybedtools0 1.004577 1.013097 1.007701 0.09x pygenomics 4.264575 4.275965 4.269055 0.02x genomicranges 2.919675 2.926785 2.923549 0.03x"},{"location":"performance/#s-size-1-0","title":"S-size (1-0)","text":"<p>Output size: 320,955</p> Library Min (s) Max (s) Mean (s) Speedup bioframe 0.521093 0.549674 0.534084 0.28x polars_bio 0.135411 0.168570 0.147222 1.00x pyranges0 0.271539 0.282081 0.276298 0.53x pyranges1 0.418972 0.426373 0.422060 0.35x pybedtools0 1.258828 1.269215 1.264674 0.12x pygenomics 7.877381 7.908531 7.894108 0.02x genomicranges 4.222082 4.266592 4.244865 0.03x"},{"location":"performance/#m-size","title":"M-size","text":""},{"location":"performance/#m-size-7-0","title":"M-size (7-0)","text":"<p>Output size: 2,761,621</p> Library Min (s) Max (s) Mean (s) Speedup bioframe 0.935141 0.990710 0.970345 0.22x polars_bio 0.213880 0.220151 0.216288 1.00x pyranges0 0.408637 0.434262 0.422380 0.51x pyranges1 0.632015 0.642214 0.635670 0.34x pybedtools0 6.415976 6.467304 6.444268 0.03x pygenomics 9.588232 9.705035 9.653368 0.02x genomicranges 9.017886 9.058916 9.033964 0.02x"},{"location":"performance/#m-size-7-3","title":"M-size (7-3)","text":"<p>Output size: 4,408,383</p> Library Min (s) Max (s) Mean (s) Speedup bioframe 0.954765 0.969307 0.959704 0.21x polars_bio 0.198607 0.208906 0.203033 1.00x pyranges0 0.425277 0.430527 0.428594 0.47x pyranges1 0.696934 0.710050 0.702206 0.29x pybedtools0 9.403818 9.491574 9.453402 0.02x pygenomics 8.638968 8.662197 8.647764 0.02x genomicranges 10.514233 10.556004 10.540377 0.02x"},{"location":"performance/#l-size","title":"L-size","text":""},{"location":"performance/#l-size-0-8","title":"L-size (0-8)","text":"<p>Output size: 164,196,784</p> Library Min (s) Max (s) Mean (s) Speedup bioframe 15.630508 16.719793 16.080009 0.19x polars_bio 2.882900 3.135100 2.997755 1.00x pyranges0 9.276095 10.158109 9.761880 0.31x pyranges1 13.076820 13.510234 13.329948 0.22x pybedtools0 322.922915 335.123071 329.659142 0.01x pygenomics 128.849536 132.109689 130.089096 0.02x genomicranges 234.237435 239.315157 236.504565 0.01x"},{"location":"performance/#l-size-4-8","title":"L-size (4-8)","text":"<p>Output size: 227,832,153</p> Library Min (s) Max (s) Mean (s) Speedup bioframe 22.911206 23.118100 23.030572 0.16x polars_bio 3.541325 3.937760 3.684317 1.00x pyranges0 13.035069 13.510203 13.225005 0.28x pyranges1 20.924921 21.657297 21.398281 0.17x pybedtools0 505.897157 521.239276 511.310686 0.01x pygenomics 159.883847 160.942329 160.306970 0.02x genomicranges 322.217280 322.490391 322.371662 0.01x"},{"location":"performance/#l-size-7-8","title":"L-size (7-8)","text":"<p>Output size: 307,184,634</p> Library Min (s) Max (s) Mean (s) Speedup bioframe 29.128664 29.993182 29.518215 0.12x polars_bio 3.260438 3.897260 3.489278 1.00x pyranges0 16.615283 16.983202 16.753369 0.21x pyranges1 30.504657 30.912445 30.752887 0.11x pybedtools0 555.480532 559.947421 556.986772 0.01x pygenomics 156.724420 157.321514 156.935424 0.02x genomicranges 416.095573 417.284236 416.700000 0.01x"},{"location":"performance/#xl-size","title":"XL-size","text":""},{"location":"performance/#xl-size-3-0","title":"XL-size (3-0)","text":"<p>Output size: 1,086,692,495</p> Library Min (s) Max (s) Mean (s) Speedup bioframe 124.244987 126.569689 125.435831 0.12x polars_bio 12.650240 15.858913 14.776997 1.00x pyranges0 85.652054 94.383934 88.712706 0.17x pyranges1 92.802026 94.400313 93.447716 0.16x"},{"location":"performance/#amd-genoa-linux","title":"AMD Genoa (Linux) \ud83d\udc27","text":"<p>c3d-highmem-8 machine was used for benchmarking.</p> <ul> <li>cpu architecture: <code>x86_64</code></li> <li>cpu name: <code>AMD EPYC 9B14</code></li> <li>cpu cores: <code>4</code></li> <li>memory: <code>63 GB</code></li> <li>kernel: <code>#22~22.04.1-Ubuntu SMP Mon Dec  9 20:42:57 UTC 2024</code></li> <li>system: <code>Linux</code></li> <li>os-release: <code>Linux-6.8.0-1020-gcp-x86_64-with-glibc2.35</code></li> <li>python: <code>3.12.8</code></li> <li>polars-bio: <code>0.3.0</code></li> </ul>"},{"location":"performance/#s-size_1","title":"S-size","text":""},{"location":"performance/#s-size-1-2_1","title":"S-size (1-2)","text":"<p>Output size: 54,246</p> Library Min (s) Max (s) Mean (s) Speedup bioframe 0.094509 0.095311 0.094797 0.61x polars_bio 0.058527 0.066444 0.061503 0.95x pyranges0 0.057583 0.059461 0.058245 1.00x pyranges1 0.098868 0.107992 0.101964 0.57x pybedtools0 0.382701 0.384930 0.383619 0.15x pygenomics 2.335400 2.340616 2.338876 0.02x genomicranges 1.648289 1.663941 1.657652 0.04x"},{"location":"performance/#s-size-2-7_1","title":"S-size (2-7)","text":"<p>Output size: 273,500</p> Library Min (s) Max (s) Mean (s) Speedup bioframe 0.273727 0.275239 0.274383 0.60x polars_bio 0.161882 0.164253 0.163334 1.00x pyranges0 0.169721 0.171931 0.170678 0.96x pyranges1 0.304432 0.323747 0.311284 0.52x pybedtools0 1.477541 1.478301 1.477841 0.11x pygenomics 6.929725 6.932875 6.931662 0.02x genomicranges 5.096514 5.105638 5.100280 0.03x"},{"location":"performance/#s-size-1-0_1","title":"S-size (1-0)","text":"<p>Output size: 320,955</p> Library Min (s) Max (s) Mean (s) Speedup bioframe 0.457869 0.460473 0.459397 0.55x polars_bio 0.251083 0.252582 0.251673 1.00x pyranges0 0.365083 0.376212 0.369148 0.68x pyranges1 0.593858 0.605304 0.600537 0.42x pybedtools0 1.834958 1.858740 1.844379 0.14x pygenomics 12.730241 12.771149 12.756920 0.02x genomicranges 7.090998 7.121029 7.107298 0.04x"},{"location":"performance/#m-size_1","title":"M-size","text":""},{"location":"performance/#m-size-7-0_1","title":"M-size (7-0)","text":"<p>Output size: 2,761,621</p> Library Min (s) Max (s) Mean (s) Speedup bioframe 0.873343 0.875288 0.874457 0.50x polars_bio 0.420260 0.450565 0.433827 1.00x pyranges0 0.559251 0.564516 0.561273 0.77x pyranges1 1.876350 1.888463 1.880867 0.23x pybedtools0 10.379844 10.430488 10.404292 0.04x pygenomics 15.553783 15.567857 15.562953 0.03x genomicranges 15.517461 15.548186 15.535206 0.03x"},{"location":"performance/#m-size-7-3_1","title":"M-size (7-3)","text":"<p>Output size: 4,408,383</p> Library Min (s) Max (s) Mean (s) Speedup bioframe 1.022998 1.028002 1.024980 0.40x polars_bio 0.397203 0.426743 0.412704 1.00x pyranges0 0.590809 0.602570 0.594928 0.69x pyranges1 2.027123 2.074861 2.045372 0.20x pybedtools0 15.957823 16.006681 15.988963 0.03x pygenomics 13.983596 13.994300 13.990662 0.03x genomicranges 18.602139 18.625446 18.615777 0.02x"},{"location":"performance/#l-size_1","title":"L-size","text":""},{"location":"performance/#l-size-0-8_1","title":"L-size (0-8)","text":"<p>Output size: 164,196,784</p> Library Min (s) Max (s) Mean (s) Speedup bioframe 21.459718 21.516023 21.480410 0.29x polars_bio 5.713430 6.952107 6.129996 1.00x pyranges0 15.898455 16.227408 16.011707 0.38x pyranges1 21.721230 22.272518 21.917855 0.28x pybedtools0 575.612739 578.021023 577.165597 0.01x pygenomics 244.510614 245.508453 245.063967 0.03x genomicranges 440.650408 440.737924 440.706206 0.01x"},{"location":"performance/#l-size-4-8_1","title":"L-size (4-8)","text":"<p>Output size: 227,832,153</p> Library Min (s) Max (s) Mean (s) Speedup bioframe 29.460466 29.864740 29.633731 0.34x polars_bio 9.731893 10.180046 9.968996 1.00x pyranges0 21.637592 22.724399 22.011753 0.45x pyranges1 37.035666 37.531010 37.218867 0.27x"},{"location":"performance/#l-size-7-8_1","title":"L-size (7-8)","text":"<p>Output size: 307,184,634</p> Library Min (s) Max (s) Mean (s) Speedup bioframe 38.547761 38.593432 38.573512 0.18x polars_bio 6.356472 8.204682 6.980182 1.00x pyranges0 28.664496 28.878972 28.751498 0.24x pyranges1 80.373241 80.871479 80.546908 0.09x"},{"location":"performance/#intel-emerald-rapids-linux","title":"Intel Emerald Rapids (Linux) \ud83d\udc27","text":"<p>c4-highmem-8 machine was used for benchmarking.</p> <ul> <li>cpu architecture: <code>x86_64</code></li> <li>cpu name: <code>INTEL(R) XEON(R) PLATINUM 8581C CPU @ 2.30GHz</code></li> <li>cpu cores: <code>22</code></li> <li>memory: <code>86 GB</code></li> <li>kernel: <code>#27~22.04.1-Ubuntu SMP Tue Jul 16 23:03:39 UTC 2024</code></li> <li>system: <code>Linux</code></li> <li>os-release: <code>Linux-6.5.0-1025-gcp-x86_64-with-glibc2.35</code></li> <li>python: <code>3.12.8</code></li> <li>polars-bio: <code>0.3.0</code></li> </ul>"},{"location":"performance/#s-size_2","title":"S-size","text":""},{"location":"performance/#s-size-1-2_2","title":"S-size (1-2)","text":"<p>Output size: 54,246</p> Library Min (s) Max (s) Mean (s) Speedup bioframe 0.080274 0.083350 0.082125 0.67x polars_bio 0.051923 0.060853 0.055115 1.00x pyranges0 0.057737 0.063692 0.060233 0.92x pyranges1 0.092273 0.104232 0.096598 0.57x pybedtools0 0.342928 0.350446 0.345739 0.16x pygenomics 1.933479 1.980263 1.958915 0.03x genomicranges 1.317808 1.365975 1.345268 0.04x"},{"location":"performance/#s-size-2-7_2","title":"S-size (2-7)","text":"<p>Output size: 273,500</p> Library Min (s) Max (s) Mean (s) Speedup bioframe 0.242910 0.250233 0.246872 0.59x polars_bio 0.142933 0.151324 0.146654 1.00x pyranges0 0.181919 0.184524 0.183063 0.80x pyranges1 0.303359 0.305036 0.304166 0.48x pybedtools0 1.303765 1.318575 1.310322 0.11x pygenomics 5.744573 5.917737 5.816145 0.03x genomicranges 4.202981 4.298941 4.243175 0.03x"},{"location":"performance/#s-size-1-0_2","title":"S-size (1-0)","text":"<p>Output: 320,955</p> Library Min (s) Max (s) Mean (s) Speedup bioframe 0.421461 0.449266 0.434152 0.53x polars_bio 0.228252 0.233000 0.230004 1.00x pyranges0 0.383663 0.401601 0.391000 0.59x pyranges1 0.563753 0.575554 0.570290 0.40x pybedtools0 1.617740 1.643310 1.631340 0.14x pygenomics 10.491757 10.753130 10.636810 0.02x genomicranges 5.806456 5.880285 5.851234 0.04x"},{"location":"performance/#m-size_2","title":"M-size","text":""},{"location":"performance/#m-size-7-0_2","title":"M-size (7-0)","text":"<p>Output: 2,761,621</p> Library Min (s) Max (s) Mean (s) Speedup bioframe 0.900843 0.928098 0.917930 0.43x polars_bio 0.380828 0.408791 0.390157 1.00x pyranges0 0.580401 0.607483 0.595004 0.66x pyranges1 1.697365 1.705109 1.699965 0.23x pybedtools0 9.120270 9.384526 9.211789 0.04x pygenomics 13.123205 13.179993 13.160740 0.03x genomicranges 13.230635 13.690668 13.472020 0.03x"},{"location":"performance/#m-size-7-3_2","title":"M-size (7-3)","text":"<p>Output: 4,408,383</p> Library Min (s) Max (s) Mean (s) Speedup bioframe 1.137155 1.142985 1.140749 0.35x polars_bio 0.382198 0.411443 0.396179 1.00x pyranges0 0.650236 0.675971 0.659619 0.60x pyranges1 1.818395 1.841851 1.826528 0.22x pybedtools0 14.588216 14.666769 14.621019 0.03x pygenomics 11.975859 12.196851 12.121281 0.03x genomicranges 15.640415 15.839974 15.736289 0.03x"},{"location":"performance/#l-size_2","title":"L-size","text":""},{"location":"performance/#l-size-0-8_2","title":"L-size (0-8)","text":"<p>Output: 164,196,784</p> Library Min (s) Max (s) Mean (s) Speedup bioframe 28.818453 28.956365 28.884398 0.21x polars_bio 5.904987 6.562457 6.145784 1.00x pyranges0 22.664353 22.997717 22.806512 0.27x pyranges1 24.446387 24.804753 24.613135 0.25x"},{"location":"performance/#l-size-4-8_2","title":"L-size (4-8)","text":"<p>Output: 227,832,153</p> Library Min (s) Max (s) Mean (s) Speedup bioframe 39.868340 40.109302 39.951601 0.25x polars_bio 9.736690 10.277895 10.021107 1.00x pyranges0 31.146222 31.290984 31.208499 0.32x pyranges1 39.407547 40.279563 39.843926 0.25x"},{"location":"performance/#l-size-7-8_2","title":"L-size (7-8)","text":"<p>Output: 307,184,634</p> Library Min (s) Max (s) Mean (s) Speedup bioframe 51.923368 52.840132 52.354141 0.14x polars_bio 6.604371 7.975253 7.151908 1.00x pyranges0 41.702499 42.557826 42.027393 0.17x pyranges1 63.524302 63.774618 63.679367 0.11x"},{"location":"performance/#sorted-input","title":"Sorted input","text":"<p>Todo</p> <ul> <li>Add sorted input benchmarks</li> </ul>"},{"location":"performance/#nearest-closest-operation","title":"Nearest (closest) operation","text":""},{"location":"performance/#apple-silicon-macos_1","title":"Apple Silicon (macOS) \ud83c\udf4e","text":""},{"location":"performance/#s-size_3","title":"S-size","text":""},{"location":"performance/#s-size-1-2_3","title":"S-size (1-2)","text":"Library Min (s) Max (s) Mean (s) Speedup bioframe 0.282320 0.288275 0.285267 0.31x polars_bio 0.085046 0.091221 0.087545 1.00x pyranges0 0.131831 0.134894 0.132961 0.66x pyranges1 0.174185 0.176994 0.175650 0.50x pybedtools0 0.639068 0.648982 0.644444 0.14x"},{"location":"performance/#s-size-2-7_3","title":"S-size (2-7)","text":"Library Min (s) Max (s) Mean (s) Speedup bioframe 0.819279 0.829933 0.826124 0.27x polars_bio 0.219446 0.222345 0.220642 1.00x pyranges0 0.336059 0.346129 0.339557 0.65x pyranges1 0.415821 0.425321 0.420848 0.52x pybedtools0 1.477262 1.490676 1.483696 0.15x"},{"location":"performance/#s-size-1-0_3","title":"S-size (1-0)","text":"Library Min (s) Max (s) Mean (s) Speedup bioframe 1.130541 1.140416 1.134000 0.18x polars_bio 0.196717 0.208593 0.204053 1.00x pyranges0 0.705755 0.734622 0.720525 0.28x pyranges1 0.764694 0.848379 0.803320 0.25x pybedtools0 1.054003 1.106122 1.075032 0.19x"},{"location":"performance/#m-size_3","title":"M-size","text":""},{"location":"performance/#m-size-7-0_3","title":"M-size (7-0)","text":"Library Min (s) Max (s) Mean (s) Speedup bioframe 2.477785 2.568602 2.509461 0.17x polars_bio 0.428317 0.444993 0.435540 1.00x pyranges0 0.776533 0.816372 0.795476 0.55x pyranges1 0.944443 0.956939 0.952353 0.46x pybedtools0 3.891626 3.920097 3.907743 0.11x"},{"location":"performance/#m-size-7-3_3","title":"M-size (7-3)","text":"Library Min (s) Max (s) Mean (s) Speedup bioframe 2.753566 2.862580 2.797397 0.17x polars_bio 0.456776 0.473078 0.465653 1.00x pyranges0 0.763046 0.791106 0.773746 0.60x pyranges1 0.915549 0.943690 0.931994 0.50x pybedtools0 3.781775 3.803629 3.794066 0.12x"},{"location":"performance/#l-size_3","title":"L-size","text":""},{"location":"performance/#l-size-0-8_3","title":"L-size (0-8)","text":"Library Min (s) Max (s) Mean (s) Speedup bioframe 48.211329 50.000878 49.136208 0.03x polars_bio 1.493048 1.620652 1.552847 1.00x pyranges0 3.082013 3.146659 3.116663 0.50x pyranges1 3.662140 3.706852 3.684952 0.42x pybedtools0 10.561658 10.661184 10.603403 0.15x"},{"location":"performance/#l-size-4-8_3","title":"L-size (4-8)","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 1.778845 1.806916 1.793131 1.00x pyranges0 4.083200 4.223863 4.162773 0.43x pyranges1 5.263221 5.281766 5.274634 0.34x pybedtools0 25.670818 25.789135 25.725079 0.07x"},{"location":"performance/#l-size-7-8_3","title":"L-size (7-8)","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 1.002583 1.018811 1.009884 1.00x pyranges0 2.700747 2.722101 2.712181 0.37x pyranges1 2.988758 3.026965 3.008430 0.34x pybedtools0 9.403173 9.474385 9.441718 0.11x"},{"location":"performance/#l-size-3-0","title":"L-size (3-0)","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.917404 0.924649 0.921255 0.73x pyranges0 0.648998 0.701870 0.669507 1.00x pyranges1 0.945559 0.962450 0.956495 0.70x pybedtools0 18.643435 18.860937 18.717684 0.04x"},{"location":"performance/#intel-emerald-rapids-linux_1","title":"Intel Emerald Rapids (Linux) \ud83d\udc27","text":""},{"location":"performance/#s-size_4","title":"S-size","text":""},{"location":"performance/#s-size-1-2_4","title":"S-size (1-2)","text":"Library Min (s) Max (s) Mean (s) Speedup bioframe 0.166667 0.170252 0.167967 0.64x polars_bio 0.059757 0.200437 0.106731 1.00x pyranges0 0.179466 0.193530 0.187714 0.57x pyranges1 0.242658 0.249828 0.245899 0.43x pybedtools0 0.917019 0.931643 0.926433 0.12x"},{"location":"performance/#s-size-2-7_4","title":"S-size (2-7)","text":"Library Min (s) Max (s) Mean (s) Speedup bioframe 0.422708 0.429339 0.426257 0.48x polars_bio 0.198758 0.213887 0.205798 1.00x pyranges0 0.416234 0.421807 0.418619 0.49x pyranges1 0.531085 0.541683 0.536459 0.38x pybedtools0 2.085108 2.086168 2.085780 0.10x"},{"location":"performance/#s-size-1-0_4","title":"S-size (1-0)","text":"Library Min (s) Max (s) Mean (s) Speedup bioframe 0.563159 0.578794 0.572864 0.33x polars_bio 0.175278 0.206305 0.189767 1.00x pyranges0 0.683342 0.692020 0.687795 0.28x pyranges1 0.747615 0.754387 0.750464 0.25x pybedtools0 1.177251 1.192054 1.185509 0.16x"},{"location":"performance/#m-size_4","title":"M-size","text":""},{"location":"performance/#m-size-7-0_4","title":"M-size (7-0)","text":"Library Min (s) Max (s) Mean (s) Speedup bioframe 1.200956 1.222905 1.210387 0.32x polars_bio 0.380992 0.388604 0.383601 1.00x pyranges0 0.811919 0.826886 0.821876 0.47x pyranges1 1.001142 1.039321 1.025727 0.37x pybedtools0 5.498016 5.507600 5.503549 0.07x"},{"location":"performance/#m-size-7-3_4","title":"M-size (7-3)","text":"Library Min (s) Max (s) Mean (s) Speedup bioframe 1.254999 1.262944 1.259257 0.29x polars_bio 0.359952 0.380151 0.367453 1.00x pyranges0 0.772943 0.777694 0.775242 0.47x pyranges1 0.969036 0.976941 0.972361 0.38x pybedtools0 5.394985 5.454826 5.431693 0.07x"},{"location":"performance/#l-size_4","title":"L-size","text":""},{"location":"performance/#l-size-0-8_4","title":"L-size (0-8)","text":"Library Min (s) Max (s) Mean (s) Speedup bioframe 21.325827 22.295620 21.664544 0.10x polars_bio 2.031079 2.123994 2.062699 1.00x pyranges0 4.146689 4.172250 4.161514 0.50x pyranges1 4.658245 5.014545 4.811681 0.43x pybedtools0 16.245615 16.445935 16.373377 0.13x"},{"location":"performance/#l-size-4-8_4","title":"L-size (4-8)","text":"Library Min (s) Max (s) Mean (s) Speedup bioframe 29.156254 30.051563 29.682944 0.10x polars_bio 3.013091 3.045008 3.032196 1.00x pyranges0 5.610373 5.752500 5.693492 0.53x pyranges1 7.469939 7.503416 7.486032 0.41x pybedtools0 39.047774 39.824156 39.436474 0.08x"},{"location":"performance/#l-size-7-8_4","title":"L-size (7-8)","text":"Library Min (s) Max (s) Mean (s) Speedup bioframe 39.984802 40.155487 40.065113 0.05x polars_bio 1.868242 1.878759 1.873989 1.00x pyranges0 4.016616 4.034881 4.028447 0.47x pyranges1 4.251595 4.516436 4.341912 0.43x pybedtools0 15.481021 15.533977 15.501447 0.12x"},{"location":"performance/#l-size-3-0_1","title":"L-size (3-0)","text":"Library Min (s) Max (s) Mean (s) Speedup bioframe 85.826377 86.023302 85.933492 0.01x polars_bio 1.093852 1.129010 1.113706 0.79x pyranges0 0.871503 0.903123 0.883908 1.00x pyranges1 1.151087 1.200064 1.168257 0.76x pybedtools0 28.309991 28.624311 28.421618 0.03x"},{"location":"performance/#xl-size_1","title":"XL-size","text":""},{"location":"performance/#xl-size-0-4","title":"XL-size (0-4)","text":"Library Min (s) Max (s) Mean (s) Speedup bioframe 365.207028 365.455848 365.327366 0.01x polars_bio 3.553054 3.568899 3.559842 0.81x pyranges0 2.867332 2.888607 2.879132 1.00x pyranges1 3.145913 3.294060 3.227092 0.89x pybedtools0 126.338178 127.104972 126.646993 0.02x"},{"location":"performance/#xl-size-0-5","title":"XL-size (0-5)","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 15.814259 15.907476 15.845883 1.00x pyranges0 20.752479 20.837782 20.797340 0.76x pyranges1 20.315355 20.490498 20.376382 0.78x"},{"location":"performance/#parallel-execution-and-scalability","title":"Parallel execution and scalability","text":"<p>Apple Silicon and c4-standard-32 machine were used for benchmarking.</p> <ul> <li>cpu architecture: <code>x86_64</code></li> <li>cpu name: <code>INTEL(R) XEON(R) PLATINUM 8581C CPU @ 2.30GHz</code></li> <li>cpu cores: <code>16</code></li> <li>memory: <code>118 GB</code></li> <li>kernel: <code>#27~22.04.1-Ubuntu SMP Tue Jul 16 23:03:39 UTC 2024</code></li> <li>system: <code>Linux</code></li> <li>os-release: <code>Linux-6.5.0-1025-gcp-x86_64-with-glibc2.35</code></li> <li>python: <code>3.12.8</code></li> <li>polars-bio: <code>0.3.0</code></li> </ul> <p>Two strategies were used for parallel execution (<code>n</code> - degree of parallelism):</p> <ul> <li> <p><code>polars_bio-n</code>: Default, dynamic partitioning schema (median of 2 partitions/dataset) with repartitioning in DataFusion on Parquet scan and join operations: </p><pre><code>import polars_bio as pb\npb.set_option(\"datafusion.optimizer.repartition_joins\", \"true\")\npb.set_option(\"datafusion.optimizer.repartition_file_scans\", \"true\")\npb.set_option(\"datafusion.execution.coalesce_batches\", \"false\")\n</code></pre> the <code>single-thread</code> dataset was used (see Test datasets)<p></p> </li> <li> <p><code>polars_bio-n-p</code>: Custom partitioning schema (constant number of 8 partitions/dataset) without any repartitioning in DataFusion: </p><pre><code>import polars_bio as pb\npb.set_option(\"datafusion.optimizer.repartition_joins\", \"false\")\npb.set_option(\"datafusion.optimizer.repartition_file_scans\", \"false\")\npb.set_option(\"datafusion.execution.coalesce_batches\", \"false\")\n</code></pre> the <code>parallel</code> dataset was used (see Test datasets)<p></p> </li> </ul>"},{"location":"performance/#overlap-operation_1","title":"Overlap operation","text":""},{"location":"performance/#apple-silicon-macos_2","title":"Apple Silicon (macOS) \ud83c\udf4e","text":""},{"location":"performance/#0-8","title":"0-8","text":"Library Min (s) Max (s) Mean (s) Speedup pyranges0-1 9.146743 10.067171 9.512946 0.31x pyranges1-1 17.084293 17.394639 17.207398 0.17x polars_bio-1 2.784917 3.184688 2.963876 1.00x polars_bio-2 1.447746 2.194926 1.716935 1.73x polars_bio-4 1.023359 1.031373 1.027862 2.88x polars_bio-8 0.745024 0.766747 0.757039 3.92x -------------- ----------- ----------- ----------- ----------- polars_bio-1-p 3.106839 3.335528 3.221183 1.00x polars_bio-2-p 1.610237 1.643171 1.626704 1.98x polars_bio-4-p 0.947643 0.948290 0.947967 3.40x polars_bio-8-p 0.579332 0.585496 0.582414 5.53x"},{"location":"performance/#7-8","title":"7-8","text":"Library Min (s) Max (s) Mean (s) Speedup pyranges0-1 16.015978 16.471722 16.183480 0.24x pyranges1-1 30.504657 30.912445 30.752887 0.13x polars_bio-1 3.582070 4.331780 3.930337 1.00x polars_bio-2 1.798026 1.866828 1.829596 2.15x polars_bio-4 1.126025 1.135795 1.132349 3.47x polars_bio-8 0.703821 0.707697 0.705424 5.57x -------------- ----------- ----------- ----------- ----------- polars_bio-1-p 3.862783 4.572159 4.217471 1.00x polars_bio-2-p 2.003091 2.006744 2.004917 2.10x polars_bio-4-p 1.081358 1.097517 1.089438 3.87x polars_bio-8-p 0.652152 0.653111 0.652631 6.46x"},{"location":"performance/#2-5","title":"2-5","text":"Library Min (s) Max (s) Mean (s) Speedup pyranges0-1 7.359118 8.248676 7.747425 0.42x pyranges1-1 12.017050 12.394313 12.172057 0.26x polars_bio-1 3.168840 3.279239 3.221157 1.00x polars_bio-2 1.807984 1.896123 1.840286 1.75x polars_bio-4 1.565173 1.855352 1.667814 1.93x polars_bio-8 1.328546 1.644594 1.525987 2.11x -------------- ---------- ---------- ---------- --------- polars_bio-1-p 4.691770 4.771827 4.731799 1.00x polars_bio-2-p 2.498744 2.529678 2.514211 1.88x polars_bio-4-p 1.350183 1.360860 1.355522 3.49x polars_bio-8-p 0.746751 0.746974 0.746863 6.34x"},{"location":"performance/#3-0","title":"3-0","text":"Library Min (s) Max (s) Mean (s) Speedup pyranges0-1 78.437583 80.667906 79.691000 0.16x pyranges1-1 149.301588 150.696560 150.214238 0.08x polars_bio-1 8.699317 15.876707 12.749627 1.00x polars_bio-2 7.107510 11.556344 8.876861 1.44x polars_bio-4 6.368686 6.746370 6.558874 1.94x polars_bio-8 5.673492 6.341975 6.052686 2.11x -------------- ----------- ----------- ----------- --------- polars_bio-1-p 16.687511 19.602739 18.145125 1.00x polars_bio-2-p 9.592687 11.276648 10.434667 1.74x polars_bio-4-p 6.641342 6.712883 6.677113 2.72x polars_bio-8-p 5.365854 5.920471 5.643162 3.22x"},{"location":"performance/#intel-emerald-rapids-linux_2","title":"Intel Emerald Rapids (Linux) \ud83d\udc27","text":""},{"location":"performance/#0-8_1","title":"0-8","text":"Library Min (s) Max (s) Mean (s) Speedup pyranges0-1 22.427066 23.052440 22.637076 0.29x pyranges1-1 35.304058 35.420546 35.342961 0.18x polars_bio-1 5.664570 7.867539 6.508315 1.00x polars_bio-2 3.485226 3.621209 3.564963 1.83x polars_bio-4 2.262268 2.320929 2.287944 2.84x polars_bio-8 1.287120 1.311752 1.297510 5.02x --------------- ----------- ----------- ----------- ----------- polars_bio-1-p 5.825176 7.007628 6.252962 1.00x polars_bio-2-p 2.976644 3.025504 2.995113 2.09x polars_bio-4-p 1.622611 1.690197 1.650458 3.79x polars_bio-8-p 1.009492 1.046739 1.025886 6.10x"},{"location":"performance/#7-8_1","title":"7-8","text":"Library Min (s) Max (s) Mean (s) Speedup pyranges0-1 41.676284 42.974216 42.485708 0.18x pyranges1-1 63.524302 63.774618 63.679367 0.12x polars_bio-1 6.510632 9.640636 7.619978 1.00x polars_bio-2 4.063316 4.558028 4.316856 1.77x polars_bio-4 3.006938 3.116209 3.053199 2.50x polars_bio-8 1.733345 1.782316 1.752699 4.35x --------------- ----------- ----------- ----------- ----------- polars_bio-1-p 6.666566 8.707044 7.364003 1.00x polars_bio-2-p 3.314721 3.438854 3.396223 2.17x polars_bio-4-p 1.755033 1.766559 1.760501 4.18x polars_bio-8-p 1.023208 1.055603 1.035804 7.11x"},{"location":"performance/#2-5_1","title":"2-5","text":"Library Min (s) Max (s) Mean (s) Speedup pyranges0-1 15.954215 17.879185 16.597961 0.32x pyranges1-1 22.822777 23.005675 22.899582 0.23x polars_bio-1 5.096198 5.652478 5.296669 1.00x polars_bio-2 3.216712 3.390962 3.296900 1.61x polars_bio-4 2.941997 3.078842 3.004835 1.76x polars_bio-8 2.373662 2.483793 2.423432 2.19x --------------- ----------- ----------- ----------- ----------- polars_bio-1-p 6.835908 8.957041 7.564713 1.00x polars_bio-2-p 3.406529 3.430781 3.416292 2.21x polars_bio-4-p 1.815182 1.942866 1.872337 4.04x polars_bio-8-p 1.026042 1.065482 1.039835 7.27x"},{"location":"performance/#3-0_1","title":"3-0","text":"Library Min (s) Max (s) Mean (s) Speedup pyranges0-1 158.193622 159.014103 158.563798 0.17x pyranges1-1 OOM OOM OOM OOM polars_bio-1 26.957168 27.577231 27.316310 1.00x polars_bio-2 19.440849 19.631778 19.567068 1.40x polars_bio-4 16.432316 16.657353 16.570284 1.65x polars_bio-8 12.845359 13.136680 12.951113 2.11x -------------- ------------ ------------ ------------ ----------- polars_bio-1-p 34.869014 35.937479 35.249302 1.00x polars_bio-2-p 20.142638 20.460018 20.338156 1.73x polars_bio-4-p 11.641085 11.721084 11.672311 3.02x polars_bio-8-p 7.169329 7.339832 7.258024 4.86x"},{"location":"performance/#nearest-operation","title":"Nearest operation","text":""},{"location":"performance/#apple-silicon-macos_3","title":"Apple Silicon (macOS) \ud83c\udf4e","text":""},{"location":"performance/#7-8_2","title":"7-8","text":"Library Min (s) Max (s) Mean (s) Speedup pyranges0-1 2.617098 2.632619 2.626817 0.39x pyranges1-1 2.928071 2.952393 2.942198 0.35x polars_bio-1 0.999672 1.038078 1.016733 1.00x polars_bio-2 0.624798 0.630383 0.627723 1.62x polars_bio-4 0.412480 0.422400 0.417233 2.44x polars_bio-8 0.286680 0.291932 0.290088 3.50x -------------- ---------- ---------- ---------- --------- polars_bio-1-p 1.450476 1.478374 1.466073 1.00x polars_bio-2-p 1.013673 1.029715 1.019719 1.44x polars_bio-4-p 0.812852 0.816052 0.814196 1.80x polars_bio-8-p 0.715727 0.726820 0.722109 2.03x"},{"location":"performance/#0-8_2","title":"0-8","text":"Library Min (s) Max (s) Mean (s) Speedup pyranges0-1 2.727178 2.788261 2.748268 0.39x pyranges1-1 3.193758 3.290031 3.238138 0.33x polars_bio-1 1.050470 1.111362 1.071476 1.00x polars_bio-2 0.603192 0.614378 0.607069 1.76x polars_bio-4 0.477346 0.484047 0.480698 2.23x polars_bio-8 0.371300 0.377496 0.374637 2.86x -------------- ---------- ---------- ---------- --------- polars_bio-1-p 1.459614 1.499385 1.485763 1.00x polars_bio-2-p 1.031476 1.032248 1.031815 1.44x polars_bio-4-p 0.813042 0.818795 0.815014 1.82x polars_bio-8-p 0.727105 0.735477 0.730848 2.03x"},{"location":"performance/#0-3","title":"0-3","text":"Library Min (s) Max (s) Mean (s) Speedup pyranges0-1 0.605024 0.655083 0.628258 1.45x pyranges1-1 0.892005 0.899846 0.895358 1.01x polars_bio-1 0.901617 0.919927 0.908753 1.00x polars_bio-2 0.565947 0.570143 0.568091 1.60x polars_bio-4 0.486143 0.487667 0.486668 1.87x polars_bio-8 0.404343 0.408482 0.406183 2.24x -------------- ---------- ---------- ---------- --------- polars_bio-1-p 1.934625 1.983807 1.962461 1.00x polars_bio-2-p 1.063823 1.069980 1.067726 1.84x polars_bio-4-p 0.618193 0.620918 0.619740 3.17x polars_bio-8-p 0.378345 0.381553 0.380137 5.16x"},{"location":"performance/#2-5_2","title":"2-5","text":"Library Min (s) Max (s) Mean (s) Speedup pyranges0-1 11.466866 11.731607 11.589608 0.16x pyranges1-1 12.607366 12.686471 12.659969 0.15x polars_bio-1 1.785067 1.984576 1.859738 1.00x polars_bio-2 1.550936 1.616606 1.576498 1.18x polars_bio-4 1.293685 1.354271 1.327746 1.40x polars_bio-8 1.229453 1.255038 1.244184 1.49x -------------- ---------- ---------- ---------- --------- polars_bio-1-p 4.099759 4.148830 4.119466 1.00x polars_bio-2-p 3.899533 3.927669 3.910608 1.05x polars_bio-4-p 3.730015 3.764352 3.748710 1.10x polars_bio-8-p 3.740107 3.776072 3.758196 1.10x"},{"location":"performance/#intel-emerald-rapids-linux_3","title":"Intel Emerald Rapids (Linux) \ud83d\udc27","text":""},{"location":"performance/#7-8_3","title":"7-8","text":"Library Min (s) Max (s) Mean (s) Speedup pyranges0-1 4.206312 4.294343 4.241896 0.45x pyranges1-1 4.442628 4.488366 4.458621 0.43x polars_bio-1 1.891354 1.948671 1.912490 1.00x polars_bio-2 1.283955 1.302282 1.295635 1.48x polars_bio-4 1.078061 1.105893 1.094149 1.75x polars_bio-8 0.712460 0.752014 0.727148 2.63x --------------- ---------- ---------- ---------- --------- polars_bio-1-p 2.828171 2.931478 2.890248 1.00x polars_bio-2-p 1.916772 1.936575 1.927385 1.50x polars_bio-4-p 1.456353 1.481438 1.472393 1.96x polars_bio-8-p 1.295097 1.350529 1.315073 2.20x"},{"location":"performance/#0-8_3","title":"0-8","text":"Library Min (s) Max (s) Mean (s) Speedup pyranges0-1 4.231424 4.330897 4.274404 0.48x pyranges1-1 4.763702 4.832694 4.797875 0.43x polars_bio-1 2.041052 2.054510 2.048951 1.00x polars_bio-2 1.427670 1.462461 1.445207 1.42x polars_bio-4 1.133872 1.182288 1.150375 1.78x polars_bio-8 0.743850 0.786324 0.769738 2.66x --------------- ---------- ---------- ---------- --------- polars_bio-1-p 2.757630 2.817896 2.794401 1.00x polars_bio-2-p 1.912476 1.934266 1.926969 1.45x polars_bio-4-p 1.483118 1.571922 1.534062 1.82x polars_bio-8-p 1.345707 1.363905 1.356738 2.06x"},{"location":"performance/#0-3_1","title":"0-3","text":"Library Min (s) Max (s) Mean (s) Speedup pyranges0-1 0.929409 1.043995 0.989172 1.29x pyranges1-1 1.232229 1.277243 1.250441 1.02x polars_bio-1 1.121446 1.560456 1.272248 1.00x polars_bio-2 0.790184 0.798372 0.795170 1.60x polars_bio-4 0.645712 0.662949 0.652850 1.95x polars_bio-8 0.473802 0.505282 0.489050 2.60x --------------- ---------- ---------- ---------- --------- polars_bio-1-p 2.863790 3.048004 2.927503 1.00x polars_bio-2-p 1.545225 1.580199 1.560213 1.88x polars_bio-4-p 0.921394 0.944576 0.934756 3.13x polars_bio-8-p 0.625656 0.637595 0.632294 4.63x"},{"location":"performance/#2-5_3","title":"2-5","text":"Library Min (s) Max (s) Mean (s) Speedup pyranges0-1 21.188286 21.349862 21.267732 0.24x pyranges1-1 19.789758 20.092196 19.913107 0.26x polars_bio-1 5.085885 5.158123 5.120825 1.00x polars_bio-2 4.084694 4.328842 4.207706 1.22x polars_bio-4 3.570746 3.885584 3.752306 1.36x polars_bio-8 3.250405 3.413984 3.329296 1.54x --------------- ---------- ---------- ---------- --------- polars_bio-1-p 8.974429 9.101872 9.048213 1.00x polars_bio-2-p 8.297582 8.385287 8.339353 1.09x polars_bio-4-p 8.007553 8.099914 8.053293 1.12x polars_bio-8-p 7.851029 8.051397 7.939732 1.14x polars_bio-16-p 7.746810 7.985732 7.882904 1.15x"},{"location":"performance/#dataframes-comparison","title":"DataFrames comparison","text":"<p>Note</p> <p>In the following benchmarks we compared the perfoemance of Python DataFrames libraries in the following scenarios:</p> <ul> <li><code>polars_bio</code>: native Rust Parquet (default) reader and Polars LazyFrame (default) as an output.</li> <li><code>polars_bio_pandas_lf</code>: Pandas DataFrames as an input and Polars LazyFrame as an output.</li> <li><code>polars_bio_pandas_pd</code>: Pandas DataFrames as an input and Pandas DataFrame as an output.</li> <li><code>polars_bio_polars_eager</code>: Polars DataFrames as an input and Polars LazyFrame as an output.</li> <li><code>polars_bio_polars_lazy</code>: Polars LazyFrames as an input and Polars LazyFrame as an output.</li> </ul>"},{"location":"performance/#apple-silicon-macos_4","title":"Apple Silicon (macOS) \ud83c\udf4e","text":""},{"location":"performance/#s-size_5","title":"S-size","text":""},{"location":"performance/#1-2","title":"1-2","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.035567 0.036777 0.035995 0.91x polars_bio_pandas_lf 0.040237 0.041256 0.040694 0.80x polars_bio_pandas_pd 0.040554 0.040888 0.040761 0.80x polars_bio_polars_eager 0.032051 0.033022 0.032693 1.00x polars_bio_polars_lazy 0.034346 0.035225 0.034775 0.94x"},{"location":"performance/#2-7","title":"2-7","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.094768 0.096217 0.095266 1.00x polars_bio_pandas_lf 0.163054 0.164207 0.163713 0.58x polars_bio_pandas_pd 0.163245 0.166200 0.165022 0.58x polars_bio_polars_eager 0.142344 0.145895 0.144110 0.66x polars_bio_polars_lazy 0.149738 0.150299 0.149929 0.64x"},{"location":"performance/#1-0","title":"1-0","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.145564 0.151407 0.147679 1.00x polars_bio_pandas_lf 0.238292 0.240374 0.239504 0.62x polars_bio_pandas_pd 0.239330 0.252445 0.244414 0.60x polars_bio_polars_eager 0.208421 0.214513 0.210896 0.70x polars_bio_polars_lazy 0.219629 0.222126 0.220908 0.67x"},{"location":"performance/#m-size_5","title":"M-size","text":""},{"location":"performance/#7-0","title":"7-0","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.224327 0.227891 0.225606 1.00x polars_bio_pandas_lf 0.377938 0.378380 0.378205 0.60x polars_bio_pandas_pd 0.413825 0.415470 0.414630 0.54x polars_bio_polars_eager 0.332434 0.335960 0.334393 0.67x polars_bio_polars_lazy 0.347608 0.350382 0.349330 0.65x"},{"location":"performance/#7-3","title":"7-3","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.206701 0.217080 0.210280 1.00x polars_bio_pandas_lf 0.345310 0.355560 0.349561 0.60x polars_bio_pandas_pd 0.415459 0.417442 0.416609 0.50x polars_bio_polars_eager 0.311204 0.313540 0.312487 0.67x polars_bio_polars_lazy 0.321170 0.322826 0.321981 0.65x"},{"location":"performance/#l-size_5","title":"L-size","text":""},{"location":"performance/#0-8_4","title":"0-8","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 2.750666 2.895516 2.802942 1.00x polars_bio_pandas_lf 3.525844 3.646709 3.592018 0.78x polars_bio_pandas_pd 6.455399 6.539737 6.487919 0.43x polars_bio_polars_eager 3.236083 3.428796 3.331644 0.84x polars_bio_polars_lazy 3.220374 3.251365 3.232736 0.87x"},{"location":"performance/#4-8","title":"4-8","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 3.677363 3.877014 3.749576 1.00x polars_bio_pandas_lf 4.875777 5.007774 4.953983 0.76x polars_bio_pandas_pd 8.595318 8.809947 8.704564 0.43x polars_bio_polars_eager 4.473527 4.608746 4.561838 0.82x polars_bio_polars_lazy 4.728077 4.786690 4.758805 0.79x"},{"location":"performance/#7-8_4","title":"7-8","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 3.439489 3.917193 3.633215 1.00x polars_bio_pandas_lf 3.930340 4.079147 3.985301 0.91x polars_bio_pandas_pd 9.646125 9.994008 9.798255 0.37x polars_bio_polars_eager 3.742098 3.995767 3.832054 0.95x polars_bio_polars_lazy 3.767904 4.058453 3.882342 0.94x Source Peak Memory (MB)) Factor polars_bio 14,671 1.0x polars_bio_pandas_pd 22,589 1.54x polars_bio_pandas_eager 23,681 1.61x"},{"location":"performance/#memory-characteristic-polars_bio","title":"Memory characteristic polars_bio","text":""},{"location":"performance/#memory-characteristic-polars_bio_pandas_pd","title":"Memory characteristic polars_bio_pandas_pd","text":""},{"location":"performance/#memory-characteristic-polars_bio_polars_eager","title":"Memory characteristic polars_bio_polars_eager","text":""},{"location":"performance/#intel-emerald-rapids-linux_4","title":"Intel Emerald Rapids (Linux) \ud83d\udc27","text":""},{"location":"performance/#s-size_6","title":"S-size","text":""},{"location":"performance/#1-2_1","title":"1-2","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.044786 0.051431 0.047491 0.75x polars_bio_pandas_lf 0.049655 0.053397 0.051123 0.70x polars_bio_pandas_pd 0.049221 0.049408 0.049292 0.73x polars_bio_polars_eager 0.035443 0.037997 0.036327 0.99x polars_bio_polars_lazy 0.035665 0.036124 0.035831 1.00x"},{"location":"performance/#2-7_1","title":"2-7","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.121025 0.125696 0.122869 0.88x polars_bio_pandas_lf 0.136112 0.146342 0.141704 0.76x polars_bio_pandas_pd 0.136125 0.137920 0.137167 0.79x polars_bio_polars_eager 0.106999 0.111473 0.108813 0.99x polars_bio_polars_lazy 0.107742 0.108038 0.107885 1.00x"},{"location":"performance/#1-0_1","title":"1-0","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.180773 0.185704 0.183153 0.91x polars_bio_pandas_lf 0.210633 0.217342 0.213262 0.78x polars_bio_pandas_pd 0.211245 0.211972 0.211680 0.79x polars_bio_polars_eager 0.166366 0.169000 0.167294 1.00x polars_bio_polars_lazy 0.166566 0.167847 0.167033 1.00x"},{"location":"performance/#m-size_6","title":"M-size","text":""},{"location":"performance/#7-0_1","title":"7-0","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.306130 0.314524 0.309803 1.00x polars_bio_pandas_lf 0.416123 0.432397 0.422839 0.73x polars_bio_pandas_pd 0.410937 0.414566 0.412503 0.75x polars_bio_polars_eager 0.353321 0.364626 0.358433 0.86x polars_bio_polars_lazy 0.355099 0.359842 0.357666 0.87x"},{"location":"performance/#7-3_1","title":"7-3","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.283038 0.292933 0.288120 1.00x polars_bio_pandas_lf 0.496220 0.510444 0.502504 0.57x polars_bio_pandas_pd 0.495243 0.498179 0.497064 0.58x polars_bio_polars_eager 0.446789 0.455552 0.450521 0.64x polars_bio_polars_lazy 0.450512 0.456530 0.453931 0.63x"},{"location":"performance/#l-size_6","title":"L-size","text":""},{"location":"performance/#0-8_5","title":"0-8","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 4.519114 4.539367 4.532138 1.00x polars_bio_pandas_lf 12.710922 12.805014 12.751166 0.36x polars_bio_pandas_pd 12.699757 12.820158 12.759016 0.36x polars_bio_polars_eager 12.455788 12.555952 12.501217 0.36x polars_bio_polars_lazy 12.536595 12.579006 12.561026 0.36x"},{"location":"performance/#4-8_1","title":"4-8","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 6.043839 6.129112 6.088359 1.00x polars_bio_pandas_lf 16.528438 16.674857 16.605654 0.37x polars_bio_pandas_pd 16.575829 16.643302 16.600709 0.37x polars_bio_polars_eager 16.177433 16.185123 16.180217 0.38x polars_bio_polars_lazy 16.214009 16.395757 16.281422 0.37x"},{"location":"performance/#7-8_5","title":"7-8","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 4.600700 4.806215 4.689799 1.00x polars_bio_pandas_lf 19.858977 20.342740 20.104499 0.23x polars_bio_pandas_pd 20.263301 20.594552 20.402049 0.23x polars_bio_polars_eager 19.837098 20.012580 19.922743 0.24x polars_bio_polars_lazy 19.803839 19.818197 19.813257 0.24x"},{"location":"performance/#parallel-execution","title":"Parallel execution","text":""},{"location":"performance/#apple-silicon-macos_5","title":"Apple Silicon (macOS) \ud83c\udf4e","text":""},{"location":"performance/#7-8_6","title":"7-8","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio-1 2.809130 3.025777 2.892455 1.00x polars_bio_pandas_lf-1 6.113947 6.343946 6.200402 0.47x polars_bio_pandas_pd-1 7.101394 7.133803 7.121356 0.41x polars_bio_polars_eager-1 6.083699 6.269502 6.188576 0.47x polars_bio_polars_lazy-1 6.282692 6.359252 6.328352 0.46x polars_bio-2 1.382220 1.489515 1.435259 2.02x polars_bio_pandas_lf-2 4.088560 4.139376 4.107673 0.70x polars_bio_pandas_pd-2 5.591844 5.612861 5.603072 0.52x polars_bio_polars_eager-2 3.995305 4.058894 4.020753 0.72x polars_bio_polars_lazy-2 3.961027 4.047943 4.001683 0.72x polars_bio-4 1.017920 1.077169 1.041166 2.78x polars_bio_pandas_lf-4 3.084727 3.126532 3.102596 0.93x polars_bio_pandas_pd-4 4.925954 4.957415 4.943877 0.59x polars_bio_polars_eager-4 2.914283 2.971686 2.935221 0.99x polars_bio_polars_lazy-4 2.918793 2.944206 2.930453 0.99x polars_bio-8 0.688430 0.751968 0.711147 4.07x polars_bio_pandas_lf-8 2.558258 2.606939 2.588139 1.12x polars_bio_pandas_pd-8 4.448074 4.490866 4.474837 0.65x polars_bio_polars_eager-8 2.398288 2.513702 2.446019 1.18x polars_bio_polars_lazy-8 2.406907 2.418571 2.411573 1.20x"},{"location":"performance/#memory-characteristics","title":"Memory characteristics","text":"<p>How to run the benchmarks with memory-profiler: </p><pre><code>(polars-bio-py3.12) \u279c  polars-bio git:(master) \u2717 pip list | grep memory\nmemory-profiler            0.61.0\n\nmprof run --include-children benchmark/src/memory/mem_xxx.py\nmprof plot mprofile_xxx.dat\n</code></pre><p></p> <p>Tip</p> <ol> <li>Here we report end-to-end time, i.e. including reading and writing to a file and all the required operations in between, such as data transformation, Python object creation, etc.</li> </ol>"},{"location":"performance/#apple-silicon-macos_6","title":"Apple Silicon (macOS) \ud83c\udf4e","text":""},{"location":"performance/#read-parquet-files-and-count-overlaps-7-8","title":"Read Parquet files and count overlaps  7-8","text":"Library Peak Memory (MB) Factor polars-bio 14,650 1.0x bioframe 35,720 2.43x pyranges0 30,140 2.06x pyranges1 35,940 2.45x"},{"location":"performance/#polars-bio","title":"polars-bio","text":""},{"location":"performance/#bioframe","title":"bioframe","text":""},{"location":"performance/#pyranges0","title":"pyranges0","text":""},{"location":"performance/#pyranges1","title":"pyranges1","text":""},{"location":"performance/#calculate-overlaps-and-export-to-a-csv-file-7-8","title":"Calculate overlaps and export to a CSV file 7-8","text":"Library Time (s) Speedup Peak Memory (MB) Factor polars-bio 23.765 0.77x 14,660 26.07x polars-bio-stream 18.221<sup>1</sup> 1.0x 562.22 1.0x bioframe 370.010 0.05x 33,352 59.32x pyranges0 275.237 0.07x 30.052 53.45x pyranges1 351.041 0.05x 36,530 64.97x <p><sup>1</sup> Despite limiting the number of threads in DataFusion (<code>datafusion.execution.target_partitions=1</code>) and in Polars (<code>POLARS_MAX_THREADS=1</code>) cpu utilization was constant and approx.160%.</p>"},{"location":"performance/#polars-bio_1","title":"polars-bio","text":""},{"location":"performance/#polars-bio_stream","title":"polars-bio_stream","text":""},{"location":"performance/#bioframe_1","title":"bioframe","text":""},{"location":"performance/#pyranges0_1","title":"pyranges0","text":""},{"location":"performance/#pyranges1_1","title":"pyranges1","text":""},{"location":"performance/#how-to-run-the-benchmarks","title":"How to run the benchmarks","text":"<p>Check the repository for more details on how to run the benchmarks. ```</p> <p>Todo</p> <ul> <li>Add more details on how to run the benchmarks</li> </ul>"},{"location":"quickstart/","title":"\ud83c\udfc3\ud83c\udffc\u200d\u2642\ufe0f Quick start","text":"<p>polars-bio is available on PyPI and can be installed with pip: </p><pre><code>pip install polars-bio\n</code></pre> To enable support for Pandas DataFrames, install the <code>pandas</code> extra: <pre><code>pip install polars-bio[pandas]\n</code></pre> For visualization features, which depend on <code>bioframe</code> and <code>matplotlib</code>, install the <code>viz</code> extra: <pre><code>pip install polars-bio[viz]\n</code></pre> There are binary versions for Linux (x86_64), MacOS (x86_64 and arm64) and Windows (x86_64). In case of other platforms (or errors indicating incompatibilites between Python's ABI), it is fairly easy to build polars-bio from source with poetry and maturin: <pre><code>git clone https://github.com/biodatageeks/polars-bio.git\ncd polars-bio\npoetry env use 3.12\npoetry update\nRUSTFLAGS=\"-Ctarget-cpu=native\" maturin build --release -m Cargo.toml\n</code></pre> and you should see the following output: <pre><code>Compiling polars_bio v0.22.0 (/Users/mwiewior/research/git/polars-bio)\nFinished `release` profile [optimized] target(s) in 1m 25s\n\ud83d\udce6 Built wheel for abi3 Python \u2265 3.8 to /Users/mwiewior/research/git/polars-bio/target/wheels/polars_bio-0.22.0-cp38-abi3-macosx_11_0_arm64.whl\n</code></pre> and finally install the package with pip: <pre><code>pip install /Users/mwiewior/research/git/polars-bio/target/wheels/polars_bio-0.10.3-cp38-abi3-macosx_11_0_arm64.whl\n</code></pre><p></p> <p>Tip</p> <p>Required dependencies:</p> <ul> <li>Python&gt;=3.10&lt;3.15 (3.12 or 3.13 are recommended, 3.14 is experimental),</li> <li>poetry</li> <li>cmake,</li> <li>Rust compiler</li> <li>Cargo are required to build the package from source. rustup is the recommended way to install Rust.</li> </ul> <pre><code>import polars_bio as pb\npb.read_vcf(\"gs://gcp-public-data--gnomad/release/4.1/genome_sv/gnomad.v4.1.sv.sites.vcf.gz\", compression_type=\"bgz\").limit(3).collect()\n</code></pre> <pre><code>shape: (3, 8)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 chrom \u2506 start \u2506 end    \u2506 id                             \u2506 ref \u2506 alt   \u2506 qual  \u2506 filter              \u2502\n\u2502 ---   \u2506 ---   \u2506 ---    \u2506 ---                            \u2506 --- \u2506 ---   \u2506 ---   \u2506 ---                 \u2502\n\u2502 str   \u2506 u32   \u2506 u32    \u2506 str                            \u2506 str \u2506 str   \u2506 f64   \u2506 str                 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 chr1  \u2506 10000 \u2506 295666 \u2506 gnomAD-SV_v3_DUP_chr1_01c2781c \u2506 N   \u2506 &lt;DUP&gt; \u2506 134.0 \u2506 HIGH_NCR            \u2502\n\u2502 chr1  \u2506 10434 \u2506 10434  \u2506 gnomAD-SV_v3_BND_chr1_1a45f73a \u2506 N   \u2506 &lt;BND&gt; \u2506 260.0 \u2506 HIGH_NCR;UNRESOLVED \u2502\n\u2502 chr1  \u2506 10440 \u2506 10440  \u2506 gnomAD-SV_v3_BND_chr1_3fa36917 \u2506 N   \u2506 &lt;BND&gt; \u2506 198.0 \u2506 HIGH_NCR;UNRESOLVED \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>If you see the above output, you have successfully installed polars-bio and can start using it. Please refer to the Tutorial and API documentation for more details on how to use the library.</p>"},{"location":"supplement/","title":"\ud83d\ude80 Performance","text":""},{"location":"supplement/#benchmark-setup","title":"Benchmark setup","text":""},{"location":"supplement/#code-and-benchmarking-scenarios","title":"Code and  benchmarking scenarios","text":"<p>Repository</p>"},{"location":"supplement/#operating-systems-and-hardware-configurations","title":"Operating systems and hardware configurations","text":""},{"location":"supplement/#macos","title":"macOS","text":"<ul> <li>cpu architecture: <code>arm64</code></li> <li>cpu name: <code>Apple M3 Max</code></li> <li>cpu cores: <code>16</code></li> <li>memory: <code>64 GB</code></li> <li>kernel: <code>Darwin Kernel Version 24.2.0: Fri Dec  6 19:02:12 PST 2024; root:xnu-11215.61.5~2/RELEASE_ARM64_T6031</code></li> <li>system: <code>Darwin</code></li> <li>os-release: <code>macOS-15.2-arm64-arm-64bit</code></li> <li>python: <code>3.12.4</code></li> <li>polars-bio: <code>0.8.3</code></li> </ul>"},{"location":"supplement/#linux","title":"Linux","text":"<p>c3-standard-22 machine was used for benchmarking.</p> <ul> <li>cpu architecture: <code>x86_64</code></li> <li>cpu name: <code>Intel(R) Xeon(R) Platinum 8481C CPU @ 2.70GHz</code></li> <li>cpu cores: <code>22</code></li> <li>memory: <code>88 GB</code></li> <li>kernel: <code>Linux-6.8.0-1025-gcp-x86_64-with-glibc2.35</code></li> <li>system: <code>Linux</code></li> <li>os-release: <code>#27~22.04.1-Ubuntu SMP Mon Feb 24 16:42:24 UTC 2025</code></li> <li>python: <code>3.12.8</code></li> <li>polars-bio: <code>0.8.3</code></li> </ul>"},{"location":"supplement/#software","title":"Software","text":"<ul> <li>Bioframe-0.7.2</li> <li>PyRanges0-0.0.132</li> <li>PyRanges1-e634a11</li> <li>pybedtools-0.10.0</li> <li>PyGenomics-0.1.1</li> <li>GenomicRanges-0.5.0</li> </ul>"},{"location":"supplement/#data","title":"Data","text":""},{"location":"supplement/#real-dataset","title":"Real dataset","text":"<p>The AIList dataset after transcoding into the Parquet file format (with the Snappy compression) was used for benchmarking. This dataset was published with the AIList paper:</p> <p>Jianglin Feng , Aakrosh Ratan , Nathan C Sheffield, Augmented Interval List: a novel data structure for efficient genomic interval search, Bioinformatics 2019.</p> Dataset# Name Size(x1000) Description 0 chainRn4 2,351 Source 1 fBrain 199 Source 2 exons 439 Dataset used in the BEDTools tutorial. 3 chainOrnAna1 1,957 Source 4 chainVicPac2 7,684 Source 5 chainXenTro3Link 50,981 Source 6 chainMonDom5Link 128,187 Source 7 ex-anno 1,194 Dataset contains GenCode annotations with ~1.2 million lines, mixing all types of features. 8 ex-rna 9,945 Dataset contains ~10 million direct-RNA mappings. <p>Source: AIList Github</p> Rank Dataset 1 Dataset 2 # of overlaps 1 chainMonDom5Link chainXenTro3Link 416,157,506,000 2 chainMonDom5Link chainVicPac2 248,984,248,721 3 chainVicPac2 chainXenTro3Link 117,131,343,532 4 chainMonDom5Link chainOrnAna1 52,992,648,116 5 chainMonDom5Link chainRn4 27,741,145,443 6 chainXenTro3Link chainOrnAna1 26,405,758,645 7 chainRn4 chainXenTro3Link 18,432,254,632 8 chainVicPac2 chainOrnAna1 6,864,638,705 9 chainMonDom5Link ex-rna 4,349,989,219 10 chainRn4 chainVicPac2 3,892,115,928 11 ex-rna chainXenTro3Link 1,830,555,949 --- ---------------- ---------------- --------------- 12 chainRn4 chainOrnAna1 1,086,692,495 13 ex-rna ex-anno 307,184,634 14 ex-rna chainVicPac2 227,832,153 15 ex-rna chainRn4 164,196,784 16 chainMonDom5Link exons 116,300,901 17 ex-rna chainOrnAna1 109,300,082 18 chainXenTro3Link exons 52,395,369 19 ex-rna exons 36,411,474 20 chainMonDom5Link ex-anno 33,966,070 21 chainXenTro3Link ex-anno 13,693,852 22 chainVicPac2 exons 10,566,462 23 ex-rna fBrain 8,385,799 24 chainVicPac2 ex-anno 5,745,319 25 chainOrnAna1 ex-anno 4,408,383 26 chainOrnAna1 exons 3,255,513 27 chainRn4 ex-anno 2,761,621 28 chainRn4 exons 2,633,098 29 chainMonDom5Link fBrain 2,380,147 30 fBrain chainXenTro3Link 625,718 31 fBrain chainOrnAna1 398,738 32 fBrain chainVicPac2 357,564 33 chainRn4 fBrain 320,955 34 ex-anno exons 273,500 35 fBrain ex-anno 73,437 36 fBrain exons 54,246 <p>Source: Calculated with polars-bio (using 0-based coordinates) in streaming mode.</p> <p>All Parquet files from this dataset shared the same schema: </p><pre><code>  contig STRING\n  pos_start INT32\n  pos_end INT32\n</code></pre><p></p>"},{"location":"supplement/#synthetic-dataset","title":"Synthetic dataset","text":"<p>Randomly generated intervals (100-10,000,000) inspired by bioframe performance analysis. Generated with generate_dataset.py </p><pre><code>poetry run python src/generate_dataset.py\n</code></pre> All Parquet files from this dataset shared the same schema: <pre><code>  contig STRING\n  pos_start INT64\n  pos_end INT64\n</code></pre><p></p> <p>Note</p> <p>Test datasets in the Parquet format can be downloaded from:</p> <ul> <li>single thread benchmarks<ul> <li>databio.zip</li> <li>random_intervals_20250622_221714-1p.zip</li> </ul> </li> <li>parallel benchmarks (partitioned)<ul> <li>databio-8p.zip</li> <li>random_intervals_20250622_221714-8p.zip</li> </ul> </li> </ul>"},{"location":"supplement/#overlap-summary","title":"Overlap summary","text":"Test case polars_bio<sup>1</sup> - # of overlaps bioframe<sup>2</sup> - # of overlaps pyranges0 - # of overlaps pyranges1 - # of overlaps 1-2 54,246 54,246 54,246 54,246 8-7 307,184,634 307,184,634 307,184,634 307,184,634 100 781 781 781 781 1000 8,859 8,859 8,859 8,859 10000 90,236 90,236 90,236 90,236 100000 902,553 902,553 902,553 902,553 1000000 9,007,817 9,007,817 9,007,817 9,007,817 10000000 90,005,371 90,005,371 90,005,371 90,005,371 <p><sup>1</sup> bioframe and pyranges are zero-based. In polars-bio &gt;= 0.19.0, coordinate system is managed via DataFrame metadata. Use <code>pb.scan_*(..., use_zero_based=True)</code> to read data in 0-based coordinates.</p> <p><sup>2</sup> bioframe <code>how</code> parameter is set to <code>inner</code> (<code>left</code> by default)</p>"},{"location":"supplement/#summary-statistics","title":"Summary statistics","text":""},{"location":"supplement/#single-thread-results","title":"Single-thread results","text":"<p>Results for <code>overlap</code>, <code>nearest</code>, <code>count-overlaps</code>, and <code>coverage</code> operations with single-thread performance on <code>apple-m3-max</code> and <code>gcp-linux</code> platforms.</p> <p>Note</p> <p>Please note that in case of <code>pyranges0</code> we were unable to compute the results of coverage and count-overlaps operations for macOS and Linux in the synthetic benchmark, so the results are not presented here.</p> <p></p>"},{"location":"supplement/#exec-1--apple-m3-max","title":"apple-m3-max","text":""},{"location":"supplement/#exec-1--1-2","title":"1-2","text":""},{"location":"supplement/#exec-1--overlap","title":"overlap","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.035619 0.043113 0.0383 2.70x bioframe 0.102257 0.104425 0.103354 1.00x pyranges0 0.025425 0.032821 0.028001 3.69x pyranges1 0.059608 0.064147 0.061763 1.67x pybedtools 0.343204 0.352804 0.348434 0.30x genomicranges 1.042893 1.044245 1.043488 0.10x"},{"location":"supplement/#exec-1--nearest","title":"nearest","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.039943 0.045166 0.042109 4.45x bioframe 0.185452 0.189631 0.187388 1.00x pyranges0 0.092334 0.09634 0.093688 2.00x pyranges1 0.133631 0.134179 0.133981 1.40x pybedtools 0.756676 0.761866 0.75953 0.25x"},{"location":"supplement/#exec-1--count-overlaps","title":"count-overlaps","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.026706 0.029754 0.028142 4.69x bioframe 0.131124 0.133729 0.132052 1.00x pyranges0 0.039136 0.039774 0.039377 3.35x pyranges1 0.061976 0.063181 0.062658 2.11x pybedtools 0.665804 0.673844 0.668534 0.20x genomicranges 0.994963 1.006435 0.999389 0.13x"},{"location":"supplement/#exec-1--coverage","title":"coverage","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.0262 0.028749 0.027418 6.30x bioframe 0.16949 0.176628 0.172842 1.00x pyranges0 0.07376 0.076708 0.075369 2.29x pyranges1 0.128027 0.133263 0.130247 1.33x pybedtools 0.701817 0.708726 0.705839 0.24x genomicranges 1.032651 1.049059 1.040799 0.17x"},{"location":"supplement/#exec-1--8-7","title":"8-7","text":""},{"location":"supplement/#exec-1--overlap_1","title":"overlap","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 3.987391 4.648581 4.235518 7.17x bioframe 29.793837 30.991576 30.375518 1.00x pyranges0 15.632212 15.974075 15.857213 1.92x pyranges1 31.622804 33.699074 32.680701 0.93x pybedtools 916.711575 919.974811 918.154834 0.03x genomicranges 479.214112 487.832054 484.579554 0.06x"},{"location":"supplement/#exec-1--nearest_1","title":"nearest","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 2.116922 2.169534 2.139006 32.13x bioframe 68.581465 68.992651 68.725495 1.00x pyranges0 1.381964 1.508513 1.424446 48.25x pyranges1 2.697684 2.728407 2.717532 25.29x pybedtools 35.528719 35.876667 35.699544 1.93x"},{"location":"supplement/#exec-1--count-overlaps_1","title":"count-overlaps","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 1.445467 1.484052 1.46225 58.77x bioframe 85.632767 86.26148 85.935955 1.00x pyranges0 9.674847 9.833233 9.753982 8.81x pyranges1 10.170249 10.254359 10.201813 8.42x pybedtools 33.101592 33.966188 33.423595 2.57x genomicranges 488.972732 490.395787 489.548184 0.18x"},{"location":"supplement/#exec-1--coverage_1","title":"coverage","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 1.195279 1.205765 1.199323 20.45x bioframe 24.423391 24.682901 24.525909 1.00x pyranges0 11.093644 11.328071 11.220416 2.19x pyranges1 11.987003 12.147925 12.066045 2.03x pybedtools 59.699275 60.04087 59.84965 0.41x genomicranges 500.041974 503.31936 502.043072 0.05x"},{"location":"supplement/#exec-1--100-1p","title":"100-1p","text":""},{"location":"supplement/#exec-1--overlap_2","title":"overlap","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.002471 0.006262 0.003855 0.54x bioframe 0.001374 0.002735 0.002067 1.00x pyranges0 0.000977 0.001952 0.001337 1.55x pyranges1 0.002276 0.003591 0.002739 0.75x pybedtools 0.006856 0.010064 0.008032 0.26x genomicranges 0.001784 0.002115 0.001938 1.07x pygenomics 0.000475 0.000541 0.000509 4.06x"},{"location":"supplement/#exec-1--nearest_2","title":"nearest","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.002802 0.007312 0.004371 0.51x bioframe 0.00157 0.00347 0.002251 1.00x pyranges0 0.00135 0.004085 0.002281 0.99x pyranges1 0.002084 0.003622 0.002633 0.85x pybedtools 0.005288 0.023073 0.011717 0.19x"},{"location":"supplement/#exec-1--count-overlaps_2","title":"count-overlaps","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.001892 0.006355 0.003397 0.52x bioframe 0.001563 0.002165 0.001775 1.00x pyranges1 0.00181 0.002209 0.001972 0.90x pybedtools 0.020892 0.062978 0.036866 0.05x genomicranges 0.001896 0.002057 0.001957 0.91x"},{"location":"supplement/#exec-1--coverage_2","title":"coverage","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.001911 0.006057 0.003343 1.03x bioframe 0.003065 0.00411 0.003452 1.00x pyranges1 0.004455 0.005845 0.005021 0.69x pybedtools 0.02477 0.059532 0.037421 0.09x"},{"location":"supplement/#exec-1--1000-1p","title":"1000-1p","text":""},{"location":"supplement/#exec-1--overlap_3","title":"overlap","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.00262 0.004367 0.003278 0.71x bioframe 0.001909 0.002988 0.002313 1.00x pyranges0 0.001361 0.00182 0.001543 1.50x pyranges1 0.002678 0.003166 0.002927 0.79x pybedtools 0.037238 0.039737 0.038453 0.06x genomicranges 0.019265 0.019945 0.01957 0.12x pygenomics 0.006876 0.006994 0.006949 0.33x"},{"location":"supplement/#exec-1--nearest_3","title":"nearest","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.003048 0.0083 0.00553 0.65x bioframe 0.003269 0.004119 0.003604 1.00x pyranges0 0.002514 0.003506 0.003099 1.16x pyranges1 0.003722 0.00418 0.003935 0.92x pybedtools 0.00881 0.011281 0.009729 0.37x"},{"location":"supplement/#exec-1--count-overlaps_3","title":"count-overlaps","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.001854 0.004714 0.002898 1.00x bioframe 0.002523 0.003547 0.002898 1.00x pyranges1 0.002302 0.002838 0.002498 1.16x pybedtools 0.032681 0.047822 0.037981 0.08x genomicranges 0.020029 0.02029 0.020192 0.14x"},{"location":"supplement/#exec-1--coverage_3","title":"coverage","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.002202 0.003516 0.002696 1.77x bioframe 0.004238 0.005691 0.004758 1.00x pyranges1 0.004909 0.005934 0.005284 0.90x pybedtools 0.030735 0.045004 0.03646 0.13x"},{"location":"supplement/#exec-1--10000-1p","title":"10000-1p","text":""},{"location":"supplement/#exec-1--overlap_4","title":"overlap","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.004603 0.008294 0.006073 1.81x bioframe 0.010529 0.011367 0.011014 1.00x pyranges0 0.006498 0.007306 0.006811 1.62x pyranges1 0.01096 0.012611 0.011684 0.94x pybedtools 0.94646 0.94995 0.948121 0.01x genomicranges 0.198868 0.200266 0.199428 0.06x pygenomics 0.080325 0.08121 0.080663 0.14x"},{"location":"supplement/#exec-1--nearest_4","title":"nearest","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.004851 0.007782 0.005908 4.50x bioframe 0.025947 0.027779 0.026584 1.00x pyranges0 0.00501 0.005703 0.00526 5.05x pyranges1 0.007517 0.007937 0.00769 3.46x pybedtools 0.040749 0.043864 0.041889 0.63x"},{"location":"supplement/#exec-1--count-overlaps_4","title":"count-overlaps","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.003283 0.008069 0.005083 3.12x bioframe 0.014669 0.016689 0.015834 1.00x pyranges1 0.007637 0.008979 0.008178 1.94x pybedtools 0.720797 0.730655 0.725407 0.02x genomicranges 0.202131 0.209398 0.204628 0.08x"},{"location":"supplement/#exec-1--coverage_4","title":"coverage","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.002756 0.004613 0.003377 3.06x bioframe 0.009849 0.011243 0.010339 1.00x pyranges1 0.01326 0.015308 0.013973 0.74x pybedtools 0.727294 0.733098 0.73116 0.01x"},{"location":"supplement/#exec-1--100000-1p","title":"100000-1p","text":""},{"location":"supplement/#exec-1--overlap_5","title":"overlap","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.030583 0.038892 0.033394 3.33x bioframe 0.108358 0.115233 0.111059 1.00x pyranges0 0.059633 0.065599 0.061791 1.80x pyranges1 0.100074 0.105947 0.102267 1.09x pybedtools 13.434458 13.602339 13.496321 0.01x genomicranges 2.030365 2.052434 2.039897 0.05x pygenomics 1.001974 1.018231 1.009213 0.11x"},{"location":"supplement/#exec-1--nearest_5","title":"nearest","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.03013 0.036718 0.03339 10.61x bioframe 0.352786 0.356839 0.354241 1.00x pyranges0 0.032403 0.034701 0.033667 10.52x pyranges1 0.044958 0.046169 0.045629 7.76x pybedtools 0.369122 0.379131 0.3729 0.95x"},{"location":"supplement/#exec-1--count-overlaps_5","title":"count-overlaps","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.021035 0.026894 0.023802 13.86x bioframe 0.308013 0.347919 0.329806 1.00x pyranges1 0.076199 0.085019 0.079372 4.16x pybedtools 11.056327 11.280248 11.149039 0.03x genomicranges 2.057607 2.07651 2.067998 0.16x"},{"location":"supplement/#exec-1--coverage_5","title":"coverage","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.013263 0.014998 0.013874 5.68x bioframe 0.077717 0.081116 0.078865 1.00x pyranges1 0.094753 0.114552 0.10257 0.77x pybedtools 11.374602 11.428316 11.393849 0.01x"},{"location":"supplement/#exec-1--1000000-1p","title":"1000000-1p","text":""},{"location":"supplement/#exec-1--overlap_6","title":"overlap","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.482548 0.538737 0.507383 2.55x bioframe 1.26082 1.35031 1.296195 1.00x pyranges0 0.775969 0.828801 0.810501 1.60x pyranges1 1.272326 1.29706 1.28585 1.01x"},{"location":"supplement/#exec-1--nearest_6","title":"nearest","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.439544 0.488414 0.458975 14.86x bioframe 6.592501 7.111734 6.818208 1.00x pyranges0 0.398173 0.413055 0.406623 16.77x pyranges1 0.51649 0.520946 0.518407 13.15x"},{"location":"supplement/#exec-1--count-overlaps_6","title":"count-overlaps","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.257781 0.305275 0.28525 17.65x bioframe 4.640915 5.437883 5.033454 1.00x pyranges1 0.916714 0.925945 0.920594 5.47x"},{"location":"supplement/#exec-1--coverage_6","title":"coverage","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.128241 0.137198 0.132474 7.71x bioframe 0.996542 1.065777 1.021738 1.00x pyranges1 1.115134 1.247674 1.172964 0.87x"},{"location":"supplement/#exec-1--10000000-1p","title":"10000000-1p","text":""},{"location":"supplement/#exec-1--overlap_7","title":"overlap","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 8.532137 9.738828 8.978132 2.20x bioframe 19.276665 20.295566 19.708064 1.00x pyranges0 14.819439 15.339048 15.092611 1.31x pyranges1 20.153432 22.654892 21.56345 0.91x"},{"location":"supplement/#exec-1--nearest_7","title":"nearest","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 7.12779 7.490779 7.263011 22.17x bioframe 156.356696 169.531002 160.989714 1.00x pyranges0 6.402183 6.879779 6.62806 24.29x pyranges1 7.526236 8.176338 7.857803 20.49x"},{"location":"supplement/#exec-1--count-overlaps_7","title":"count-overlaps","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 4.887937 5.553197 5.165014 20.21x bioframe 102.637625 105.903506 104.389343 1.00x pyranges1 13.35283 15.167609 14.19713 7.35x"},{"location":"supplement/#exec-1--coverage_7","title":"coverage","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 1.627897 1.683304 1.655288 9.86x bioframe 15.586487 16.774274 16.316676 1.00x pyranges1 16.99118 17.447484 17.195844 0.95x"},{"location":"supplement/#exec-1--gcp-linux","title":"gcp-linux","text":""},{"location":"supplement/#exec-1--1-2_1","title":"1-2","text":""},{"location":"supplement/#exec-1--overlap_8","title":"overlap","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.045943 0.064732 0.054234 1.66x bioframe 0.084137 0.099481 0.090107 1.00x pyranges0 0.056206 0.065654 0.061844 1.46x pyranges1 0.09908 0.119018 0.106228 0.85x pybedtools 0.38246 0.406379 0.39153 0.23x genomicranges 1.19939 1.224621 1.208255 0.07x"},{"location":"supplement/#exec-1--nearest_8","title":"nearest","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.057012 0.073822 0.064665 2.49x bioframe 0.158764 0.165707 0.161273 1.00x pyranges0 0.172297 0.176259 0.17363 0.93x pyranges1 0.217619 0.234088 0.22335 0.72x pybedtools 0.845945 0.84898 0.847447 0.19x"},{"location":"supplement/#exec-1--count-overlaps_8","title":"count-overlaps","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.035631 0.043555 0.04066 2.74x bioframe 0.108015 0.116522 0.111266 1.00x pyranges0 0.077336 0.080282 0.07844 1.42x pyranges1 0.100883 0.106671 0.103181 1.08x pybedtools 0.745958 0.759006 0.754393 0.15x genomicranges 1.154942 1.164158 1.158506 0.10x"},{"location":"supplement/#exec-1--coverage_8","title":"coverage","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.036476 0.040001 0.037897 5.10x bioframe 0.189201 0.20046 0.193401 1.00x pyranges0 0.141659 0.14424 0.143188 1.35x pyranges1 0.206033 0.224902 0.213089 0.91x pybedtools 0.773732 0.780424 0.776934 0.25x genomicranges 1.186341 1.194172 1.189255 0.16x"},{"location":"supplement/#exec-1--8-7_1","title":"8-7","text":""},{"location":"supplement/#exec-1--overlap_9","title":"overlap","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 6.235223 9.61441 7.723144 6.54x bioframe 50.319263 50.956633 50.537202 1.00x pyranges0 36.371926 36.581642 36.448645 1.39x pyranges1 63.336711 63.455435 63.40654 0.80x pybedtools 1149.001487 1152.127068 1150.070659 0.04x genomicranges 597.951648 599.960895 599.002871 0.08x"},{"location":"supplement/#exec-1--nearest_9","title":"nearest","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 3.576373 3.679698 3.633697 15.54x bioframe 56.301865 56.776617 56.464305 1.00x pyranges0 2.45308 2.60494 2.505172 22.54x pyranges1 4.975662 5.011008 4.997007 11.30x pybedtools 44.181913 44.79409 44.386971 1.27x"},{"location":"supplement/#exec-1--count-overlaps_9","title":"count-overlaps","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 2.052196 2.104447 2.075706 38.15x bioframe 79.174164 79.234115 79.194209 1.00x pyranges0 18.797436 18.851941 18.824498 4.21x pyranges1 20.399172 20.436149 20.418562 3.88x pybedtools 35.850631 36.142479 36.041115 2.20x genomicranges 612.985873 613.52087 613.229997 0.13x"},{"location":"supplement/#exec-1--coverage_9","title":"coverage","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 1.829478 1.838981 1.834999 15.44x bioframe 28.29136 28.361417 28.326821 1.00x pyranges0 18.611247 20.021441 19.473105 1.45x pyranges1 22.118838 22.210733 22.161329 1.28x pybedtools 74.477086 74.868659 74.618066 0.38x genomicranges 623.865655 623.94955 623.896645 0.05x"},{"location":"supplement/#exec-1--100-1p_1","title":"100-1p","text":""},{"location":"supplement/#exec-1--overlap_10","title":"overlap","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.004992 0.01456 0.008429 0.47x bioframe 0.003258 0.005112 0.00392 1.00x pyranges0 0.002368 0.003408 0.002777 1.41x pyranges1 0.005606 0.006547 0.005975 0.66x pybedtools 0.005909 0.006483 0.006194 0.63x genomicranges 0.003124 0.003404 0.003233 1.21x pygenomics 0.000777 0.000879 0.000818 4.79x"},{"location":"supplement/#exec-1--nearest_10","title":"nearest","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.039758 0.157059 0.082499 0.06x bioframe 0.004496 0.005139 0.004808 1.00x pyranges1 0.005232 0.006285 0.005613 0.86x pybedtools 0.002655 0.002957 0.002758 1.74x"},{"location":"supplement/#exec-1--count-overlaps_10","title":"count-overlaps","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.004349 0.072032 0.027109 0.16x bioframe 0.003966 0.004761 0.004247 1.00x pyranges0 0.002885 0.00314 0.002973 1.43x pyranges1 0.004525 0.004943 0.004694 0.90x pybedtools 0.002502 0.002934 0.0027 1.57x genomicranges 0.003229 0.003376 0.003278 1.30x"},{"location":"supplement/#exec-1--coverage_10","title":"coverage","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.004251 0.062087 0.0237 0.37x bioframe 0.007449 0.011114 0.008755 1.00x pyranges1 0.010586 0.012078 0.011134 0.79x pybedtools 0.002555 0.002829 0.002686 3.26x"},{"location":"supplement/#exec-1--1000-1p_1","title":"1000-1p","text":""},{"location":"supplement/#exec-1--overlap_11","title":"overlap","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.005234 0.008876 0.006523 0.77x bioframe 0.004581 0.005864 0.005016 1.00x pyranges0 0.003191 0.003455 0.003296 1.52x pyranges1 0.008031 0.008103 0.008074 0.62x pybedtools 0.053782 0.054005 0.053929 0.09x genomicranges 0.032026 0.032674 0.032265 0.16x pygenomics 0.010626 0.01142 0.010918 0.46x"},{"location":"supplement/#exec-1--nearest_11","title":"nearest","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.005791 0.006665 0.006123 1.01x bioframe 0.005982 0.006628 0.00621 1.00x pyranges0 0.006279 0.006752 0.006447 0.96x pyranges1 0.009039 0.009504 0.009217 0.67x pybedtools 0.007826 0.007978 0.007917 0.78x"},{"location":"supplement/#exec-1--count-overlaps_11","title":"count-overlaps","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.004343 0.007692 0.005481 0.98x bioframe 0.005139 0.005735 0.005359 1.00x pyranges1 0.005589 0.005976 0.005719 0.94x pybedtools 0.01436 0.014635 0.014456 0.37x genomicranges 0.032931 0.03307 0.033016 0.16x"},{"location":"supplement/#exec-1--coverage_11","title":"coverage","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.004259 0.005305 0.004947 2.08x bioframe 0.009969 0.010782 0.010297 1.00x pyranges1 0.011982 0.012304 0.012103 0.85x pybedtools 0.014775 0.015246 0.014956 0.69x"},{"location":"supplement/#exec-1--10000-1p_1","title":"10000-1p","text":""},{"location":"supplement/#exec-1--overlap_12","title":"overlap","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.01015 0.018572 0.013027 1.84x bioframe 0.02268 0.025605 0.023968 1.00x pyranges0 0.016065 0.018936 0.017143 1.40x pyranges1 0.030509 0.031181 0.030868 0.78x pybedtools 1.335037 1.358509 1.345311 0.02x genomicranges 0.322956 0.326403 0.324169 0.07x pygenomics 0.136783 0.141169 0.13853 0.17x"},{"location":"supplement/#exec-1--nearest_12","title":"nearest","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.009616 0.011293 0.010447 3.08x bioframe 0.031761 0.032938 0.032167 1.00x pyranges0 0.010939 0.011387 0.01109 2.90x pyranges1 0.015275 0.015676 0.015419 2.09x pybedtools 0.059244 0.059899 0.059542 0.54x"},{"location":"supplement/#exec-1--count-overlaps_12","title":"count-overlaps","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.007028 0.007725 0.007363 2.50x bioframe 0.018051 0.019179 0.018436 1.00x pyranges1 0.014252 0.014683 0.014423 1.28x pybedtools 0.926946 1.012523 0.973852 0.02x genomicranges 0.330064 0.33175 0.331123 0.06x"},{"location":"supplement/#exec-1--coverage_12","title":"coverage","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.005994 0.006967 0.006396 2.78x bioframe 0.017402 0.018389 0.017779 1.00x pyranges1 0.022651 0.023034 0.022779 0.78x pybedtools 0.952175 1.000698 0.97678 0.02x"},{"location":"supplement/#exec-1--100000-1p_1","title":"100000-1p","text":""},{"location":"supplement/#exec-1--overlap_13","title":"overlap","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.059271 0.08788 0.070483 3.02x bioframe 0.209074 0.218512 0.21252 1.00x pyranges0 0.144653 0.164863 0.151749 1.40x pyranges1 0.228314 0.247017 0.234636 0.91x pybedtools 19.263571 19.313483 19.286741 0.01x genomicranges 3.290473 3.294306 3.291987 0.06x pygenomics 1.881858 1.924059 1.896222 0.11x"},{"location":"supplement/#exec-1--nearest_13","title":"nearest","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.054573 0.060741 0.057958 6.31x bioframe 0.363422 0.368554 0.365524 1.00x pyranges0 0.062446 0.06448 0.06321 5.78x pyranges1 0.084614 0.086633 0.085545 4.27x pybedtools 0.570352 0.57555 0.572301 0.64x"},{"location":"supplement/#exec-1--count-overlaps_13","title":"count-overlaps","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.034675 0.047883 0.039593 7.85x bioframe 0.309819 0.311936 0.310958 1.00x pyranges1 0.113469 0.114316 0.113866 2.73x pybedtools 15.265868 16.802575 16.206183 0.02x genomicranges 3.369224 3.374411 3.371411 0.09x"},{"location":"supplement/#exec-1--coverage_13","title":"coverage","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.02543 0.026808 0.026079 4.08x bioframe 0.104575 0.1096 0.106393 1.00x pyranges1 0.147505 0.151673 0.149512 0.71x pybedtools 16.382024 17.619212 16.802475 0.01x"},{"location":"supplement/#exec-1--1000000-1p_1","title":"1000000-1p","text":""},{"location":"supplement/#exec-1--overlap_14","title":"overlap","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.947673 1.10915 1.003667 2.49x bioframe 2.490142 2.513556 2.499533 1.00x pyranges0 2.119717 2.178453 2.148959 1.16x pyranges1 3.274957 3.298976 3.288601 0.76x"},{"location":"supplement/#exec-1--nearest_14","title":"nearest","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.771199 0.783292 0.777746 6.96x bioframe 5.394265 5.434618 5.411728 1.00x pyranges0 0.874484 0.932857 0.901145 6.01x pyranges1 1.127032 1.149141 1.140538 4.74x"},{"location":"supplement/#exec-1--count-overlaps_14","title":"count-overlaps","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.501775 0.530884 0.514401 8.01x bioframe 4.117035 4.131015 4.121744 1.00x pyranges1 1.583204 1.678121 1.631619 2.53x"},{"location":"supplement/#exec-1--coverage_14","title":"coverage","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.247626 0.266565 0.256522 4.86x bioframe 1.243608 1.250394 1.246153 1.00x pyranges1 1.916323 2.005555 1.949487 0.64x"},{"location":"supplement/#exec-1--10000000-1p_1","title":"10000000-1p","text":""},{"location":"supplement/#exec-1--overlap_15","title":"overlap","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 18.531273 18.806266 18.632293 1.62x bioframe 30.074841 30.116671 30.097846 1.00x pyranges0 29.579651 30.536834 29.904783 1.01x pyranges1 42.196037 42.278681 42.232728 0.71x"},{"location":"supplement/#exec-1--nearest_15","title":"nearest","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 14.43136 14.548645 14.496101 5.56x bioframe 80.443039 80.705181 80.548879 1.00x pyranges0 13.64936 14.330292 13.882901 5.80x pyranges1 17.384461 17.654503 17.561143 4.59x"},{"location":"supplement/#exec-1--count-overlaps_15","title":"count-overlaps","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 9.812223 9.925855 9.856571 6.23x bioframe 61.348815 61.558393 61.444649 1.00x pyranges1 24.969282 25.069392 25.029806 2.45x"},{"location":"supplement/#exec-1--coverage_15","title":"coverage","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 3.188742 3.31096 3.235061 6.11x bioframe 19.748385 19.802079 19.778009 1.00x pyranges1 30.304058 30.446378 30.353857 0.65x"},{"location":"supplement/#parallel-performance","title":"Parallel performance","text":"<p>Results for parallel operations with 1, 2, 4, 6 and 8 threads.</p> <p></p>"},{"location":"supplement/#exec-2--apple-m3-max","title":"apple-m3-max","text":""},{"location":"supplement/#exec-2--8-7-8p","title":"8-7-8p","text":""},{"location":"supplement/#exec-2--overlap","title":"overlap","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 3.247022 3.803021 3.370889 1.00x polars_bio-2 1.798569 1.848162 1.811417 1.86x polars_bio-4 1.140229 1.158243 1.147355 2.94x polars_bio-6 0.959703 0.968725 0.962915 3.50x polars_bio-8 0.694637 0.710492 0.701048 4.81x"},{"location":"supplement/#exec-2--nearest","title":"nearest","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 2.186354 2.248171 2.220822 1.00x polars_bio-2 1.162969 1.222115 1.187505 1.87x polars_bio-4 0.708508 0.735763 0.720115 3.08x polars_bio-6 0.632877 0.652955 0.642816 3.45x polars_bio-8 0.456674 0.476473 0.465284 4.77x"},{"location":"supplement/#exec-2--count-overlaps","title":"count-overlaps","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 1.502551 1.534006 1.515078 1.00x polars_bio-2 0.811236 0.821365 0.815682 1.86x polars_bio-4 0.440628 0.46778 0.455358 3.33x polars_bio-6 0.331317 0.338207 0.334638 4.53x polars_bio-8 0.280465 0.282707 0.281311 5.39x"},{"location":"supplement/#exec-2--coverage","title":"coverage","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 1.181806 1.185549 1.183889 1.00x polars_bio-2 0.644288 0.645076 0.644587 1.84x polars_bio-4 0.362752 0.363411 0.363036 3.26x polars_bio-6 0.258583 0.272702 0.264111 4.48x polars_bio-8 0.222888 0.234884 0.229052 5.17x"},{"location":"supplement/#exec-2--1000000-8p","title":"1000000-8p","text":""},{"location":"supplement/#exec-2--overlap_1","title":"overlap","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.468442 0.523065 0.494609 1.00x polars_bio-2 0.262861 0.26828 0.265028 1.87x polars_bio-4 0.1629 0.166657 0.164536 3.01x polars_bio-6 0.137724 0.146893 0.143772 3.44x polars_bio-8 0.111952 0.11465 0.113521 4.36x"},{"location":"supplement/#exec-2--nearest_1","title":"nearest","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.393067 0.415076 0.404032 1.00x polars_bio-2 0.234559 0.235746 0.235051 1.72x polars_bio-4 0.158996 0.167352 0.16349 2.47x polars_bio-6 0.14634 0.14935 0.148215 2.73x polars_bio-8 0.125472 0.128158 0.126606 3.19x"},{"location":"supplement/#exec-2--count-overlaps_1","title":"count-overlaps","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.267875 0.296727 0.277677 1.00x polars_bio-2 0.163662 0.170045 0.165917 1.67x polars_bio-4 0.111136 0.114835 0.112891 2.46x polars_bio-6 0.097944 0.104607 0.101477 2.74x polars_bio-8 0.099474 0.117493 0.106059 2.62x"},{"location":"supplement/#exec-2--coverage_1","title":"coverage","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.128377 0.131261 0.129598 1.00x polars_bio-2 0.081762 0.085104 0.08324 1.56x polars_bio-4 0.064151 0.066197 0.064851 2.00x polars_bio-6 0.066926 0.06892 0.06768 1.91x polars_bio-8 0.072767 0.074339 0.073589 1.76x"},{"location":"supplement/#exec-2--10000000-8p","title":"10000000-8p","text":""},{"location":"supplement/#exec-2--overlap_2","title":"overlap","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 9.081732 9.388126 9.203018 1.00x polars_bio-2 4.696455 4.912478 4.793254 1.92x polars_bio-4 2.885023 2.902893 2.896218 3.18x polars_bio-6 2.196605 2.217945 2.209839 4.16x polars_bio-8 1.813586 1.860947 1.833498 5.02x"},{"location":"supplement/#exec-2--nearest_2","title":"nearest","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 7.299887 7.659385 7.495962 1.00x polars_bio-2 4.01928 4.158504 4.069511 1.84x polars_bio-4 2.683383 2.720981 2.704975 2.77x polars_bio-6 2.141075 2.162109 2.150595 3.49x polars_bio-8 1.859186 1.865634 1.862653 4.02x"},{"location":"supplement/#exec-2--count-overlaps_2","title":"count-overlaps","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 5.30938 5.450332 5.381068 1.00x polars_bio-2 2.893766 2.91378 2.906401 1.85x polars_bio-4 1.748771 1.797485 1.768895 3.04x polars_bio-6 1.352671 1.385655 1.369312 3.93x polars_bio-8 1.178559 1.199971 1.192577 4.51x"},{"location":"supplement/#exec-2--coverage_2","title":"coverage","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 1.638818 1.678156 1.655573 1.00x polars_bio-2 0.994195 0.996554 0.995701 1.66x polars_bio-4 0.678722 0.701234 0.689151 2.40x polars_bio-6 0.620289 0.662175 0.639026 2.59x polars_bio-8 0.570659 0.582937 0.57688 2.87x"},{"location":"supplement/#exec-2--gcp-linux","title":"gcp-linux","text":""},{"location":"supplement/#exec-2--8-7-8p_1","title":"8-7-8p","text":""},{"location":"supplement/#exec-2--overlap_3","title":"overlap","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 6.325617 8.185275 7.005925 1.00x polars_bio-2 3.920645 4.617084 4.198055 1.67x polars_bio-4 3.036273 3.060781 3.0452 2.30x polars_bio-6 2.127994 2.134505 2.131016 3.29x polars_bio-8 1.731485 1.789347 1.752986 4.00x"},{"location":"supplement/#exec-2--nearest_3","title":"nearest","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 4.047329 4.439016 4.198198 1.00x polars_bio-2 2.624132 2.722843 2.682361 1.57x polars_bio-4 1.809028 1.917798 1.871763 2.24x polars_bio-6 1.309557 1.362131 1.333989 3.15x polars_bio-8 1.066945 1.113168 1.087907 3.86x"},{"location":"supplement/#exec-2--count-overlaps_3","title":"count-overlaps","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 2.426441 2.456318 2.439266 1.00x polars_bio-2 1.22516 1.272066 1.245401 1.96x polars_bio-4 0.711421 0.744023 0.724315 3.37x polars_bio-6 0.563797 0.607321 0.580574 4.20x polars_bio-8 0.459308 0.493886 0.479126 5.09x"},{"location":"supplement/#exec-2--coverage_3","title":"coverage","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 2.212958 2.23035 2.222531 1.00x polars_bio-2 1.132056 1.15405 1.146413 1.94x polars_bio-4 0.645737 0.661564 0.652277 3.41x polars_bio-6 0.50589 0.511256 0.50839 4.37x polars_bio-8 0.439503 0.450924 0.447075 4.97x"},{"location":"supplement/#exec-2--1000000-8p_1","title":"1000000-8p","text":""},{"location":"supplement/#exec-2--overlap_4","title":"overlap","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.903831 1.098046 0.974229 1.00x polars_bio-2 0.50099 0.512259 0.504852 1.93x polars_bio-4 0.300453 0.328605 0.318188 3.06x polars_bio-6 0.257792 0.278203 0.268718 3.63x polars_bio-8 0.22321 0.243244 0.230621 4.22x"},{"location":"supplement/#exec-2--nearest_4","title":"nearest","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.758815 0.78919 0.769877 1.00x polars_bio-2 0.465192 0.47484 0.468824 1.64x polars_bio-4 0.332101 0.336953 0.334461 2.30x polars_bio-6 0.276071 0.29266 0.281794 2.73x polars_bio-8 0.237269 0.263256 0.254046 3.03x"},{"location":"supplement/#exec-2--count-overlaps_4","title":"count-overlaps","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.496938 0.517659 0.505043 1.00x polars_bio-2 0.295325 0.313859 0.302686 1.67x polars_bio-4 0.194371 0.20433 0.200853 2.51x polars_bio-6 0.175505 0.181913 0.178222 2.83x polars_bio-8 0.15672 0.163036 0.160701 3.14x"},{"location":"supplement/#exec-2--coverage_4","title":"coverage","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 0.245895 0.250118 0.247479 1.00x polars_bio-2 0.167378 0.173578 0.171251 1.45x polars_bio-4 0.122749 0.126635 0.124491 1.99x polars_bio-6 0.11385 0.119157 0.116185 2.13x polars_bio-8 0.108127 0.110327 0.10942 2.26x"},{"location":"supplement/#exec-2--10000000-8p_1","title":"10000000-8p","text":""},{"location":"supplement/#exec-2--overlap_5","title":"overlap","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 18.215782 19.091392 18.510207 1.00x polars_bio-2 9.399565 9.680242 9.566631 1.93x polars_bio-4 5.303647 5.555487 5.442898 3.40x polars_bio-6 4.022274 4.066371 4.051045 4.57x polars_bio-8 3.369559 3.416123 3.388564 5.46x"},{"location":"supplement/#exec-2--nearest_5","title":"nearest","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 14.325736 14.444885 14.39027 1.00x polars_bio-2 8.095907 8.178189 8.136852 1.77x polars_bio-4 5.096407 5.15379 5.122893 2.81x polars_bio-6 3.986362 4.205706 4.128561 3.49x polars_bio-8 3.491618 3.711814 3.577309 4.02x"},{"location":"supplement/#exec-2--count-overlaps_5","title":"count-overlaps","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 9.701679 9.796148 9.740035 1.00x polars_bio-2 5.346433 5.399117 5.370757 1.81x polars_bio-4 3.150557 3.203458 3.178719 3.06x polars_bio-6 2.485947 2.56386 2.52768 3.85x polars_bio-8 2.156472 2.176608 2.163483 4.50x"},{"location":"supplement/#exec-2--coverage_5","title":"coverage","text":"Library Min (s) Max (s) Mean (s) Speedup polars_bio 3.091184 3.216964 3.135982 1.00x polars_bio-2 1.998423 2.041581 2.01331 1.56x polars_bio-4 1.412483 1.45218 1.426102 2.20x polars_bio-6 1.281432 1.328666 1.301256 2.41x polars_bio-8 1.176944 1.193294 1.18414 2.65x"},{"location":"supplement/#end-to-end-tests","title":"End to end tests","text":"<p>Results for an end-to-end test with calculating overlaps, nearest, coverage and count overlaps and saving results to a CSV file.</p> <p>Note</p> <p>Please note that in case of <code>pyranges0</code> we were unable to export the results of coverage and count-overlaps operations to a CSV file, so the results are not presented here.</p> <p></p>"},{"location":"supplement/#exec-3--apple-m3-max","title":"apple-m3-max","text":""},{"location":"supplement/#exec-3--1-2","title":"1-2","text":""},{"location":"supplement/#exec-3--e2e-overlap-csv","title":"e2e-overlap-csv","text":"Library Min (s) Max (s) Mean (s) Speedup Peak memory (MB) polars_bio 0.042378 0.130957 0.071929 3.10x 285.468 polars_bio_streaming 0.035498 0.037438 0.036653 6.09x 274.093 bioframe 0.208548 0.251457 0.223219 1.00x 300.75 pyranges0 0.409707 0.415361 0.412135 0.54x 329.968 pyranges1 0.47518 0.491508 0.482739 0.46x 324.468"},{"location":"supplement/#exec-3--e2e-nearest-csv","title":"e2e-nearest-csv","text":"Library Min (s) Max (s) Mean (s) Speedup Peak memory (MB) polars_bio 0.053349 0.058382 0.055362 9.14x 321.062 polars_bio_streaming 0.051385 0.053979 0.052764 9.59x 311.422 bioframe 0.503887 0.510257 0.506123 1.00x 316.969 pyranges0 1.135469 1.183369 1.151801 0.44x 364.594 pyranges1 1.327935 1.334101 1.331346 0.38x 357.734"},{"location":"supplement/#exec-3--e2e-coverage-csv","title":"e2e-coverage-csv","text":"Library Min (s) Max (s) Mean (s) Speedup Peak memory (MB) polars_bio 0.034756 0.038718 0.036421 13.40x 290.078 polars_bio_streaming 0.03607 0.037332 0.036534 13.35x 274.344 bioframe 0.48449 0.492328 0.487891 1.00x 419.312 pyranges1 0.971084 0.980085 0.975012 0.50x 407.562"},{"location":"supplement/#exec-3--e2e-count-overlaps-csv","title":"e2e-count-overlaps-csv","text":"Library Min (s) Max (s) Mean (s) Speedup Peak memory (MB) polars_bio 0.03452 0.037714 0.035927 9.27x 294.266 polars_bio_streaming 0.035863 0.036756 0.036414 9.14x 278.438 bioframe 0.328145 0.338734 0.332951 1.00x 306.234 pyranges1 0.532739 0.544914 0.538646 0.62x 328.328"},{"location":"supplement/#exec-3--8-7","title":"8-7","text":""},{"location":"supplement/#exec-3--e2e-overlap-csv_1","title":"e2e-overlap-csv","text":"Library Min (s) Max (s) Mean (s) Speedup Peak memory (MB) polars_bio 22.781745 23.916568 23.161559 16.64x 14677.0468 polars_bio_streaming 18.501279 18.797602 18.676707 20.63x 555.109 bioframe 383.108514 387.500069 385.309331 1.00x 33806.062 pyranges0 276.421312 279.839508 277.845198 1.39x 29777.312 pyranges1 355.703878 367.680249 360.875151 1.07x 34526.859"},{"location":"supplement/#exec-3--e2e-nearest-csv_1","title":"e2e-nearest-csv","text":"Library Min (s) Max (s) Mean (s) Speedup Peak memory (MB) polars_bio 2.597955 2.760537 2.674482 32.02x 1060.031 polars_bio_streaming 2.65088 2.685157 2.665171 32.13x 560.453 bioframe 85.238305 86.131916 85.644961 1.00x 6894.062 pyranges0 13.530549 13.705834 13.620471 6.29x 3031.797 pyranges1 16.290782 16.385961 16.322671 5.25x 3509.984"},{"location":"supplement/#exec-3--e2e-coverage-csv_1","title":"e2e-coverage-csv","text":"Library Min (s) Max (s) Mean (s) Speedup Peak memory (MB) polars_bio 1.523833 1.555472 1.541038 21.41x 717.984 polars_bio_streaming 1.336613 1.397324 1.364051 24.19x 411.703 bioframe 32.294844 33.421618 32.99334 1.00x 16651.922 pyranges1 26.382409 27.382901 27.020202 1.22x 6119.125"},{"location":"supplement/#exec-3--e2e-count-overlaps-csv_1","title":"e2e-count-overlaps-csv","text":"Library Min (s) Max (s) Mean (s) Speedup Peak memory (MB) polars_bio 1.806838 1.845584 1.82594 54.33x 729.078 polars_bio_streaming 1.681187 1.767811 1.714943 57.85x 416.094 bioframe 97.91802 101.736351 99.210461 1.00x 23029.219 pyranges1 19.498264 19.676838 19.561322 5.07x 5270.234"},{"location":"supplement/#exec-3--100-1p","title":"100-1p","text":""},{"location":"supplement/#exec-3--e2e-overlap-csv_2","title":"e2e-overlap-csv","text":"Library Min (s) Max (s) Mean (s) Speedup Peak memory (MB) polars_bio 0.009118 0.077181 0.032054 1.55x 248.594 polars_bio_streaming 0.003382 0.004769 0.003853 12.92x 247.562 bioframe 0.030154 0.088667 0.049769 1.00x 231.641 pyranges0 0.045764 0.051035 0.047857 1.04x 228.516 pyranges1 0.053751 0.072545 0.060221 0.83x 228.609"},{"location":"supplement/#exec-3--e2e-nearest-csv_2","title":"e2e-nearest-csv","text":"Library Min (s) Max (s) Mean (s) Speedup Peak memory (MB) polars_bio 0.009145 0.038799 0.019201 2.24x 253.156 polars_bio_streaming 0.003964 0.005051 0.004504 9.53x 248.188 bioframe 0.033372 0.061107 0.042931 1.00x 229.906 pyranges0 0.049586 0.057381 0.052364 0.82x 231.812 pyranges1 0.054496 0.059205 0.056362 0.76x 231.688"},{"location":"supplement/#exec-3--e2e-coverage-csv_2","title":"e2e-coverage-csv","text":"Library Min (s) Max (s) Mean (s) Speedup Peak memory (MB) polars_bio 0.005492 0.020652 0.012584 5.20x 245.578 polars_bio_streaming 0.003059 0.003746 0.003397 19.25x 243.5 bioframe 0.060684 0.074157 0.065378 1.00x 230.953 pyranges1 0.093668 0.096265 0.094567 0.69x 243.5"},{"location":"supplement/#exec-3--e2e-count-overlaps-csv_2","title":"e2e-count-overlaps-csv","text":"Library Min (s) Max (s) Mean (s) Speedup Peak memory (MB) polars_bio 0.005291 0.008843 0.006568 5.53x 249.406 polars_bio_streaming 0.003279 0.003697 0.003447 10.53x 245.672 bioframe 0.032914 0.042309 0.036302 1.00x 234.141 pyranges1 0.045085 0.045477 0.045224 0.80x 232.703"},{"location":"supplement/#exec-3--10000000-1p","title":"10000000-1p","text":""},{"location":"supplement/#exec-3--e2e-overlap-csv_3","title":"e2e-overlap-csv","text":"Library Min (s) Max (s) Mean (s) Speedup Peak memory (MB) polars_bio 11.109423 11.871893 11.397992 10.38x 7064.312 polars_bio_streaming 12.049206 12.327491 12.191582 9.71x 1505.109 bioframe 117.701516 119.51073 118.356016 1.00x 16380.234 pyranges0 235.484308 243.216406 239.726101 0.49x 14245.203 pyranges1 109.722359 112.326873 111.23273 1.06x 19423.172"},{"location":"supplement/#exec-3--e2e-nearest-csv_3","title":"e2e-nearest-csv","text":"Library Min (s) Max (s) Mean (s) Speedup Peak memory (MB) polars_bio 7.842314 8.84828 8.510181 21.04x 2301.0 polars_bio_streaming 7.589706 8.153016 7.842404 22.83x 1327.531 bioframe 174.790383 183.458906 179.035999 1.00x 10996.234 pyranges0 32.793505 32.826686 32.809101 5.46x 4882.656 pyranges1 18.866156 19.570609 19.142653 9.35x 5253.281"},{"location":"supplement/#exec-3--e2e-coverage-csv_3","title":"e2e-coverage-csv","text":"Library Min (s) Max (s) Mean (s) Speedup Peak memory (MB) polars_bio 1.901833 1.957711 1.928367 12.15x 956.844 polars_bio_streaming 1.797332 1.802527 1.800497 13.01x 651.266 bioframe 23.269774 23.55838 23.430125 1.00x 6493.234 pyranges1 26.370249 27.172173 26.879266 0.87x 10397.531"},{"location":"supplement/#exec-3--e2e-count-overlaps-csv_3","title":"e2e-count-overlaps-csv","text":"Library Min (s) Max (s) Mean (s) Speedup Peak memory (MB) polars_bio 5.025462 5.234103 5.129963 20.79x 1036.734 polars_bio_streaming 4.956087 5.076052 5.014242 21.27x 968.719 bioframe 105.322287 107.758078 106.64158 1.00x 12803.828 pyranges1 22.079391 23.069931 22.618209 4.71x 10039.297"},{"location":"supplement/#exec-3--gcp-linux","title":"gcp-linux","text":""},{"location":"supplement/#exec-3--1-2_1","title":"1-2","text":""},{"location":"supplement/#exec-3--e2e-overlap-csv_4","title":"e2e-overlap-csv","text":"Library Min (s) Max (s) Mean (s) Speedup Peak memory (MB) polars_bio 0.072393 0.151871 0.09916 2.80x 314.234 polars_bio_streaming 0.064092 0.067914 0.066202 4.19x 288.621 bioframe 0.258278 0.31288 0.277225 1.00x 287.101 pyranges0 0.591745 0.599954 0.595204 0.47x 307.218 pyranges1 0.683388 0.702289 0.690362 0.40x 327.863"},{"location":"supplement/#exec-3--e2e-nearest-csv_4","title":"e2e-nearest-csv","text":"Library Min (s) Max (s) Mean (s) Speedup Peak memory (MB) polars_bio 0.123656 1.702108 0.659015 1.99x 331.398 polars_bio_streaming 0.111801 0.762227 0.328874 3.98x 308.738 bioframe 0.881782 2.161628 1.309551 1.00x 297.695 pyranges0 1.728053 2.579086 2.030527 0.64x 308.93 pyranges1 1.953048 2.161655 2.049615 0.64x 337.352"},{"location":"supplement/#exec-3--e2e-coverage-csv_4","title":"e2e-coverage-csv","text":"Library Min (s) Max (s) Mean (s) Speedup Peak memory (MB) polars_bio 0.064626 0.146983 0.093048 8.11x 299.094 polars_bio_streaming 0.065193 0.072839 0.068651 10.99x 280.387 bioframe 0.704155 0.791049 0.754463 1.00x 328.184 pyranges1 1.41166 1.432833 1.42261 0.53x 352.582"},{"location":"supplement/#exec-3--e2e-count-overlaps-csv_4","title":"e2e-count-overlaps-csv","text":"Library Min (s) Max (s) Mean (s) Speedup Peak memory (MB) polars_bio 0.063012 0.207803 0.113156 4.03x 309.176 polars_bio_streaming 0.062735 0.071474 0.065886 6.92x 286.336 bioframe 0.436935 0.491839 0.455688 1.00x 303.07 pyranges1 0.785823 0.786847 0.786487 0.58x 316.227"},{"location":"supplement/#exec-3--8-7_1","title":"8-7","text":""},{"location":"supplement/#exec-3--e2e-overlap-csv_5","title":"e2e-overlap-csv","text":"Library Min (s) Max (s) Mean (s) Speedup Peak memory (MB) polars_bio 44.539766 45.543038 45.196903 12.55x 14575.14 polars_bio_streaming 34.007093 35.972075 35.309756 16.06x 480.207 bioframe 566.167037 567.617695 567.13069 1.00x 43295.378 pyranges0 417.291061 421.875539 419.571591 1.35x 22915.917 pyranges1 538.365637 548.624613 543.918168 1.04x 43408.699"},{"location":"supplement/#exec-3--e2e-nearest-csv_5","title":"e2e-nearest-csv","text":"Library Min (s) Max (s) Mean (s) Speedup Peak memory (MB) polars_bio 6.565142 7.696104 6.973448 12.21x 1070.016 polars_bio_streaming 5.840416 6.828222 6.203 13.73x 527.008 bioframe 84.30831 86.512823 85.150539 1.00x 2418.629 pyranges0 20.679566 21.424632 20.949203 4.06x 2239.047 pyranges1 25.352803 27.604137 26.544063 3.21x 2534.629"},{"location":"supplement/#exec-3--e2e-coverage-csv_5","title":"e2e-coverage-csv","text":"Library Min (s) Max (s) Mean (s) Speedup Peak memory (MB) polars_bio 3.532309 3.660049 3.579428 12.53x 738.887 polars_bio_streaming 3.167344 3.169622 3.168694 14.15x 416.164 bioframe 41.150587 51.89725 44.839673 1.00x 14297.098 pyranges1 40.065526 41.350493 40.892187 1.10x 3096.812"},{"location":"supplement/#exec-3--e2e-count-overlaps-csv_5","title":"e2e-count-overlaps-csv","text":"Library Min (s) Max (s) Mean (s) Speedup Peak memory (MB) polars_bio 3.827717 3.83517 3.830823 25.47x 737.059 polars_bio_streaming 3.346898 3.388796 3.372987 28.93x 428.422 bioframe 97.272988 97.790775 97.572564 1.00x 25981.051 pyranges1 30.021737 30.181438 30.124339 3.24x 3102.84"},{"location":"supplement/#exec-3--100-1p_1","title":"100-1p","text":""},{"location":"supplement/#exec-3--e2e-overlap-csv_6","title":"e2e-overlap-csv","text":"Library Min (s) Max (s) Mean (s) Speedup Peak memory (MB) polars_bio 0.012022 0.433918 0.153077 0.55x 262.656 polars_bio_streaming 0.007427 0.153294 0.056144 1.49x 259.039 bioframe 0.039406 0.172494 0.08386 1.00x 229.824 pyranges0 0.059086 0.075573 0.06466 1.30x 231.199 pyranges1 0.069077 0.088036 0.075488 1.11x 230.684"},{"location":"supplement/#exec-3--e2e-nearest-csv_6","title":"e2e-nearest-csv","text":"Library Min (s) Max (s) Mean (s) Speedup Peak memory (MB) polars_bio 0.012814 0.408332 0.145315 1.24x 263.16 polars_bio_streaming 0.007605 0.007975 0.00779 23.20x 260.242 bioframe 0.044222 0.45263 0.180742 1.00x 230.684 pyranges0 0.066032 0.074886 0.068992 2.62x 231.195 pyranges1 0.07111 0.075383 0.072851 2.48x 230.68"},{"location":"supplement/#exec-3--e2e-coverage-csv_6","title":"e2e-coverage-csv","text":"Library Min (s) Max (s) Mean (s) Speedup Peak memory (MB) polars_bio 0.008954 0.106466 0.041616 2.35x 258.184 polars_bio_streaming 0.006726 0.007524 0.007124 13.72x 255.258 bioframe 0.07866 0.135591 0.097742 1.00x 230.68 pyranges1 0.120404 0.12302 0.121487 0.80x 231.023"},{"location":"supplement/#exec-3--e2e-count-overlaps-csv_6","title":"e2e-count-overlaps-csv","text":"Library Min (s) Max (s) Mean (s) Speedup Peak memory (MB) polars_bio 0.008886 0.095835 0.038506 1.53x 262.312 polars_bio_streaming 0.006988 0.008915 0.007637 7.71x 259.555 bioframe 0.043766 0.08625 0.058895 1.00x 230.852 pyranges1 0.060574 0.060725 0.06064 0.97x 231.195"},{"location":"supplement/#exec-3--10000000-1p_1","title":"10000000-1p","text":""},{"location":"supplement/#exec-3--e2e-overlap-csv_7","title":"e2e-overlap-csv","text":"Library Min (s) Max (s) Mean (s) Speedup Peak memory (MB) polars_bio 35.783773 37.155414 36.274578 4.73x 6926.227 polars_bio_streaming 28.364026 33.834447 32.005189 5.37x 1172.484 bioframe 170.321558 173.826371 171.750864 1.00x 17544.5 pyranges0 374.384106 377.106338 375.972726 0.46x 12951.133 pyranges1 174.205234 176.465726 174.996859 0.98x 23198.973"},{"location":"supplement/#exec-3--e2e-nearest-csv_7","title":"e2e-nearest-csv","text":"Library Min (s) Max (s) Mean (s) Speedup Peak memory (MB) polars_bio 16.577973 16.599008 16.591114 5.88x 2208.477 polars_bio_streaming 14.957252 15.214483 15.115055 6.45x 1202.555 bioframe 96.638005 98.21701 97.559755 1.00x 7832.391 pyranges0 54.678927 55.140916 54.905002 1.78x 3125.051 pyranges1 31.874441 33.028755 32.303297 3.02x 4447.332"},{"location":"supplement/#exec-3--e2e-coverage-csv_7","title":"e2e-coverage-csv","text":"Library Min (s) Max (s) Mean (s) Speedup Peak memory (MB) polars_bio 4.184608 4.403694 4.281924 6.97x 768.75 polars_bio_streaming 3.792566 3.917329 3.838723 7.77x 591.812 bioframe 29.609636 29.972788 29.838014 1.00x 4040.508 pyranges1 41.671912 42.238756 41.949904 0.71x 8503.844"},{"location":"supplement/#exec-3--e2e-count-overlaps-csv_7","title":"e2e-count-overlaps-csv","text":"Library Min (s) Max (s) Mean (s) Speedup Peak memory (MB) polars_bio 11.003474 11.126526 11.052697 6.35x 1012.105 polars_bio_streaming 9.939793 10.434084 10.264927 6.83x 696.078 bioframe 70.00646 70.300308 70.14716 1.00x 8315.176 pyranges1 33.521685 33.726979 33.593637 2.09x 8495.672"},{"location":"supplement/#comparison-of-the-output-schemas-and-data-types","title":"Comparison of the output schemas and data types","text":"<p><code>polars-bio</code> tries to preserve the output schema of the <code>bioframe</code> package, <code>pyranges</code> uses its own internal representation that can be converted to a Pandas dataframe. It is also worth mentioning that <code>pyranges</code> always uses <code>int64</code> for start/end positions representation (polars-bio and bioframe determine it adaptively based on the input file formats/DataFrames datatypes used. polars-bio does not support interval operations on chromosomes longer than 2Gp(issue)). However, in the analyzed test case (<code>8-7</code>) input/output data structures have similar memory requirements. Please compare the following schema and memory size estimates of the input and output DataFrames for <code>8-7</code> test case: </p><pre><code>import bioframe as bf\nimport polars_bio as pb\nimport pandas as pd\nimport polars as pl\nimport pyranges0 as pr0\n\n\nDATA_DIR=\"/Users/mwiewior/research/polars-bio-benchmarking/data/\"\ndf_1 = f\"{DATA_DIR}/ex-anno/*.parquet\"\ndf_2 = f\"{DATA_DIR}/ex-rna/*.parquet\"\ndf1 = pd.read_parquet(df_1.replace(\"*.parquet\", \"\"))\ndf2 = pd.read_parquet(df_2.replace(\"*.parquet\", \"\"))\ncols = [\"contig\", \"pos_start\", \"pos_end\"]\n\ndef df2pr0(df):\n    return pr0.PyRanges(\n        chromosomes=df.contig,\n        starts=df.pos_start,\n        ends=df.pos_end,\n    )\n</code></pre><p></p>"},{"location":"supplement/#input-datasets-sizes-and-schemas","title":"Input datasets sizes and schemas","text":"<pre><code>df1.info()\n</code></pre> <pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1194285 entries, 0 to 1194284\nData columns (total 3 columns):\n#   Column     Non-Null Count    Dtype\n---  ------     --------------    -----\n0   contig     1194285 non-null  object\n1   pos_start  1194285 non-null  int32\n2   pos_end    1194285 non-null  int32\ndtypes: int32(2), object(1)\nmemory usage: 18.2+ MB\n</code></pre> <pre><code>df2.info()\n</code></pre> <pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 9944559 entries, 0 to 9944558\nData columns (total 3 columns):\n #   Column     Dtype\n---  ------     -----\n 0   contig     object\n 1   pos_start  int32\n 2   pos_end    int32\ndtypes: int32(2), object(1)\nmemory usage: 151.7+ MB\n</code></pre>"},{"location":"supplement/#polars-bio-output-dataframes-schema-and-memory-used-polars-and-pandas","title":"polars-bio output DataFrames schema and memory used (Polars and Pandas)","text":"<pre><code># Note: In polars-bio &gt;= 0.19.0, coordinate system is read from DataFrame metadata\n# Set metadata on DataFrames before range operations:\ndf_1.config_meta.set(coordinate_system_zero_based=True)\ndf_2.config_meta.set(coordinate_system_zero_based=True)\ndf_pb = pb.overlap(df_1, df_2, cols1=cols, cols2=cols)\ndf_pb.count().collect()\n</code></pre> <pre><code>307184634\n</code></pre> <pre><code>df_pb.collect_schema()\n</code></pre> <pre><code>Schema([('contig_1', String),\n        ('pos_start_1', Int32),\n        ('pos_end_1', Int32),\n        ('contig_2', String),\n        ('pos_start_2', Int32),\n        ('pos_end_2', Int32)])\n</code></pre> <pre><code>df_pb.collect().estimated_size(\"mb\")\n</code></pre> <pre><code>7360.232946395874\n</code></pre> <pre><code>df_pb.collect().to_pandas().info()\n</code></pre> <pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 307184634 entries, 0 to 307184633\nData columns (total 6 columns):\n #   Column       Dtype\n---  ------       -----\n 0   contig_1     object\n 1   pos_start_1  int32\n 2   pos_end_1    int32\n 3   contig_2     object\n 4   pos_start_2  int32\n 5   pos_end_2    int32\ndtypes: int32(4), object(2)\nmemory usage: 9.2+ GB\n</code></pre>"},{"location":"supplement/#bioframe-output-dataframe-schema-and-memory-used-pandas","title":"bioframe output DataFrame schema and memory used (Pandas)","text":"<pre><code>df_bf = bf.overlap(df1, df2, cols1=cols, cols2=cols, how=\"inner\")\nlen(df_bf)\n</code></pre> <pre><code>307184634\n</code></pre> <pre><code>df_bf.info()\n</code></pre> <pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 307184634 entries, 0 to 307184633\nData columns (total 6 columns):\n #   Column      Dtype\n---  ------      -----\n 0   contig      object\n 1   pos_start   int32\n 2   pos_end     int32\n 3   contig_     object\n 4   pos_start_  int32\n 5   pos_end_    int32\ndtypes: int32(4), object(2)\nmemory usage: 9.2+ GB\n</code></pre>"},{"location":"supplement/#pyranges0-output-dataframe-schema-and-memory-used-pandas","title":"pyranges0 output DataFrame schema and memory used (Pandas)","text":"<pre><code>df_pr0_1 = df2pr0(df1)\ndf_pr0_2 = df2pr0(df2)\n</code></pre> <pre><code>df_pr0_1.df.info()\n</code></pre> <pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1194285 entries, 0 to 1194284\nData columns (total 3 columns):\n #   Column      Non-Null Count    Dtype\n---  ------      --------------    -----\n 0   Chromosome  1194285 non-null  category\n 1   Start       1194285 non-null  int64\n 2   End         1194285 non-null  int64\ndtypes: category(1), int64(2)\nmemory usage: 19.4 MB\n</code></pre> <pre><code>df_pr0_2.df.info()\n</code></pre> <pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 9944559 entries, 0 to 9944558\nData columns (total 3 columns):\n #   Column      Dtype\n---  ------      -----\n 0   Chromosome  category\n 1   Start       int64\n 2   End         int64\ndtypes: category(1), int64(2)\nmemory usage: 161.2 MB\n</code></pre> <pre><code>df_pr0 = df_pr0_1.join(df_pr0_2)\nlen(df_pr0)\n</code></pre> <pre><code>307184634\n</code></pre> <pre><code>df_pr0.df.info()\n</code></pre> <pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 307184634 entries, 0 to 307184633\nData columns (total 5 columns):\n #   Column      Dtype\n---  ------      -----\n 0   Chromosome  category\n 1   Start       int64\n 2   End         int64\n 3   Start_b     int64\n 4   End_b       int64\ndtypes: category(1), int64(4)\nmemory usage: 9.4 GB\n</code></pre> <p>Note</p> <p>Please note that <code>pyranges</code> unlike bioframe and polars-bio returns only one chromosome column but uses <code>int64</code> data types for encoding start and end positions even if input datasets use <code>int32</code>.</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/#blog","title":"Blog","text":""},{"location":"blog/2025/09/05/interval-operations-benchmark--update-september-2025/","title":"Interval operations benchmark \u2014 update September 2025","text":""},{"location":"blog/2025/09/05/interval-operations-benchmark--update-september-2025/#interval-operations-benchmark-update-september-2025","title":"Interval operations benchmark \u2014 update September 2025","text":""},{"location":"blog/2025/09/05/interval-operations-benchmark--update-september-2025/#introduction","title":"Introduction","text":"<p>Benchmarking isn\u2019t a one-and-done exercise\u2014it\u2019s a moving target. As tools evolve, new versions can shift performance profiles in meaningful ways, so keeping results current is just as important as the first round of measurements.</p> <p>Recently, three novel libraries that have started to gain traction: pyranges1, GenomicRanges and polars-bio </p> <p>shipped major updates:</p> <ul> <li>pyranges1 adopted a new Rust backend (ruranges),</li> <li>GenomicRanges switched its interval core to a Nested Containment List (NCLS) and added multithreaded execution,</li> <li>polars-bio migrated to the new Polars streaming engine and added support for new interval data structures. As of version <code>0.12.0</code> it supports:<ul> <li>COITrees</li> <li>IITree</li> <li>AVL-tree</li> <li>rust-lapper</li> <li>superintervals</li> </ul> </li> </ul> <p>Each of these changes has the potential to meaningfully alter performance and memory characteristics for common genomic interval tasks.</p> <p>In this post, we revisit our benchmarks with those releases in mind. We focus on three everyday operations:</p> <ul> <li>overlap detection,</li> <li>nearest feature queries</li> <li>overlap counting.</li> </ul> <p>For comparability, we use the same AIList dataset from our previous write-up, so you can see exactly how the new backends and data structures change the picture. Let\u2019s dive in and see what\u2019s faster, what\u2019s leaner, and where the trade-offs now live.</p>"},{"location":"blog/2025/09/05/interval-operations-benchmark--update-september-2025/#setup","title":"Setup","text":""},{"location":"blog/2025/09/05/interval-operations-benchmark--update-september-2025/#benchmark-test-cases","title":"Benchmark test cases","text":"Dataset pairs Size # of overlaps (1-based) 1-2 &amp; 2-1 Small 54,246 7-3 &amp; 3-7 Medium 4,408,383 8-7 &amp; 7-8 Large 307,184,634"},{"location":"blog/2025/09/05/interval-operations-benchmark--update-september-2025/#software-versions","title":"Software versions","text":"Library Version polars_bio 0.13.1 pyranges 0.1.14 genomicranges 0.7.2"},{"location":"blog/2025/09/05/interval-operations-benchmark--update-september-2025/#results","title":"Results","text":""},{"location":"blog/2025/09/05/interval-operations-benchmark--update-september-2025/#polars-bio-interval-data-structures-performance-comparison","title":"polars-bio interval data structures performance comparison","text":"<p>Key takeaways:</p> <ul> <li>Superintervals seems to be the best default. Across all three test cases, it is consistently the fastest or tied for fastest, delivering 1.25\u20131.44x speedups over the polars-bio default (COITrees) and avoiding worst\u2011case behavior.</li> <li>Lapper caveat: performs well on 1\u20112 and 8\u20117, but collapses on 7\u20113 (\u224825x slower than default), so it\u2019s risky as a general\u2011purpose algorithm.</li> <li>Intervaltree/Arrayintervaltree: reliable but slower. They trail superintervals by 20\u201370% depending on the case.</li> </ul>"},{"location":"blog/2025/09/05/interval-operations-benchmark--update-september-2025/#all-operations-comparison","title":"All operations comparison","text":"<p>Key takeaways:</p> <ul> <li>Overlap: GenomicRanges wins on small inputs (1\u20112, 2\u20111) by ~2.1\u20132.3x, but polars\u2011bio takes over from medium size onward and dominates on large (7\u20118, 8\u20117), where PyRanges falls far behind. Interesting case of 7-8 vs 8-7 when swapping inputs can significantly affect performance of GenomicRanges.</li> <li>Nearest: polars\u2011bio leads decisively at every size; speedups over the others grow with input size (orders of magnitude on large datasets).</li> <li>Count overlaps: GenomicRanges edges out polars\u2011bio on the smallest inputs, while polars\u2011bio is faster on medium and substantially faster on large inputs.</li> </ul>"},{"location":"blog/2025/09/05/interval-operations-benchmark--update-september-2025/#all-operations-parallel-execution","title":"All operations parallel execution","text":"<p>Key takeaways:</p> <ul> <li>Thread scaling: both libraries (GenomicRanges and polars-bio) benefit from additional threads, but the absolute gap favors polars\u2011bio for medium/large datasets across overlap, nearest, and count overlaps.</li> <li>Small overlaps: GenomicRanges remains &gt;2x faster at 1\u20138 threads; on medium/large pairs its relative speed drops below 1x.</li> <li>Nearest: polars\u2011bio stays on the 1x reference line; GenomicRanges is typically 10\u2013100x slower (log scale) even with more threads.</li> <li>Count overlaps: small inputs slightly favor GenomicRanges; for larger inputs polars\u2011bio maintains 2\u201310x advantage with stable scaling.</li> </ul>"},{"location":"blog/2025/09/05/interval-operations-benchmark--update-september-2025/#end-to-end-data-proecesing","title":"End to-end data proecesing","text":"<p>Here we compare end-to-end performance including data loading, overlap operation, and saving results to CSV.</p> <p>Info</p> <ol> <li><code>POLARS_MAX_THREADS=1</code> was set to ensure fair comparison with single-threaded PyRanges.</li> <li>Since GenomicRanges supports Polars DataFrames as input and output, we used them instead of Pandas to again ensure fair comparison with polars-bio.</li> <li>GenomicRanges find_overlaps method returns hits-only table (indices of genomic intervals instead of genomic coordinates), we also benchmarked an extended version with additional lookup of intervals (<code>full rows</code>, code) for fair comparison.</li> </ol> <p></p> <p>Key takeaways:</p> <ul> <li>Wall time: GenomicRanges (hits\u2011only) is the fastest end\u2011to\u2011end here (~1.16x vs polars_bio) by avoiding full materialization of genomic intervals (unlike PyRanges and polars-bio that return pairs of genomic interval coordinates for each overlap); PyRanges is far slower; GenomicRanges (full rows, so with the output comparable with PyRanges and polars-bio) is much slower.</li> <li>Memory: polars-bio (streaming) minimizes peak RAM (~0.7 GB) while keeping speed comparable to polars-bio. GenomicRanges (full rows) peaks at ~40 GB; hits\u2011only sits in the middle (~8.2 GB) as it only returns DataFrame with pairs of indices not full genomic coordinates.</li> </ul>"},{"location":"blog/2025/09/05/interval-operations-benchmark--update-september-2025/#summary","title":"Summary","text":"<p>For small and medium datasets, all tools perform well; at large scale, polars-bio excels with better scalability and memory efficiency, achieving an ultra\u2011low footprint in streaming mode.</p>"},{"location":"blog/2026/02/20/interval-operations-benchmark--update-february-2026/","title":"Interval operations benchmark \u2014 update February 2026","text":""},{"location":"blog/2026/02/20/interval-operations-benchmark--update-february-2026/#interval-operations-benchmark-update-february-2026","title":"Interval operations benchmark \u2014 update February 2026","text":""},{"location":"blog/2026/02/20/interval-operations-benchmark--update-february-2026/#introduction","title":"Introduction","text":"<p>Back in September 2025 we benchmarked three libraries across three operations. A lot has changed since then. In December 2025, pyranges1 published a preprint describing its Rust-powered backend (ruranges) and an expanded set of interval operations. On the polars-bio side, version 0.24.0 ships a fully rewritten range-operations engine built on upstream DataFusion UDTF providers (OverlapProvider, NearestProvider, and the new coverage/cluster/complement/merge/subtract providers from datafusion-bio-function-ranges), replacing the earlier sequila-native backend.</p> <p>This rewrite also expanded the operation set from three to eight. In addition to overlap, nearest, and count_overlaps, polars-bio 0.24.0 supports coverage, cluster, complement, merge, and subtract \u2014 covering all the everyday interval manipulation tasks that genomics workflows depend on.</p> <p>We also added a fourth contender: Bioframe, a pandas-based genomic interval library widely used in the 3D genomics community. This gives us a broader view of the Python genomic interval landscape.</p> <p>For comparability with our previous benchmarks, we continue to use the same AIList dataset. All benchmark code and raw results are available in the polars-bio-bench repository.</p>"},{"location":"blog/2026/02/20/interval-operations-benchmark--update-february-2026/#setup","title":"Setup","text":""},{"location":"blog/2026/02/20/interval-operations-benchmark--update-february-2026/#software-versions","title":"Software versions","text":"Library Version polars-bio 0.24.0 pyranges1 1.2.0 GenomicRanges 0.8.4 bioframe 0.8.0"},{"location":"blog/2026/02/20/interval-operations-benchmark--update-february-2026/#benchmark-test-cases","title":"Benchmark test cases","text":"<p>Binary operations (overlap, nearest, count_overlaps, coverage):</p> Dataset pairs Size # of overlaps (1-based) 1-2 &amp; 2-1 Small 54,246 3-7 &amp; 7-3 Medium 4,408,383 7-8 &amp; 8-7 Large 307,184,634 <p>Unary operations (cluster, complement, merge, subtract):</p> Dataset Size Name (intervals) 1 Small fBrain (199K) 2 Small exons (439K) 7 Medium ex-anno (1,194K) 3 Medium chainOrnAna1 (1,957K) 8 Large ex-rna (9,945K) 5 Large chainXenTro3Link (50,981K)"},{"location":"blog/2026/02/20/interval-operations-benchmark--update-february-2026/#operations-and-tool-support","title":"Operations and tool support","text":"Operation polars-bio PyRanges1 GenomicRanges Bioframe overlap yes yes yes yes nearest yes yes yes yes count_overlaps yes yes yes yes coverage yes -- yes yes cluster yes yes -- yes complement yes yes yes yes merge yes yes yes yes subtract yes yes -- yes"},{"location":"blog/2026/02/20/interval-operations-benchmark--update-february-2026/#results","title":"Results","text":""},{"location":"blog/2026/02/20/interval-operations-benchmark--update-february-2026/#speedup-comparison-across-all-operations","title":"Speedup comparison across all operations","text":"<p>Info</p> <ol> <li>Missing bars indicate that the operation is not supported by the library for the given dataset.</li> <li>Crash bars indicate that the library failed to complete.</li> </ol> <p>Key takeaways:</p> <ul> <li>polars-bio is the fastest library in 7 out of 8 operations on the large dataset (8-7). The sole exception is nearest, where GenomicRanges holds a 1.63x advantage.</li> <li>On small datasets (1-2, 2-1), GenomicRanges leads in overlap (1.74x) and count_overlaps, reflecting lower per-call overhead for small inputs.</li> <li>Bioframe is consistently the slowest library, falling 5-50x behind polars-bio depending on the operation and dataset size.</li> <li>For the new operations (coverage, cluster, complement, merge, subtract), polars-bio leads across the board with PyRanges1 a respectable second (0.23-0.61x relative to polars-bio on the large dataset).</li> </ul> <p>Summary \u2014 dataset 8-7 speedup relative to polars-bio (higher is better for polars-bio):</p> Operation polars-bio PyRanges1 GenomicRanges Bioframe overlap 1.00x 0.18x 0.73x 0.13x nearest 1.00x 0.06x 1.63x 0.04x count_overlaps 1.00x 0.24x 0.19x 0.02x coverage 1.00x -- 0.36x 0.05x cluster 1.00x 0.61x -- 0.20x complement 1.00x 0.53x 0.02x 0.06x merge 1.00x 0.51x 0.02x 0.12x subtract 1.00x 0.23x -- 0.05x"},{"location":"blog/2026/02/20/interval-operations-benchmark--update-february-2026/#thread-scalability-dataset-8-7","title":"Thread scalability (dataset 8-7)","text":"<p> One of polars-bio's key advantages is transparent multithreaded execution. The table below shows wall-clock times for all eight operations on the large dataset (8-7) as thread count increases:</p> Operation 1 thread 8 threads Speedup overlap 4.03s 0.74s 5.43x nearest 2.55s 0.44s 5.77x count_overlaps 1.55s 0.27s 5.75x coverage 1.20s 0.22s 5.48x subtract 1.15s 0.17s 6.59x merge 0.29s 0.05s 5.34x complement 0.29s 0.06s 4.99x cluster 0.48s 0.15s 3.27x <p>Key takeaways:</p> <ul> <li>Most operations achieve near-linear scaling, with 5-6x speedup at 8 threads.</li> <li>subtract scales best at 6.59x, while cluster shows more modest scaling (3.27x) \u2014 expected, as clustering is inherently sequential within each chromosome.</li> <li>At 8 threads, overlap on 307M result pairs completes in just 0.74 seconds.</li> </ul>"},{"location":"blog/2026/02/20/interval-operations-benchmark--update-february-2026/#summary","title":"Summary","text":"<ul> <li>polars-bio is the fastest single-threaded library for 7 out of 8 operations at scale, with speedups ranging from 1.5x to 25x over alternatives.</li> <li>GenomicRanges wins the nearest operation and leads on small datasets, making it a solid choice when input sizes are modest.</li> <li>polars-bio delivers excellent thread scaling (5-6x at 8 threads), turning already-fast single-threaded times into sub-second performance on large datasets.</li> <li>PyRanges1, despite its recent preprint claiming \"ultrafast\" performance, is slower than polars-bio in every operation tested on large datasets (0.06-0.61x). While it is a solid improvement over its predecessor, the \"ultrafast\" characterization does not hold when compared against polars-bio's DataFusion-based engine.</li> <li>Bioframe is consistently the slowest across all operations and dataset sizes.</li> <li>The expanded eight-operation benchmark confirms that polars-bio's advantage extends beyond overlap and nearest to all standard range operation categories \u2014 coverage, cluster, complement, merge, and subtract.</li> </ul> <p>All benchmark code, raw results, and additional figures are available at polars-bio-bench.</p>"},{"location":"blog/2026/02/14/benchmarking-genomic-format-readers-in-python-with-polars/","title":"Benchmarking Genomic Format Readers in Python with Polars","text":""},{"location":"blog/2026/02/14/benchmarking-genomic-format-readers-in-python-with-polars/#benchmarking-genomic-format-readers-in-python-with-polars","title":"Benchmarking Genomic Format Readers in Python with Polars","text":"<p>Genomic analyses in Python typically start with reading BAM, VCF, or FASTQ files into memory. The choice of library for this step can have a dramatic impact on both wall-clock time and memory consumption \u2014 especially as datasets grow to tens or hundreds of millions of records.</p> <p>pysam has long been the go-to Python library for working with these formats. It provides comprehensive bindings to htslib and is battle-tested across thousands of projects. However, several newer libraries have emerged that leverage Apache Arrow columnar format and Rust-based parsers to offer potentially better performance.</p> <p>In this post, we benchmark four Python libraries head-to-head on real-world genomic data to find out which offers the best combination of speed and memory efficiency for reading BAM, VCF, and FASTQ files.</p>"},{"location":"blog/2026/02/14/benchmarking-genomic-format-readers-in-python-with-polars/#libraries-compared","title":"Libraries Compared","text":"Library Version Parallel Out-of-Core / Streaming pysam 0.23.3 Decompression only No oxbow 0.5.1 No Yes (streams) biobear 0.23.7 No No polars-bio 0.23.0 Yes Yes <p>All four libraries return data as Arrow-backed DataFrames (Polars or Pandas), making them easy to integrate into modern data analysis pipelines.</p>"},{"location":"blog/2026/02/14/benchmarking-genomic-format-readers-in-python-with-polars/#about-polars-bio","title":"About polars-bio","text":"<p>polars-bio is built on the Apache DataFusion query engine and Polars. Genomic format parsing is powered by datafusion-bio-formats, which uses noodles for low-level format I/O.</p> <p>Key architectural features that contribute to its performance:</p> <ul> <li>Streaming execution \u2014 records are processed in batches without materializing the full dataset in memory</li> <li>Automatic parallel partitioning \u2014 index files (BAI, TBI, GZI, CRAI) are used to split work across threads</li> <li>Predicate and projection pushdown \u2014 filtering and column selection happen during parsing, not after</li> <li>Broad format support \u2014 BAM, CRAM, VCF, FASTQ, FASTA, GFF, BED, and Pairs formats</li> </ul>"},{"location":"blog/2026/02/14/benchmarking-genomic-format-readers-in-python-with-polars/#benchmark-setup","title":"Benchmark Setup","text":""},{"location":"blog/2026/02/14/benchmarking-genomic-format-readers-in-python-with-polars/#test-data","title":"Test Data","text":"Format File Rows Source BAM NA12878 WES chr1 (~2 GB) 19.3M Extracted from NA12878 VCF Ensembl chr1 (bgzipped) 86.8M Ensembl Variation FASTQ ERR194158 (bgzipped) 10.8M EBI SRA"},{"location":"blog/2026/02/14/benchmarking-genomic-format-readers-in-python-with-polars/#methodology","title":"Methodology","text":"<ul> <li>Hardware: Apple Silicon (M-series)</li> <li>Runs: 2 per configuration, median reported</li> <li>Memory: Peak RSS measured via <code>psutil</code> polling (process + children)</li> <li>Thread counts: 1, 2, 4, 8 for polars-bio; other libraries are single-threaded</li> <li>Each benchmark runs as a separate subprocess to ensure clean memory measurement</li> </ul> <p>The full benchmark code is available in the bioformats-benchmark repository.</p>"},{"location":"blog/2026/02/14/benchmarking-genomic-format-readers-in-python-with-polars/#results","title":"Results","text":""},{"location":"blog/2026/02/14/benchmarking-genomic-format-readers-in-python-with-polars/#fastq","title":"FASTQ","text":"<p>At single-thread, polars-bio matches biobear's speed (4.8s vs 4.4s) while using 17x less memory (204 MB vs 3.7 GB).</p> <p>With 8 threads, polars-bio reads 10.8M FASTQ records in 0.6 seconds \u2014 that's 20x faster than pysam and 7x faster than biobear, while staying under 300 MB of memory.</p>"},{"location":"blog/2026/02/14/benchmarking-genomic-format-readers-in-python-with-polars/#bam-without-tags","title":"BAM \u2014 without tags","text":"<p>polars-bio is the fastest library at every thread count. At 1 thread: 15.4s (1.5x faster than biobear, 11x faster than pysam). At 8 threads: 4.5 seconds to read 19.3M BAM records.</p> <p>Memory tells an even more dramatic story: 209 MB for polars-bio vs 28.8 GB for pysam \u2014 a 138x difference.</p>"},{"location":"blog/2026/02/14/benchmarking-genomic-format-readers-in-python-with-polars/#bam-with-tags","title":"BAM \u2014 with tags","text":"<p>Including auxiliary tags increases the data volume per record significantly. polars-bio at 1 thread (20.8s) is 1.6x faster than biobear (34.1s) and 16x faster than pysam (328s).</p> <p>At 8 threads: 6.3 seconds \u2014 52x faster than pysam. Memory remains under 410 MB compared to pysam's 32.9 GB and biobear's 39.9 GB.</p>"},{"location":"blog/2026/02/14/benchmarking-genomic-format-readers-in-python-with-polars/#vcf-without-info-columns","title":"VCF \u2014 without INFO columns","text":"<p>For the 86.8M-row VCF, polars-bio at 1 thread (33.6s) is competitive with oxbow (35.2s) and faster than biobear (43.0s). pysam takes 111s.</p> <p>At 8 threads, polars-bio finishes in 6.2 seconds \u2014 18x faster than pysam \u2014 with only 255 MB of memory vs pysam's 19.8 GB.</p>"},{"location":"blog/2026/02/14/benchmarking-genomic-format-readers-in-python-with-polars/#vcf-with-info-columns","title":"VCF \u2014 with INFO columns","text":"<p>This is the most challenging test: parsing the full INFO field across 86.8M VCF records.</p> <ul> <li>pysam: exceeded the 10-minute timeout</li> <li>biobear: skipped (fails on this dataset)</li> <li>oxbow: 104.8s (lazy) / 110.6s (stream), with the lazy mode consuming 29.6 GB</li> </ul> <p>polars-bio completed the task at every thread count \u2014 1 thread: 69s, 8 threads: 11.2 seconds \u2014 while staying under 300 MB of memory. This is a test where polars-bio is the only library that delivers both correctness and practical performance.</p>"},{"location":"blog/2026/02/14/benchmarking-genomic-format-readers-in-python-with-polars/#thread-scaling","title":"Thread Scaling","text":"<p>polars-bio achieves near-linear scaling for most formats from 1 to 8 threads. FASTQ shows the best scaling efficiency (7.4x speedup at 8 threads), while BAM and VCF formats achieve 3-6x speedups.</p> <p>Memory remains remarkably stable across thread counts \u2014 staying below 410 MB even at 8 threads, regardless of format. This means you can leverage all available cores without worrying about memory pressure.</p>"},{"location":"blog/2026/02/14/benchmarking-genomic-format-readers-in-python-with-polars/#conclusions","title":"Conclusions","text":"<ul> <li>polars-bio 0.23.0 is the fastest library across all three formats, especially when multi-threading is enabled</li> <li>10-100x less memory than traditional eager libraries \u2014 polars-bio stays under 400 MB where pysam and biobear consume 20-40 GB</li> <li>Handles edge cases where other libraries fail (complex VCF INFO parsing) or time out</li> <li>True streaming execution enables processing datasets larger than available RAM, making it practical for whole-genome scale data</li> </ul> <p>A follow-up post will dive into the performance improvements between polars-bio 0.22.0 and 0.23.0, detailing the optimizations in datafusion-bio-formats that made these results possible.</p>"},{"location":"blog/2026/02/14/benchmarking-genomic-format-readers-in-python-with-polars/#try-it-yourself","title":"Try It Yourself","text":"<pre><code>pip install polars-bio\n</code></pre> <pre><code>import polars_bio as pb\nimport polars as pl\n\n# Scan lazily with predicate pushdown\nlf = pb.scan_bam(\"sample.bam\")\nresult = lf.filter(pl.col(\"chrom\") == \"chr1\").sink_parquet(\"/temp/chr1_alignments.parquet\")\n</code></pre> <ul> <li>Documentation</li> <li>GitHub</li> <li>PyPI</li> <li>Benchmark repository</li> </ul>"},{"location":"blog/2025/09/08/gff-file-reading-performance-enhancements-in-polars-bio-0150/","title":"GFF File Reading Performance Enhancements in polars-bio 0.15.0","text":""},{"location":"blog/2025/09/08/gff-file-reading-performance-enhancements-in-polars-bio-0150/#gff-file-reading-performance-enhancements-in-polars-bio-0150","title":"GFF File Reading Performance Enhancements in polars-bio 0.15.0","text":"<p>We're excited to announce significant performance improvements to GFF file reading in polars-bio <code>0.15.0</code>. This release introduces two major optimizations that dramatically improve both speed and memory efficiency when working with GFF files:</p>"},{"location":"blog/2025/09/08/gff-file-reading-performance-enhancements-in-polars-bio-0150/#key-enhancements","title":"Key Enhancements","text":"<p>Projection Pushdown: Only the columns you need are read from disk, reducing I/O overhead and memory usage. This is particularly beneficial when working with wide GFF files that contain many optional attributes.</p> <p>Predicate Pushdown: Row filtering is applied during the file reading process, eliminating the need to load irrelevant data into memory. This allows for lightning-fast queries on large GFF datasets.</p> <p>Fully Streamed Parallel Reads: BGZF-compressed files can now be read in parallel with true streaming, enabling out-of-core processing of massive genomic datasets without memory constraints.</p>"},{"location":"blog/2025/09/08/gff-file-reading-performance-enhancements-in-polars-bio-0150/#benchmark-methodology","title":"Benchmark Methodology","text":"<p>To evaluate these improvements, we conducted comprehensive benchmarks comparing three popular data processing libraries:</p> <ul> <li>Pandas: The traditional Python data analysis library</li> <li>Polars: High-performance DataFrame library with lazy evaluation</li> <li>polars-bio: Our specialized genomic data processing library built on Polars and Apache DataFusion</li> </ul> <p>All benchmarks were performed on a large GFF file (~7.7 million records, file and index needed for parallel reading) with both full scan and filtered query scenarios to demonstrate real-world performance gains.</p> <p>For pandas and polars reading, we used the following methods (thanks to @urineri for the Polars code). Since Polars decompresses compressed CSV/TSV files completely in memory as highlighted here, we also used <code>polars_streaming_csv_decompression</code>, a great plugin developed by @ghuls to enable streaming decompression in Polars.</p> <p>Test query used for filtered benchmarks (Polars and polars-bio):</p> <pre><code> result = (\n        lf.filter(\n            (pl.col(\"seqid\") == \"chrY\")\n            &amp; (pl.col(\"start\") &lt; 500000)\n            &amp; (pl.col(\"end\") &gt; 510000)\n        )\n        .select([\"seqid\", \"start\", \"end\", \"type\"])\n        .collect()\n    )\n</code></pre> <p>The above query is very selective and returns only two rows from the entire dataset.</p>"},{"location":"blog/2025/09/08/gff-file-reading-performance-enhancements-in-polars-bio-0150/#results","title":"Results","text":"<p>Complete benchmark code and results are available in the polars-bio repository.</p>"},{"location":"blog/2025/09/08/gff-file-reading-performance-enhancements-in-polars-bio-0150/#single-threaded-performance","title":"Single-threaded performance","text":"<p>Key takeaways:</p> <ul> <li>polars-bio delivers comparable performance to standard Polars for full scan operations and both significantly outperform Pandas.</li> <li>In the case of filtered queries, we can see further performance improvements with Polars and polars-bio thanks to predicate and projection pushdown optimizations. polars-bio is 2.5x faster than standard Polars.</li> </ul>"},{"location":"blog/2025/09/08/gff-file-reading-performance-enhancements-in-polars-bio-0150/#memory-usage","title":"Memory usage","text":"<p>Key takeaways:</p> <ul> <li>Polars and polars-bio use significantly less memory than Pandas for all operations.</li> <li>polars-bio and Polars with <code>polars_streaming_csv_decompression</code> can use more than 20x less memory than vanilla Polars and more than two orders of magnitude less memory than Pandas for operations involving filtering.</li> </ul>"},{"location":"blog/2025/09/08/gff-file-reading-performance-enhancements-in-polars-bio-0150/#thread-scalability","title":"Thread scalability","text":"<p>Key takeaways:</p> <ul> <li>polars-bio achieves near-linear scaling up to 8 threads for full scan operations, reaching 9.5x speedup at 16 threads compared to single-threaded performance.</li> <li>Filtered operations show excellent parallelization with polars-bio reaching 11x speedup at 16 threads, significantly outperforming other libraries. There is, however, non-negligible overhead due to parallelism at 1 thread (2.25s vs 4.2s, compared to the single-threaded benchmark).</li> <li>polars-streaming shows diminishing returns at higher thread counts due to the overhead of spawning decompression program threads (in the default configuration, this is capped at 4), while polars-bio maintains consistent scaling benefits.</li> </ul>"},{"location":"blog/2025/09/08/gff-file-reading-performance-enhancements-in-polars-bio-0150/#summary","title":"Summary","text":"<p>The benchmarks demonstrate that polars-bio <code>0.15.0</code> delivers significant performance improvements for GFF file processing. These optimizations, combined with near-linear thread scaling and fully streamed parallel reads, make polars-bio an ideal choice for high-performance genomic data analysis workflows.</p> <p>If you haven't tried polars-bio yet, now is a great time to explore its capabilities for efficient genomic data processing with Python! Join our upcoming seminar on September 15, 2025, to learn more about polars-bio and its applications in genomics. </p>"},{"location":"blog/2026/02/14/polars-bio-0230-faster-parsing-and-python-314-support/","title":"polars-bio 0.23.0: Faster Parsing and Python 3.14 Support","text":""},{"location":"blog/2026/02/14/polars-bio-0230-faster-parsing-and-python-314-support/#polars-bio-0230-faster-parsing-and-python-314-support","title":"polars-bio 0.23.0: Faster Parsing and Python 3.14 Support","text":"<p>polars-bio 0.23.0 is here with significant parsing performance improvements across VCF, BAM, and FASTQ formats, plus first-day support for Python 3.14. This release bumps the underlying datafusion-bio-formats engine to 0.5.0, delivering up to 3.6x faster VCF parsing with no API changes required.</p>"},{"location":"blog/2026/02/14/polars-bio-0230-faster-parsing-and-python-314-support/#performance-improvements","title":"Performance Improvements","text":"<p>The headline of this release is PR #300, which bumps datafusion-bio-formats to 0.5.0. The changes span two pull requests and touch every major format:</p> <p>VCF: The previous implementation allocated a new <code>Vec&lt;String&gt;</code> per record and then copied it into an Arrow array \u2014 O(n * k) allocations where n is the number of records and k is the number of INFO/tag fields. The new code writes directly into Arrow <code>StringBuilder</code> / <code>PrimitiveBuilder</code> arrays, reducing this to O(k) builder appends per record with no intermediate heap allocations. A reusable <code>String</code> buffer for multi-value INFO field joining eliminates repeated allocation on every record, and residual filter evaluation was moved before accumulation so non-matching rows are discarded before any Arrow builder work happens.</p> <p>BAM: Tag parsing followed the same pattern \u2014 auxiliary tags were collected into temporary <code>Vec</code>s before being copied into Arrow arrays. This has been replaced with direct <code>StringBuilder</code> / <code>ListBuilder</code> appends, cutting per-record allocations from O(t) (where t is the number of tags) to zero. Additionally, <code>libdeflate</code> was enabled for all BGZF dependencies, yielding ~30-50% faster gzip decompression across both BAM and bgzipped VCF reads.</p> <p>FASTQ: Replaced <code>Vec&lt;String&gt;</code> accumulation with <code>StringBuilder</code> for direct Arrow construction, and fixed a backpressure spin-loop that was cloning data unnecessarily.</p>"},{"location":"blog/2026/02/14/polars-bio-0230-faster-parsing-and-python-314-support/#version-comparison-1-thread","title":"Version Comparison (1 thread)","text":"<p>The chart below compares polars-bio 0.22.0 and 0.23.0 on the same datasets used in our companion benchmark post, running single-threaded on Apple Silicon:</p> <p></p> <p>Wall-time speedups:</p> Format 0.22.0 0.23.0 Speedup VCF with INFO 249.2s 69.1s 3.6x BAM with tags 39.4s 20.8s 1.9x VCF without INFO 44.4s 33.6s 1.3x BAM without tags 18.6s 15.0s 1.2x FASTQ 4.7s 4.8s ~1.0x <p>FASTQ was already highly optimized and remains essentially unchanged. The largest gains are in VCF with INFO parsing, where the combination of allocation elimination and early filtering produces a dramatic 3.6x speedup.</p> <p>Peak memory increased modestly (~150-180 MB to ~200-225 MB) as a tradeoff for the speed gains \u2014 the new code uses slightly larger intermediate buffers to enable zero-copy construction.</p> <p>For a full multi-library comparison (pysam, oxbow, biobear, polars-bio) across all formats and thread counts, see the Benchmarking Genomic Format Readers in Python post.</p>"},{"location":"blog/2026/02/14/polars-bio-0230-faster-parsing-and-python-314-support/#python-314-support","title":"Python 3.14 Support","text":"<p>PR #301 adds Python 3.14 support, closing #299. The Python upper bound was raised from <code>&lt;3.14</code> to <code>&lt;3.15</code>, and pyarrow's cap was bumped from <code>&lt;22</code> to <code>&lt;23</code> (pyarrow 22.0.0 is the first version shipping cp314 wheels). No runtime code changes were needed \u2014 the native extension uses the stable <code>abi3</code> ABI (cp39+), and all dependencies were already compatible. Python 3.14 has been added to the CI test matrix.</p>"},{"location":"blog/2026/02/14/polars-bio-0230-faster-parsing-and-python-314-support/#breaking-change","title":"Breaking Change","text":"<p>The <code>thread_num</code> parameter has been removed from <code>read_bed()</code>, <code>scan_bed()</code>, and <code>register_bed()</code>. Threading is now handled automatically by the upstream engine. If you were passing <code>thread_num</code>, simply remove the argument \u2014 everything else works the same.</p>"},{"location":"blog/2026/02/14/polars-bio-0230-faster-parsing-and-python-314-support/#install","title":"Install","text":"<pre><code>pip install polars-bio==0.23.0\n</code></pre> <ul> <li>Documentation</li> <li>GitHub</li> <li>PyPI</li> <li>Changelog</li> </ul>"},{"location":"notebooks/tutorial/","title":"\ud83d\udcdaTutorial","text":"In\u00a0[1]: Copied! <pre>import polars_bio as pb\nimport pandas as pd\nimport polars as pl\nimport bioframe as bf\n</pre> import polars_bio as pb import pandas as pd import polars as pl import bioframe as bf In\u00a0[2]: Copied! <pre>cols = [\"contig\", \"pos_start\", \"pos_end\"]\n</pre> cols = [\"contig\", \"pos_start\", \"pos_end\"] In\u00a0[6]: Copied! <pre>df1 = pd.DataFrame(\n    [[\"chr1\", 1, 5], [\"chr1\", 3, 8], [\"chr1\", 8, 10], [\"chr1\", 12, 14]],\n    columns=[\"chrom\", \"start\", \"end\"],\n)\n\ndf2 = pd.DataFrame(\n    [[\"chr1\", 4, 8], [\"chr1\", 10, 11]], columns=[\"chrom\", \"start\", \"end\"]\n)\n</pre> df1 = pd.DataFrame(     [[\"chr1\", 1, 5], [\"chr1\", 3, 8], [\"chr1\", 8, 10], [\"chr1\", 12, 14]],     columns=[\"chrom\", \"start\", \"end\"], )  df2 = pd.DataFrame(     [[\"chr1\", 4, 8], [\"chr1\", 10, 11]], columns=[\"chrom\", \"start\", \"end\"] ) In\u00a0[7]: Copied! <pre>display(df1)\n</pre> display(df1) chrom start end 0 chr1 1 5 1 chr1 3 8 2 chr1 8 10 3 chr1 12 14 In\u00a0[8]: Copied! <pre>display(df2)\n</pre> display(df2) chrom start end 0 chr1 4 8 1 chr1 10 11 In\u00a0[9]: Copied! <pre>overlapping_intervals = pb.overlap(df1, df2, output_type=\"pandas.DataFrame\")\n</pre> overlapping_intervals = pb.overlap(df1, df2, output_type=\"pandas.DataFrame\") In\u00a0[10]: Copied! <pre>display(overlapping_intervals)\n</pre> display(overlapping_intervals) chrom_1 start_1 end_1 chrom_2 start_2 end_2 0 chr1 1 5 chr1 4 8 1 chr1 3 8 chr1 4 8 2 chr1 8 10 chr1 4 8 3 chr1 8 10 chr1 10 11 In\u00a0[11]: Copied! <pre>pb.visualize_intervals(overlapping_intervals)\n</pre> pb.visualize_intervals(overlapping_intervals) In\u00a0[12]: Copied! <pre>nearest_intervals = pb.nearest(df1, df2, output_type=\"pandas.DataFrame\")\n</pre> nearest_intervals = pb.nearest(df1, df2, output_type=\"pandas.DataFrame\") In\u00a0[13]: Copied! <pre>display(nearest_intervals)\n</pre> display(nearest_intervals) chrom_1 start_1 end_1 chrom_2 start_2 end_2 distance 0 chr1 1 5 chr1 4 8 0 1 chr1 3 8 chr1 4 8 0 2 chr1 8 10 chr1 4 8 0 3 chr1 12 14 chr1 10 11 1 In\u00a0[14]: Copied! <pre>pb.visualize_intervals(nearest_intervals, \"nearest pair\")\n</pre> pb.visualize_intervals(nearest_intervals, \"nearest pair\") In\u00a0[15]: Copied! <pre>count_overlaps = pb.count_overlaps(df1, df2, output_type=\"pandas.DataFrame\")\n</pre> count_overlaps = pb.count_overlaps(df1, df2, output_type=\"pandas.DataFrame\") In\u00a0[16]: Copied! <pre>display(count_overlaps)\n</pre> display(count_overlaps) chrom start end count 0 chr1 1 5 1 1 chr1 3 8 1 2 chr1 8 10 2 3 chr1 12 14 0 In\u00a0[17]: Copied! <pre>coverage = pb.coverage(df1, df2, output_type=\"pandas.DataFrame\")\n</pre> coverage = pb.coverage(df1, df2, output_type=\"pandas.DataFrame\") In\u00a0[18]: Copied! <pre>display(coverage)\n</pre> display(coverage) chrom start end coverage 0 chr1 1 5 2 1 chr1 3 8 4 2 chr1 8 10 2 3 chr1 12 14 0 In\u00a0[19]: Copied! <pre>merge = pb.merge(df1,output_type=\"pandas.DataFrame\")\n</pre> merge = pb.merge(df1,output_type=\"pandas.DataFrame\") In\u00a0[20]: Copied! <pre>display(merge)\n</pre> display(merge) chrom start end n_intervals 0 chr1 1 10 3 1 chr1 12 14 1 In\u00a0[21]: Copied! <pre>gff = pb.scan_gff(\"data/example.gff3.bgz\")\n</pre> gff = pb.scan_gff(\"data/example.gff3.bgz\") In\u00a0[22]: Copied! <pre>gff.limit(3).collect()\n</pre> gff.limit(3).collect() <pre>0rows [00:00, ?rows/s]</pre> Out[22]: shape: (3, 9)chromstartendtypesourcescorestrandphaseattributesstru32u32strstrf32stru32list[struct[2]]\"chr1\"1186914409\"gene\"\"HAVANA\"null\"+\"null[{\"ID\",\"ENSG00000223972.5\"}, {\"gene_id\",\"ENSG00000223972.5\"}, \u2026 {\"havana_gene\",\"OTTHUMG00000000961.2\"}]\"chr1\"1186914409\"transcript\"\"HAVANA\"null\"+\"null[{\"ID\",\"ENST00000456328.2\"}, {\"Parent\",\"ENSG00000223972.5\"}, \u2026 {\"havana_transcript\",\"OTTHUMT00000362751.1\"}]\"chr1\"1186912227\"exon\"\"HAVANA\"null\"+\"null[{\"ID\",\"exon:ENST00000456328.2:1\"}, {\"Parent\",\"ENST00000456328.2\"}, \u2026 {\"havana_transcript\",\"OTTHUMT00000362751.1\"}] In\u00a0[23]: Copied! <pre>gff = pb.scan_gff(\"data/example.gff3.bgz\").select([\"start\", \"end\", \"ID\", \"havana_transcript\"])\n</pre> gff = pb.scan_gff(\"data/example.gff3.bgz\").select([\"start\", \"end\", \"ID\", \"havana_transcript\"]) In\u00a0[24]: Copied! <pre>gff.limit(3).collect()\n</pre> gff.limit(3).collect() <pre>0rows [00:00, ?rows/s]</pre> Out[24]: shape: (3, 4)startendIDhavana_transcriptu32u32strstr1186914409\"ENSG00000223972.5\"null1186914409\"ENST00000456328.2\"\"OTTHUMT00000362751.1\"1186912227\"exon:ENST00000456328.2:1\"\"OTTHUMT00000362751.1\" In\u00a0[25]: Copied! <pre>pb.scan_vcf(\"data/example.vcf\").limit(3).collect()\n</pre> pb.scan_vcf(\"data/example.vcf\").limit(3).collect() <pre>0rows [00:00, ?rows/s]</pre> Out[25]: shape: (2, 9)chromstartendidrefaltqualfiltercsqstru32u32strstrstrf64strlist[str]\"chr21\"2696007026960070\"rs116645811\"\"G\"\"A\"null\"\"[\"A|missense_variant|MODERATE|MRPL39|ENSG00000154719|Transcript|ENST00000307301|protein_coding|10/11||||1043|1001|334|T/M|aCg/aTg|||-1||HGNC|14027\", \"A|intron_variant|MODIFIER|MRPL39|ENSG00000154719|Transcript|ENST00000352957|protein_coding||9/9||||||||||-1||HGNC|14027\", \"A|upstream_gene_variant|MODIFIER|LINC00515|ENSG00000260583|Transcript|ENST00000567517|antisense|||||||||||4432|-1||HGNC|16019\"]\"chr21\"2696514826965148\"rs1135638\"\"G\"\"A\"null\"\"[\"A|synonymous_variant|LOW|MRPL39|ENSG00000154719|Transcript|ENST00000307301|protein_coding|8/11||||939|897|299|G|ggC/ggT|||-1||HGNC|14027\", \"A|synonymous_variant|LOW|MRPL39|ENSG00000154719|Transcript|ENST00000352957|protein_coding|8/10||||939|897|299|G|ggC/ggT|||-1||HGNC|14027\", \"A|synonymous_variant|LOW|MRPL39|ENSG00000154719|Transcript|ENST00000419219|protein_coding|8/8||||876|867|289|G|ggC/ggT|||-1|cds_end_NF|HGNC|14027\"] In\u00a0[26]: Copied! <pre>pb.describe_vcf(\"data/example.vcf\").sort(\"name\")\n</pre> pb.describe_vcf(\"data/example.vcf\").sort(\"name\") Out[26]: shape: (1, 3)nametypedescriptionstrstrstr\"CSQ\"\"String\"\"Consequence annotations from E\u2026 In\u00a0[27]: Copied! <pre>pb.scan_vcf(\"data/example.vcf\", info_fields=[\"CSQ\"]).limit(3).collect()\n</pre> pb.scan_vcf(\"data/example.vcf\", info_fields=[\"CSQ\"]).limit(3).collect() <pre>0rows [00:00, ?rows/s]</pre> Out[27]: shape: (2, 9)chromstartendidrefaltqualfiltercsqstru32u32strstrstrf64strlist[str]\"chr21\"2696007026960070\"rs116645811\"\"G\"\"A\"null\"\"[\"A|missense_variant|MODERATE|MRPL39|ENSG00000154719|Transcript|ENST00000307301|protein_coding|10/11||||1043|1001|334|T/M|aCg/aTg|||-1||HGNC|14027\", \"A|intron_variant|MODIFIER|MRPL39|ENSG00000154719|Transcript|ENST00000352957|protein_coding||9/9||||||||||-1||HGNC|14027\", \"A|upstream_gene_variant|MODIFIER|LINC00515|ENSG00000260583|Transcript|ENST00000567517|antisense|||||||||||4432|-1||HGNC|16019\"]\"chr21\"2696514826965148\"rs1135638\"\"G\"\"A\"null\"\"[\"A|synonymous_variant|LOW|MRPL39|ENSG00000154719|Transcript|ENST00000307301|protein_coding|8/11||||939|897|299|G|ggC/ggT|||-1||HGNC|14027\", \"A|synonymous_variant|LOW|MRPL39|ENSG00000154719|Transcript|ENST00000352957|protein_coding|8/10||||939|897|299|G|ggC/ggT|||-1||HGNC|14027\", \"A|synonymous_variant|LOW|MRPL39|ENSG00000154719|Transcript|ENST00000419219|protein_coding|8/8||||876|867|289|G|ggC/ggT|||-1|cds_end_NF|HGNC|14027\"] In\u00a0[28]: Copied! <pre>pb.scan_fastq(\"data/example.fastq.gz\").limit(3).collect()\n</pre> pb.scan_fastq(\"data/example.fastq.gz\").limit(3).collect() <pre>0rows [00:00, ?rows/s]</pre> Out[28]: shape: (3, 4)namedescriptionsequencequality_scoresstrstrstrstr\"SRR9130495.1\"\"D00236:723:HG32CBCX2:1:1108:13\u2026\"NCAATACAAAAGCAATATGGGAGAAGCTAC\u2026\"#4BDFDFFHGHGGJJJHIIIIGGIIJGJJG\u2026\"SRR9130495.2\"\"D00236:723:HG32CBCX2:1:1108:14\u2026\"NGTCAAAGATAAGATCAAAAGGCACTGGCT\u2026\"#1=DDDDD&gt;DHFH@EFHHGHGGFGIIIGIG\u2026\"SRR9130495.3\"\"D00236:723:HG32CBCX2:1:1108:17\u2026\"GTTTTCCTCTGGTTATTTCTAGGTACACTG\u2026\"@@@DDDFFHHHFHBHIIGJIJIIJIIIEHG\u2026 In\u00a0[29]: Copied! <pre>pb.scan_bam(\"data/example.bam\").limit(3).collect()\n</pre> pb.scan_bam(\"data/example.bam\").limit(3).collect() <pre>0rows [00:00, ?rows/s]</pre> Out[29]: shape: (3, 11)namechromstartendflagscigarmapping_qualitymate_chrommate_startsequencequality_scoresstrstru32u32u32stru32stru32strstr\"20FUKAAXX100202:1:21:2075:1360\u2026\"chr1\"11011187\"101M\"0\"chr1\"178\"TAACCCTAACCCTAACCCTAACCCTAACCC\u2026\"?&gt;=&gt;?@CBC@BBCCDABACCDABBB9:788\u2026\"20FUKAAXX100202:1:21:2733:1836\u2026\"chr1\"11011123\"101M\"0\"chr1\"183\"TAACCCTAACCCTAACCCTAACCCTAACCC\u2026\"CCDACCDCDABBDCDABBDCDABBDCDABB\u2026\"20FUKAAXX100202:1:22:19822:802\u2026\"chr1\"1101163\"101M\"4\"chr1\"35\"TAACCCTAACCCTAACCCTAACCCTAACCC\u2026\"@CC?@@CBC@BBCCDABBCCDABBCCDABB\u2026 In\u00a0[30]: Copied! <pre>pb.scan_bed(\"data/example.bed.bgz\").limit(3).collect()\n</pre> pb.scan_bed(\"data/example.bed.bgz\").limit(3).collect() <pre>0rows [00:00, ?rows/s]</pre> Out[30]: shape: (3, 4)chromstartendnamestru32u32str\"chr16\"1480000116800000\"FRA16A\"\"chr16\"6670000170800000\"FRA16B\"\"chr16\"6391750363934965\"FRA16C\" In\u00a0[31]: Copied! <pre>pb.register_gff(\"data/example.gff3.bgz\", \"example_gff\")\n</pre> pb.register_gff(\"data/example.gff3.bgz\", \"example_gff\") In\u00a0[32]: Copied! <pre>pb.sql(\"SHOW TABLES\").collect()\n</pre> pb.sql(\"SHOW TABLES\").collect() <pre>0rows [00:00, ?rows/s]</pre> Out[32]: shape: (18, 4)table_catalogtable_schematable_nametable_typestrstrstrstr\"datafusion\"\"public\"\"count_overlaps_coverage\"\"LOCAL TEMPORARY\"\"datafusion\"\"public\"\"example_gz\"\"BASE TABLE\"\"datafusion\"\"public\"\"vcf_schema_975224660\"\"BASE TABLE\"\"datafusion\"\"public\"\"s2\"\"BASE TABLE\"\"datafusion\"\"public\"\"example_gff\"\"BASE TABLE\"\u2026\u2026\u2026\u2026\"datafusion\"\"information_schema\"\"columns\"\"VIEW\"\"datafusion\"\"information_schema\"\"df_settings\"\"VIEW\"\"datafusion\"\"information_schema\"\"schemata\"\"VIEW\"\"datafusion\"\"information_schema\"\"routines\"\"VIEW\"\"datafusion\"\"information_schema\"\"parameters\"\"VIEW\" In\u00a0[33]: Copied! <pre>pb.sql(\"DESCRIBE example_gff\").collect()\n</pre> pb.sql(\"DESCRIBE example_gff\").collect() <pre>0rows [00:00, ?rows/s]</pre> Out[33]: shape: (9, 3)column_namedata_typeis_nullablestrstrstr\"chrom\"\"Utf8\"\"NO\"\"start\"\"UInt32\"\"NO\"\"end\"\"UInt32\"\"NO\"\"type\"\"Utf8\"\"NO\"\"source\"\"Utf8\"\"NO\"\"score\"\"Float32\"\"YES\"\"strand\"\"Utf8\"\"NO\"\"phase\"\"UInt32\"\"YES\"\"attributes\"\"List(Field { name: \"item\", dat\u2026\"YES\" In\u00a0[34]: Copied! <pre>pb.sql(\"SELECT start, end, type FROM example_gff WHERE end = 14409\").collect()\n</pre> pb.sql(\"SELECT start, end, type FROM example_gff WHERE end = 14409\").collect() <pre>0rows [00:00, ?rows/s]</pre> Out[34]: shape: (2, 3)startendtypeu32u32str1186914409\"gene\"1186914409\"transcript\" In\u00a0[35]: Copied! <pre>pb.register_view(\"v_example_gff\", \"SELECT start, end, type FROM example_gff WHERE end = 14409\")\n</pre> pb.register_view(\"v_example_gff\", \"SELECT start, end, type FROM example_gff WHERE end = 14409\") In\u00a0[36]: Copied! <pre>pb.sql(\"SELECT * FROM v_example_gff\").collect()\n</pre> pb.sql(\"SELECT * FROM v_example_gff\").collect() <pre>0rows [00:00, ?rows/s]</pre> Out[36]: shape: (2, 3)startendtypeu32u32str1186914409\"gene\"1186914409\"transcript\" In\u00a0[38]: Copied! <pre>pb.describe_vcf(\"gs://gcp-public-data--gnomad/release/4.1/genome_sv/gnomad.v4.1.sv.sites.vcf.gz\").filter(pl.col(\"description\").str.contains(r\"Latino.* allele frequency\"))\n</pre> pb.describe_vcf(\"gs://gcp-public-data--gnomad/release/4.1/genome_sv/gnomad.v4.1.sv.sites.vcf.gz\").filter(pl.col(\"description\").str.contains(r\"Latino.* allele frequency\")) Out[38]: shape: (3, 3)nametypedescriptionstrstrstr\"AF_amr\"\"Float\"\"Latino allele frequency (biall\u2026\"AF_amr_XY\"\"Float\"\"Latino XY allele frequency (bi\u2026\"AF_amr_XX\"\"Float\"\"Latino XX allele frequency (bi\u2026 In\u00a0[39]: Copied! <pre>pb.register_vcf(\"gs://gcp-public-data--gnomad/release/4.1/genome_sv/gnomad.v4.1.sv.sites.vcf.gz\", \"gnomad\", info_fields=['AF_amr'])\nquery = \"\"\"\n    SELECT\n        chrom,\n        start,\n        end,\n        alt,\n        array_element(af_amr,1) AS af_amr\n    FROM gnomad\n    WHERE\n        filter = 'HIGH_NCR'\n    AND\n        alt = '&lt;DUP&gt;'\n\"\"\"\npb.sql(f\"{query} LIMIT 3\").collect()\n</pre> pb.register_vcf(\"gs://gcp-public-data--gnomad/release/4.1/genome_sv/gnomad.v4.1.sv.sites.vcf.gz\", \"gnomad\", info_fields=['AF_amr']) query = \"\"\"     SELECT         chrom,         start,         end,         alt,         array_element(af_amr,1) AS af_amr     FROM gnomad     WHERE         filter = 'HIGH_NCR'     AND         alt = '' \"\"\" pb.sql(f\"{query} LIMIT 3\").collect() <pre>0rows [00:00, ?rows/s]</pre> Out[39]: shape: (3, 5)chromstartendaltaf_amrstru32u32strf32\"chr1\"10000295666\"&lt;DUP&gt;\"0.000293\"chr1\"138000144000\"&lt;DUP&gt;\"0.000166\"chr1\"160500172100\"&lt;DUP&gt;\"0.002639 In\u00a0[40]: Copied! <pre>pb.register_vcf(\"s3://gnomad-public-us-east-1/release/4.1/vcf/exomes/gnomad.exomes.v4.1.sites.chr1.vcf.bgz\", \"gnomad_sites_chr1\", info_fields=[])\n</pre> pb.register_vcf(\"s3://gnomad-public-us-east-1/release/4.1/vcf/exomes/gnomad.exomes.v4.1.sites.chr1.vcf.bgz\", \"gnomad_sites_chr1\", info_fields=[]) In\u00a0[41]: Copied! <pre>pb.sql(\"SELECT chrom, start, end, alt FROM gnomad_sites_chr1 LIMIT 10\").collect()\n</pre> pb.sql(\"SELECT chrom, start, end, alt FROM gnomad_sites_chr1 LIMIT 10\").collect() <pre>0rows [00:00, ?rows/s]</pre> Out[41]: shape: (10, 4)chromstartendaltstru32u32str\"chr1\"1199411994\"C\"\"chr1\"1201612016\"A\"\"chr1\"1206012065\"C\"\"chr1\"1207412074\"C\"\"chr1\"1210212102\"A\"\"chr1\"1210612106\"G\"\"chr1\"1213812138\"A\"\"chr1\"1215812158\"T\"\"chr1\"1216512165\"A\"\"chr1\"1216812168\"G\" In\u00a0[42]: Copied! <pre>pb.register_view(\"v_gnomad_sites_chr1\", \"SELECT * FROM gnomad_sites_chr1 LIMIT 20\")\n</pre> pb.register_view(\"v_gnomad_sites_chr1\", \"SELECT * FROM gnomad_sites_chr1 LIMIT 20\") In\u00a0[43]: Copied! <pre>pb.sql(\"SELECT * FROM v_gnomad_sites_chr1\").collect()\n</pre> pb.sql(\"SELECT * FROM v_gnomad_sites_chr1\").collect() <pre>0rows [00:00, ?rows/s]</pre> Out[43]: shape: (20, 8)chromstartendidrefaltqualfilterstru32u32strstrstrf64str\"chr1\"1199411994\"\"\"T\"\"C\"null\"AC0;AS_VQSR\"\"chr1\"1201612016\"\"\"G\"\"A\"null\"AC0;AS_VQSR\"\"chr1\"1206012065\"\"\"CTGGAG\"\"C\"null\"AC0;AS_VQSR\"\"chr1\"1207412074\"\"\"T\"\"C\"null\"AC0;AS_VQSR\"\"chr1\"1210212102\"\"\"G\"\"A\"null\"AC0;AS_VQSR\"\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\"chr1\"1219812198\"rs62635282\"\"G\"\"C\"null\"AS_VQSR\"\"chr1\"1220112201\"\"\"C\"\"G\"null\"AS_VQSR\"\"chr1\"1221412214\"rs202068986\"\"C\"\"G\"null\"AC0\"\"chr1\"1222512225\"\"\"C\"\"T\"null\"AS_VQSR\"\"chr1\"1223512235\"\"\"G\"\"A\"null\"AS_VQSR\" In\u00a0[44]: Copied! <pre>df = pl.DataFrame({\n    \"chrom\": [\"chr1\", \"chr1\"],\n    \"start\": [11993, 12102],\n    \"end\": [11996, 12200],\n    \"annotation\": [\"ann1\", \"ann2\"]\n})\npb.from_polars(\"test_annotation\", df)\npb.sql(\"SELECT * FROM test_annotation\").collect()\n</pre> df = pl.DataFrame({     \"chrom\": [\"chr1\", \"chr1\"],     \"start\": [11993, 12102],     \"end\": [11996, 12200],     \"annotation\": [\"ann1\", \"ann2\"] }) pb.from_polars(\"test_annotation\", df) pb.sql(\"SELECT * FROM test_annotation\").collect() <pre>0rows [00:00, ?rows/s]</pre> Out[44]: shape: (2, 4)chromstartendannotationstri64i64str\"chr1\"1199311996\"ann1\"\"chr1\"1210212200\"ann2\" In\u00a0[45]: Copied! <pre>pb.overlap(\"v_gnomad_sites_chr1\",\"test_annotation\").limit(3).collect()\n</pre> pb.overlap(\"v_gnomad_sites_chr1\",\"test_annotation\").limit(3).collect() <pre>0rows [00:00, ?rows/s]</pre> Out[45]: shape: (3, 12)chrom_1start_1end_1chrom_2start_2end_2annotation_1id_2ref_2alt_2qual_2filter_2stru32u32stri64i64strstrstrstrf64str\"chr1\"1199411994\"chr1\"1199311996\"ann1\"\"\"\"T\"\"C\"null\"AC0;AS_VQSR\"\"chr1\"1210212102\"chr1\"1210212200\"ann2\"\"\"\"G\"\"A\"null\"AC0;AS_VQSR\"\"chr1\"1210612106\"chr1\"1210212200\"ann2\"\"\"\"T\"\"G\"null\"AC0;AS_VQSR\" <p>These demo datasets are from databio.zip benchmark.</p> In\u00a0[46]: Copied! <pre>pb.set_option(\"datafusion.execution.target_partitions\", \"1\")\n</pre> pb.set_option(\"datafusion.execution.target_partitions\", \"1\") In\u00a0[47]: Copied! <pre>%%time\npb.overlap(\"data/ex-rna/*.parquet\", \"data/chainRn4/*.parquet\",  cols1=cols, cols2=cols).collect().count()\n</pre> %%time pb.overlap(\"data/ex-rna/*.parquet\", \"data/chainRn4/*.parquet\",  cols1=cols, cols2=cols).collect().count() <pre>0rows [00:00, ?rows/s]</pre> <pre>CPU times: user 3.13 s, sys: 566 ms, total: 3.69 s\nWall time: 3.72 s\n</pre> Out[47]: shape: (1, 6)contig_1pos_start_1pos_end_1contig_2pos_start_2pos_end_2u32u32u32u32u32u32164214743164214743164214743164214743164214743164214743 In\u00a0[48]: Copied! <pre>pb.set_option(\"datafusion.execution.target_partitions\", \"2\")\n</pre> pb.set_option(\"datafusion.execution.target_partitions\", \"2\") In\u00a0[50]: Copied! <pre>%%time\npb.overlap(\"data/ex-rna/*.parquet\", \"data/chainRn4/*.parquet\",  cols1=cols, cols2=cols).collect().count()\n</pre> %%time pb.overlap(\"data/ex-rna/*.parquet\", \"data/chainRn4/*.parquet\",  cols1=cols, cols2=cols).collect().count() <pre>0rows [00:00, ?rows/s]</pre> <pre>CPU times: user 3.2 s, sys: 556 ms, total: 3.76 s\nWall time: 1.95 s\n</pre> Out[50]: shape: (1, 6)contig_1pos_start_1pos_end_1contig_2pos_start_2pos_end_2u32u32u32u32u32u32164214743164214743164214743164214743164214743164214743 In\u00a0[51]: Copied! <pre>%%bash\ngsutil cat gs://genomics-public-data/platinum-genomes/fastq/ERR194146.fastq.gz | gunzip -c  | bgzip -c &gt; /tmp/ERR194146.fastq.bgz\ncd /tmp &amp;&amp; bgzip -r /tmp/ERR194146.fastq.bgz\n</pre> %%bash gsutil cat gs://genomics-public-data/platinum-genomes/fastq/ERR194146.fastq.gz | gunzip -c  | bgzip -c &gt; /tmp/ERR194146.fastq.bgz cd /tmp &amp;&amp; bgzip -r /tmp/ERR194146.fastq.bgz <pre>/opt/homebrew/share/google-cloud-sdk/platform/gsutil/third_party/google-auth-library-python/google/__init__.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools&lt;81.\n  import pkg_resources\n</pre> In\u00a0[\u00a0]: Copied! <pre>pb.set_option(\"datafusion.execution.target_partitions\", \"1\")\n</pre> pb.set_option(\"datafusion.execution.target_partitions\", \"1\") In\u00a0[59]: Copied! <pre>%%time\npb.scan_fastq(\"/tmp/ERR194146.fastq.bgz\").count().collect()\n</pre> %%time pb.scan_fastq(\"/tmp/ERR194146.fastq.bgz\").count().collect() <pre>0rows [00:00, ?rows/s]</pre> <pre>CPU times: user 3.71 s, sys: 242 ms, total: 3.95 s\nWall time: 3.77 s\n</pre> Out[59]: shape: (1, 4)namedescriptionsequencequality_scoresu32u32u32u328657652865765286576528657652 In\u00a0[53]: Copied! <pre>pb.set_option(\"datafusion.execution.target_partitions\", \"2\")\n</pre> pb.set_option(\"datafusion.execution.target_partitions\", \"2\") In\u00a0[55]: Copied! <pre>%%time\npb.scan_fastq(\"/tmp/ERR194146.fastq.bgz\").count().collect()\n</pre> %%time pb.scan_fastq(\"/tmp/ERR194146.fastq.bgz\").count().collect() <pre>0rows [00:00, ?rows/s]</pre> <pre>CPU times: user 3.83 s, sys: 344 ms, total: 4.17 s\nWall time: 2.03 s\n</pre> Out[55]: shape: (1, 4)namedescriptionsequencequality_scoresu32u32u32u328657652865765286576528657652 In\u00a0[60]: Copied! <pre>pb.set_option(\"datafusion.execution.target_partitions\", \"4\")\n</pre> pb.set_option(\"datafusion.execution.target_partitions\", \"4\") In\u00a0[61]: Copied! <pre>%%time\npb.scan_fastq(\"/tmp/ERR194146.fastq.bgz\").count().collect()\n</pre> %%time pb.scan_fastq(\"/tmp/ERR194146.fastq.bgz\").count().collect() <pre>0rows [00:00, ?rows/s]</pre> <pre>CPU times: user 3.86 s, sys: 397 ms, total: 4.26 s\nWall time: 1.23 s\n</pre> Out[61]: shape: (1, 4)namedescriptionsequencequality_scoresu32u32u32u328657652865765286576528657652 In\u00a0[62]: Copied! <pre>pb.set_option(\"datafusion.execution.target_partitions\", \"1\")\n</pre> pb.set_option(\"datafusion.execution.target_partitions\", \"1\") <p>Make sure you restart the kernel before running the next cells. memory-profiler is required.</p> In\u00a0[3]: Copied! <pre>%load_ext memory_profiler\n</pre> %load_ext memory_profiler In\u00a0[4]: Copied! <pre>%memit pb.overlap(\"data/ex-rna/*.parquet\", \"data/chainRn4/*.parquet\",  cols1=cols, cols2=cols).sink_parquet(\"/tmp/overlap.parquet\")\n</pre> %memit pb.overlap(\"data/ex-rna/*.parquet\", \"data/chainRn4/*.parquet\",  cols1=cols, cols2=cols).sink_parquet(\"/tmp/overlap.parquet\") <pre>0rows [00:00, ?rows/s]</pre> <pre>peak memory: 1399.36 MiB, increment: 1186.92 MiB\n</pre> In\u00a0[65]: Copied! <pre>%memit pb.overlap(\"data/ex-rna/*.parquet\", \"data/chainRn4/*.parquet\",  cols1=cols, cols2=cols).collect().write_parquet(\"/tmp/overlap.parquet\")\n</pre> %memit pb.overlap(\"data/ex-rna/*.parquet\", \"data/chainRn4/*.parquet\",  cols1=cols, cols2=cols).collect().write_parquet(\"/tmp/overlap.parquet\")  <pre>0rows [00:00, ?rows/s]</pre> <pre>peak memory: 17261.36 MiB, increment: 4621.69 MiB\n</pre> In\u00a0[5]: Copied! <pre>BIO_PD_DF1 = pd.read_parquet(f\"data/exons/\")\nBIO_PD_DF2 = pd.read_parquet(f\"data/fBrain-DS14718/\")\n</pre> BIO_PD_DF1 = pd.read_parquet(f\"data/exons/\") BIO_PD_DF2 = pd.read_parquet(f\"data/fBrain-DS14718/\") In\u00a0[6]: Copied! <pre>bf_overlap = bf.overlap(\n    BIO_PD_DF1,\n    BIO_PD_DF2,\n    cols1=cols,\n    cols2=cols,\n    suffixes=(\"_1\", \"_2\"),\n    how=\"inner\",\n)\n</pre> bf_overlap = bf.overlap(     BIO_PD_DF1,     BIO_PD_DF2,     cols1=cols,     cols2=cols,     suffixes=(\"_1\", \"_2\"),     how=\"inner\", ) In\u00a0[7]: Copied! <pre>pb_overlap = pb.overlap(\n    BIO_PD_DF1,\n    BIO_PD_DF2,\n    cols1=cols,\n    cols2=cols,\n    output_type=\"pandas.DataFrame\",\n    suffixes=(\"_1\", \"_2\"),\n)\n</pre> pb_overlap = pb.overlap(     BIO_PD_DF1,     BIO_PD_DF2,     cols1=cols,     cols2=cols,     output_type=\"pandas.DataFrame\",     suffixes=(\"_1\", \"_2\"), ) <p>Since polars-bio is one-based, the assertion will fail.</p> In\u00a0[8]: Copied! <pre>assert len(bf_overlap) == len(pb_overlap)\n</pre> assert len(bf_overlap) == len(pb_overlap) <pre>---------------------------------------------------------------------------\nAssertionError                            Traceback (most recent call last)\nCell In[8], line 1\n----&gt; 1 assert len(bf_overlap) == len(pb_overlap)\n\nAssertionError: </pre> <p>In polars-bio &gt;= 0.19.0, coordinate system is managed via DataFrame metadata set at I/O time. To use 0-based coordinates for compatibility with bioframe:</p> In\u00a0[9]: Copied! <pre># Read data with 0-based coordinates (for bioframe compatibility)\n# Note: coordinate system is set at I/O time and read from DataFrame metadata\ndf_1 = pb.scan_bed(file_1, use_zero_based=True).collect()\ndf_2 = pb.scan_bed(file_2, use_zero_based=True).collect()\n\ncols = (\"chrom\", \"start\", \"end\")\npb.overlap(\n    df_1,\n    df_2,\n    cols1=cols,\n    cols2=cols,\n    output_type=\"pandas.DataFrame\",\n    suffixes=(\"_1\", \"_2\"),\n)\n</pre> # Read data with 0-based coordinates (for bioframe compatibility) # Note: coordinate system is set at I/O time and read from DataFrame metadata df_1 = pb.scan_bed(file_1, use_zero_based=True).collect() df_2 = pb.scan_bed(file_2, use_zero_based=True).collect()  cols = (\"chrom\", \"start\", \"end\") pb.overlap(     df_1,     df_2,     cols1=cols,     cols2=cols,     output_type=\"pandas.DataFrame\",     suffixes=(\"_1\", \"_2\"), ) <pre>WARNING:polars_bio:0-based coordinate system was selected. Please ensure that both datasets follow this coordinate system.\n</pre> In\u00a0[10]: Copied! <pre>assert len(bf_overlap) == len(pb_overlap)\n</pre> assert len(bf_overlap) == len(pb_overlap) In\u00a0[11]: Copied! <pre>import polars_bio as pb\nimport polars as pl\n</pre> import polars_bio as pb import polars as pl In\u00a0[12]: Copied! <pre>gcs_vcf_path = (\n    \"gs://gcp-public-data--gnomad/release/4.1/genome_sv/gnomad.v4.1.sv.sites.vcf.gz\"\n)\n</pre> gcs_vcf_path = (     \"gs://gcp-public-data--gnomad/release/4.1/genome_sv/gnomad.v4.1.sv.sites.vcf.gz\" ) In\u00a0[15]: Copied! <pre># we need to override the compression type as the file is bgzipped not gzipped as the extension suggests\npb.scan_vcf(gcs_vcf_path, info_fields=[]).limit(3).collect()\n</pre> # we need to override the compression type as the file is bgzipped not gzipped as the extension suggests pb.scan_vcf(gcs_vcf_path, info_fields=[]).limit(3).collect() <pre>0rows [00:00, ?rows/s]</pre> Out[15]: shape: (3, 8)chromstartendidrefaltqualfilterstru32u32strstrstrf64str\"chr1\"10000295666\"gnomAD-SV_v3_DUP_chr1_01c2781c\"\"N\"\"&lt;DUP&gt;\"134.0\"HIGH_NCR\"\"chr1\"1043410434\"gnomAD-SV_v3_BND_chr1_1a45f73a\"\"N\"\"&lt;BND&gt;\"260.0\"HIGH_NCR;UNRESOLVED\"\"chr1\"1044010440\"gnomAD-SV_v3_BND_chr1_3fa36917\"\"N\"\"&lt;BND&gt;\"198.0\"HIGH_NCR;UNRESOLVED\" In\u00a0[16]: Copied! <pre>pb.describe_vcf(gcs_vcf_path).sort(\"name\").limit(5)\n</pre> pb.describe_vcf(gcs_vcf_path).sort(\"name\").limit(5) Out[16]: shape: (5, 3)nametypedescriptionstrstrstr\"AC\"\"Integer\"\"Number of non-reference allele\u2026\"AC_XX\"\"Integer\"\"Number of non-reference XX all\u2026\"AC_XY\"\"Integer\"\"Number of non-reference XY all\u2026\"AC_afr\"\"Integer\"\"Number of non-reference Africa\u2026\"AC_afr_XX\"\"Integer\"\"Number of non-reference Africa\u2026 In\u00a0[17]: Copied! <pre># here we can use automatic compression detection\naws_s3_vcf_path = \"s3://gnomad-public-us-east-1/release/4.1/vcf/exomes/gnomad.exomes.v4.1.sites.chr21.vcf.bgz\"\n</pre> # here we can use automatic compression detection aws_s3_vcf_path = \"s3://gnomad-public-us-east-1/release/4.1/vcf/exomes/gnomad.exomes.v4.1.sites.chr21.vcf.bgz\" In\u00a0[18]: Copied! <pre>pb.scan_vcf(aws_s3_vcf_path, chunk_size=8, concurrent_fetches=1, info_fields=[]).limit(3).collect()\n</pre> pb.scan_vcf(aws_s3_vcf_path, chunk_size=8, concurrent_fetches=1, info_fields=[]).limit(3).collect() <pre>0rows [00:00, ?rows/s]</pre> Out[18]: shape: (3, 8)chromstartendidrefaltqualfilterstru32u32strstrstrf64str\"chr21\"50319055031905\"\"\"C\"\"A\"null\"AC0;AS_VQSR\"\"chr21\"50319055031905\"\"\"C\"\"T\"null\"AC0;AS_VQSR\"\"chr21\"50319095031909\"\"\"T\"\"C\"null\"AC0;AS_VQSR\" In\u00a0[19]: Copied! <pre>vcf_info_fields = [\"SVTYPE\", \"SVLEN\"]\npb.scan_vcf(gcs_vcf_path, info_fields=vcf_info_fields).limit(3).collect()\n</pre> vcf_info_fields = [\"SVTYPE\", \"SVLEN\"] pb.scan_vcf(gcs_vcf_path, info_fields=vcf_info_fields).limit(3).collect() <pre>0rows [00:00, ?rows/s]</pre> Out[19]: shape: (3, 10)chromstartendidrefaltqualfiltersvtypesvlenstru32u32strstrstrf64strstri32\"chr1\"10000295666\"gnomAD-SV_v3_DUP_chr1_01c2781c\"\"N\"\"&lt;DUP&gt;\"134.0\"HIGH_NCR\"\"DUP\"285666\"chr1\"1043410434\"gnomAD-SV_v3_BND_chr1_1a45f73a\"\"N\"\"&lt;BND&gt;\"260.0\"HIGH_NCR;UNRESOLVED\"\"BND\"-1\"chr1\"1044010440\"gnomAD-SV_v3_BND_chr1_3fa36917\"\"N\"\"&lt;BND&gt;\"198.0\"HIGH_NCR;UNRESOLVED\"\"BND\"-1 In\u00a0[20]: Copied! <pre>! gsutil -m  cp  $gcs_vcf_path /tmp/gnomad.v4.1.sv.sites.vcf.gz &amp;&gt; /dev/null\n</pre> ! gsutil -m  cp  $gcs_vcf_path /tmp/gnomad.v4.1.sv.sites.vcf.gz &amp;&gt; /dev/null In\u00a0[\u00a0]: Copied! <pre>%%time\npb.set_option(\"datafusion.execution.target_partitions\", \"1\")\npb.scan_vcf(\"/tmp/gnomad.v4.1.sv.sites.vcf.gz\", info_fields=[]).count().collect()\n</pre> %%time pb.set_option(\"datafusion.execution.target_partitions\", \"1\") pb.scan_vcf(\"/tmp/gnomad.v4.1.sv.sites.vcf.gz\", info_fields=[]).count().collect() In\u00a0[\u00a0]: Copied! <pre>%%time\npb.set_option(\"datafusion.execution.target_partitions\", \"4\")\npb.scan_vcf(\"/tmp/gnomad.v4.1.sv.sites.vcf.gz\", info_fields=[]).count().collect()\n</pre> %%time pb.set_option(\"datafusion.execution.target_partitions\", \"4\") pb.scan_vcf(\"/tmp/gnomad.v4.1.sv.sites.vcf.gz\", info_fields=[]).count().collect() In\u00a0[24]: Copied! <pre>vcf_1 = \"gs://gcp-public-data--gnomad/release/4.1/genome_sv/gnomad.v4.1.sv.sites.vcf.gz\"\nvcf_2 = \"gs://gcp-public-data--gnomad/release/4.1/vcf/exomes/gnomad.exomes.v4.1.sites.chr21.vcf.bgz\"\n</pre> vcf_1 = \"gs://gcp-public-data--gnomad/release/4.1/genome_sv/gnomad.v4.1.sv.sites.vcf.gz\" vcf_2 = \"gs://gcp-public-data--gnomad/release/4.1/vcf/exomes/gnomad.exomes.v4.1.sites.chr21.vcf.bgz\" In\u00a0[\u00a0]: Copied! <pre>object_storage_options = pb.ObjectStorageOptions(\n    allow_anonymous=True,\n    enable_request_payer=False,\n    chunk_size=64,\n    concurrent_fetches=8,\n    max_retries=5,\n    timeout=10,\n    compression_type=\"bgz\",\n)\nvcf_read_options_1 = pb.VcfReadOptions(\n    info_fields=[\"SVTYPE\", \"SVLEN\"],\n    object_storage_options=object_storage_options,\n)\nvcf_read_options_2 = pb.VcfReadOptions(\n    object_storage_options=object_storage_options,\n)\nread_options_1 = pb.ReadOptions(vcf_read_options=vcf_read_options_1)\nread_options_2 = pb.ReadOptions(vcf_read_options=vcf_read_options_2)\n</pre> object_storage_options = pb.ObjectStorageOptions(     allow_anonymous=True,     enable_request_payer=False,     chunk_size=64,     concurrent_fetches=8,     max_retries=5,     timeout=10,     compression_type=\"bgz\", ) vcf_read_options_1 = pb.VcfReadOptions(     info_fields=[\"SVTYPE\", \"SVLEN\"],     object_storage_options=object_storage_options, ) vcf_read_options_2 = pb.VcfReadOptions(     object_storage_options=object_storage_options, ) read_options_1 = pb.ReadOptions(vcf_read_options=vcf_read_options_1) read_options_2 = pb.ReadOptions(vcf_read_options=vcf_read_options_2) In\u00a0[26]: Copied! <pre>pb.overlap(vcf_1, vcf_2, read_options1=read_options_1, read_options2=read_options_2).sink_csv(\n    \"/tmp/streaming_run.csv\"\n)\n</pre> pb.overlap(vcf_1, vcf_2, read_options1=read_options_1, read_options2=read_options_2).sink_csv(     \"/tmp/streaming_run.csv\" ) <pre>0rows [00:00, ?rows/s]</pre> In\u00a0[27]: Copied! <pre>pl.read_csv(\"/tmp/streaming_run.csv\").limit(3)\n</pre> pl.read_csv(\"/tmp/streaming_run.csv\").limit(3) Out[27]: shape: (3, 18)chrom_1start_1end_1id_1ref_1alt_1qual_1filter_1svtype_1svlen_1chrom_2start_2end_2id_2ref_2alt_2qual_2filter_2stri64i64stri64i64strstrstrstrstrstrstrstrf64strstri64\"chr21\"50191505047500\"chr21\"50361835036183\"\"\"A\"\"C\"null\"AC0;AS_VQSR\"\"gnomAD-SV_v3_DUP_chr21_029eb66\u2026\"N\"\"&lt;DUP&gt;\"34.0\"PASS\"\"DUP\"28350\"chr21\"50191505047500\"chr21\"50361845036184\"\"\"G\"\"A\"null\"AS_VQSR\"\"gnomAD-SV_v3_DUP_chr21_029eb66\u2026\"N\"\"&lt;DUP&gt;\"34.0\"PASS\"\"DUP\"28350\"chr21\"50191505047500\"chr21\"50361855036185\"\"\"G\"\"A\"null\"AS_VQSR\"\"gnomAD-SV_v3_DUP_chr21_029eb66\u2026\"N\"\"&lt;DUP&gt;\"34.0\"PASS\"\"DUP\"28350 In\u00a0[28]: Copied! <pre>pb.overlap(vcf_1, vcf_2, read_options1=read_options_1, read_options2=read_options_2).collect().count()\n</pre> pb.overlap(vcf_1, vcf_2, read_options1=read_options_1, read_options2=read_options_2).collect().count() <pre>0rows [00:00, ?rows/s]</pre> Out[28]: shape: (1, 18)chrom_1start_1end_1chrom_2start_2end_2id_1ref_1alt_1qual_1filter_1id_2ref_2alt_2qual_2filter_2svtype_2svlen_2u32u32u32u32u32u32u32u32u32u32u32u32u32u32u32u32u32u3217882545178825451788254517882545178825451788254517882545178825451788254501788254517882545178825451788254517793674178825451788254517882545 In\u00a0[29]: Copied! <pre>gcs_vcf_path = \"gs://genomics-public-data/platinum-genomes/vcf/NA12878_S1.genome.vcf\"\n</pre> gcs_vcf_path = \"gs://genomics-public-data/platinum-genomes/vcf/NA12878_S1.genome.vcf\" In\u00a0[30]: Copied! <pre>info_fields=[\"AC\", \"AF\"]\n</pre> info_fields=[\"AC\", \"AF\"] In\u00a0[31]: Copied! <pre>pb.scan_vcf(gcs_vcf_path, info_fields=info_fields).limit(3).collect()\n</pre> pb.scan_vcf(gcs_vcf_path, info_fields=info_fields).limit(3).collect() <pre>0rows [00:00, ?rows/s]</pre> Out[31]: shape: (3, 10)chromstartendidrefaltqualfilteracafstru32u32strstrstrf64strlist[i32]list[f32]\"chrM\"11\"\"\"G\"\"\"null\"PASS\"nullnull\"chrM\"272\"\"\"A\"\"\"null\"PASS\"nullnull\"chrM\"7373\"\"\"G\"\"A\"8752.780273\"TruthSensitivityTranche99.90to\u2026[2][1.0] <p>Check SQL reference for details.</p> In\u00a0[\u00a0]: Copied! <pre>pb.register_vcf(vcf_1, \"gnomad_sv\", info_fields=[\"SVTYPE\", \"SVLEN\"])\n</pre> pb.register_vcf(vcf_1, \"gnomad_sv\", info_fields=[\"SVTYPE\", \"SVLEN\"]) In\u00a0[33]: Copied! <pre>pb.sql(\"SELECT chrom, svtype  FROM gnomad_sv\").limit(3).collect()\n</pre> pb.sql(\"SELECT chrom, svtype  FROM gnomad_sv\").limit(3).collect() <pre>0rows [00:00, ?rows/s]</pre> Out[33]: shape: (3, 2)chromsvtypestrstr\"chr1\"\"DUP\"\"chr1\"\"BND\"\"chr1\"\"BND\" In\u00a0[34]: Copied! <pre>pb.sql(\"SELECT * FROM gnomad_sv WHERE SVTYPE = 'DEL' AND SVLEN &gt; 1000\").limit(3).collect()\n</pre> pb.sql(\"SELECT * FROM gnomad_sv WHERE SVTYPE = 'DEL' AND SVLEN &gt; 1000\").limit(3).collect() <pre>0rows [00:00, ?rows/s]</pre> Out[34]: shape: (3, 10)chromstartendidrefaltqualfiltersvtypesvlenstru32u32strstrstrf64strstri32\"chr1\"2200030000\"gnomAD-SV_v3_DEL_chr1_fa103016\"\"N\"\"&lt;DEL&gt;\"999.0\"HIGH_NCR\"\"DEL\"8000\"chr1\"4000047000\"gnomAD-SV_v3_DEL_chr1_b26f63f7\"\"N\"\"&lt;DEL&gt;\"145.0\"PASS\"\"DEL\"7000\"chr1\"7908688118\"gnomAD-SV_v3_DEL_chr1_733c4ef0\"\"N\"\"&lt;DEL:ME:LINE1&gt;\"344.0\"UNRESOLVED\"\"DEL\"9032 In\u00a0[35]: Copied! <pre>pb.sql(\"SELECT alt, count(*) as cnt FROM gnomad_sv group by alt\").collect_schema()\n</pre> pb.sql(\"SELECT alt, count(*) as cnt FROM gnomad_sv group by alt\").collect_schema() Out[35]: <pre>Schema([('alt', String), ('cnt', Int64)])</pre> In\u00a0[36]: Copied! <pre>pb.sql(\"SELECT alt, count(*) as cnt FROM gnomad_sv group by alt\").collect()\n</pre> pb.sql(\"SELECT alt, count(*) as cnt FROM gnomad_sv group by alt\").collect() <pre>0rows [00:00, ?rows/s]</pre> Out[36]: shape: (13, 2)altcntstri64\"&lt;DUP&gt;\"269326\"&lt;BND&gt;\"356035\"&lt;CNV&gt;\"721\"&lt;DEL&gt;\"1197080\"&lt;INS&gt;\"83441\u2026\u2026\"&lt;INS:ME:SVA&gt;\"17607\"&lt;CPX&gt;\"15189\"&lt;INV&gt;\"2193\"&lt;DEL:ME:HERVK&gt;\"693\"&lt;CTX&gt;\"99 In\u00a0[37]: Copied! <pre>pb.sql(\"SELECT chrom, count(*) as cnt FROM gnomad_sv GROUP BY chrom ORDER BY chrom\").collect()\n</pre> pb.sql(\"SELECT chrom, count(*) as cnt FROM gnomad_sv GROUP BY chrom ORDER BY chrom\").collect() <pre>0rows [00:00, ?rows/s]</pre> Out[37]: shape: (24, 2)chromcntstri64\"chr1\"182804\"chr10\"96755\"chr11\"95690\"chr12\"97655\"chr13\"63839\u2026\u2026\"chr7\"131866\"chr8\"101224\"chr9\"87748\"chrX\"78076\"chrY\"12488 In\u00a0[38]: Copied! <pre>pb.sql(\"SELECT * FROM gnomad_sv WHERE chrom='chr1'\").sink_csv(\"/tmp/gnomad_chr1.csv\")\n</pre> pb.sql(\"SELECT * FROM gnomad_sv WHERE chrom='chr1'\").sink_csv(\"/tmp/gnomad_chr1.csv\") <pre>0rows [00:00, ?rows/s]</pre> In\u00a0[39]: Copied! <pre>pl.read_csv(\"/tmp/gnomad_chr1.csv\").count()\n</pre> pl.read_csv(\"/tmp/gnomad_chr1.csv\").count() Out[39]: shape: (1, 10)chromstartendidrefaltqualfiltersvtypesvlenu32u32u32u32u32u32u32u32u32u32182804182804182804182804182804182804182765182804182804182804 In\u00a0[40]: Copied! <pre>pb.register_vcf(vcf_2, \"gnomad_exomes\", info_fields=[\"AC\", \"AF\"])\n</pre> pb.register_vcf(vcf_2, \"gnomad_exomes\", info_fields=[\"AC\", \"AF\"]) In\u00a0[41]: Copied! <pre>pb.sql(\"SELECT replace(chrom,'chr','') AS chrom, start, ac,af  FROM gnomad_exomes WHERE array_element(af,1)&gt;0.01\").limit(10).collect()\n</pre> pb.sql(\"SELECT replace(chrom,'chr','') AS chrom, start, ac,af  FROM gnomad_exomes WHERE array_element(af,1)&gt;0.01\").limit(10).collect() <pre>0rows [00:00, ?rows/s]</pre> Out[41]: shape: (10, 4)chromstartacafstru32list[i32]list[f32]\"21\"5033364[372992][0.337086]\"21\"5033539[1107064][0.996312]\"21\"5034629[15145][0.020082]\"21\"5035021[1811][0.307888]\"21\"5035108[2][0.010989]\"21\"5035658[255555][0.336447]\"21\"5035846[37233][0.286906]\"21\"5035921[1682][0.010757]\"21\"5116593[4032][0.018626]\"21\"5116760[101][0.014914] In\u00a0[42]: Copied! <pre>pb.overlap(\"gnomad_sv\", \"gnomad_exomes\").collect().count()\n</pre> pb.overlap(\"gnomad_sv\", \"gnomad_exomes\").collect().count() <pre>0rows [00:00, ?rows/s]</pre> Out[42]: shape: (1, 20)chrom_1start_1end_1chrom_2start_2end_2id_1ref_1alt_1qual_1filter_1ac_1af_1id_2ref_2alt_2qual_2filter_2svtype_2svlen_2u32u32u32u32u32u32u32u32u32u32u32u32u32u32u32u32u32u32u32u32178825451788254517882545178825451788254517882545178825451788254517882545017882545178825451758750317882545178825451788254517793674178825451788254517882545"},{"location":"notebooks/tutorial/#introduction-to-polars-bio-features","title":"Introduction to polars-bio features\u00b6","text":""},{"location":"notebooks/tutorial/#interval-operations-illustrated","title":"Interval operations illustrated\u00b6","text":"<p>Please note that for visualizaiton of the intervals we used plot_intervals method from Bioframe package.</p>"},{"location":"notebooks/tutorial/#overlap","title":"Overlap\u00b6","text":""},{"location":"notebooks/tutorial/#nearest","title":"Nearest\u00b6","text":""},{"location":"notebooks/tutorial/#count-overlaps","title":"Count overlaps\u00b6","text":""},{"location":"notebooks/tutorial/#coverage","title":"Coverage\u00b6","text":""},{"location":"notebooks/tutorial/#merge","title":"Merge\u00b6","text":""},{"location":"notebooks/tutorial/#reading-bioinformatics-data","title":"Reading bioinformatics data\u00b6","text":""},{"location":"notebooks/tutorial/#gff","title":"GFF\u00b6","text":""},{"location":"notebooks/tutorial/#basic-reading-with-attributes-unnesting","title":"Basic reading with attributes unnesting\u00b6","text":""},{"location":"notebooks/tutorial/#unnesting-attributes","title":"Unnesting attributes\u00b6","text":""},{"location":"notebooks/tutorial/#vcf","title":"VCF\u00b6","text":""},{"location":"notebooks/tutorial/#get-info-fields-available-in-the-vcf-file","title":"Get info fields available in the VCF file\u00b6","text":""},{"location":"notebooks/tutorial/#fastq","title":"FASTQ\u00b6","text":""},{"location":"notebooks/tutorial/#bam","title":"BAM\u00b6","text":""},{"location":"notebooks/tutorial/#bed","title":"BED\u00b6","text":""},{"location":"notebooks/tutorial/#sql-processing","title":"SQL processing\u00b6","text":""},{"location":"notebooks/tutorial/#registering-tables","title":"Registering tables\u00b6","text":""},{"location":"notebooks/tutorial/#querying-registered-tables","title":"Querying registered tables\u00b6","text":""},{"location":"notebooks/tutorial/#registering-views","title":"Registering views\u00b6","text":""},{"location":"notebooks/tutorial/#working-with-cloud-storage","title":"Working with cloud storage\u00b6","text":""},{"location":"notebooks/tutorial/#overlapping-with-a-local-dataframe","title":"Overlapping with a local dataframe\u00b6","text":""},{"location":"notebooks/tutorial/#parallel-processing","title":"Parallel processing\u00b6","text":""},{"location":"notebooks/tutorial/#automatic-parallel-reading-of-bgzf-compressed-fastq-files","title":"Automatic parallel reading of BGZF compressed FASTQ files\u00b6","text":""},{"location":"notebooks/tutorial/#streaming","title":"Streaming\u00b6","text":""},{"location":"notebooks/tutorial/#overlap-in-the-streaming-mode","title":"Overlap in the streaming mode\u00b6","text":""},{"location":"notebooks/tutorial/#overlap-with-materialization","title":"Overlap with materialization\u00b6","text":""},{"location":"notebooks/tutorial/#coordinate-system-configuration","title":"Coordinate System Configuration\u00b6","text":""},{"location":"notebooks/tutorial/#compatibility-with-bioframe-and-other-0-based-tools","title":"Compatibility with bioframe and other 0-based tools\u00b6","text":"<p>In polars-bio &gt;= 0.19.0, the coordinate system is managed via DataFrame metadata that is set at I/O time. By default, polars-bio uses 1-based coordinates. To work with 0-based coordinates (for bioframe compatibility):</p> <ol> <li>Use <code>use_zero_based=True</code> when reading files: <code>pb.scan_bed(file, use_zero_based=True)</code></li> <li>Or set the global config: <code>pb.set_option(pb.POLARS_BIO_COORDINATE_SYSTEM_ZERO_BASED, True)</code></li> </ol>"},{"location":"notebooks/tutorial/#example-gnomad-vcf-file-reading","title":"Example - gnomAD VCF file reading\u00b6","text":""},{"location":"notebooks/tutorial/#1-how-to-read-gnomad-vcf-files-from-google-cloud-storage-or-aws-s3","title":"1. How to read gnomAD VCF files from Google Cloud Storage or AWS S3\u00b6","text":""},{"location":"notebooks/tutorial/#google-cloud-storage","title":"Google Cloud Storage\u00b6","text":""},{"location":"notebooks/tutorial/#aws-s3","title":"AWS S3\u00b6","text":""},{"location":"notebooks/tutorial/#2-how-to-specify-additional-vcf-info-fields-to-be-parsed","title":"2. How to specify additional VCF INFO fields to be parsed\u00b6","text":""},{"location":"notebooks/tutorial/#3-how-to-speed-up-reading-local-compresed-vcf-files-with-multiple-threads","title":"3. How to speed up reading local compresed VCF files with multiple threads\u00b6","text":""},{"location":"notebooks/tutorial/#4-how-to-perform-an-overlap-operation-on-two-remote-vcf-files-in-streaming-mode","title":"4. How to perform an overlap operation on two remote VCF files in streaming mode\u00b6","text":""},{"location":"notebooks/tutorial/#5-how-to-read-a-vcf-from-google-life-sciences","title":"5. How to read a VCF from Google Life Sciences\u00b6","text":""},{"location":"notebooks/tutorial/#6-sql-data-processing","title":"6. SQL data processing\u00b6","text":""},{"location":"notebooks/tutorial/","title":"\u00b6","text":""},{"location":"blog/category/performance/","title":"performance","text":""},{"location":"blog/category/performance/#performance","title":"performance","text":""},{"location":"blog/category/benchmarks/","title":"benchmarks","text":""},{"location":"blog/category/benchmarks/#benchmarks","title":"benchmarks","text":""},{"location":"blog/category/file-formats/","title":"file formats","text":""},{"location":"blog/category/file-formats/#file-formats","title":"file formats","text":""},{"location":"blog/category/releases/","title":"releases","text":""},{"location":"blog/category/releases/#releases","title":"releases","text":""}]}